---
# Documentation: https://wowchemy.com/docs/managing-content/

title: How visual cues to speech rate influence speech perception
subtitle: ''
summary: ''
authors:
- Hans Rutger Bosker
- David Peeters
- Judith Holler
tags:
- visual
- ratenorm
categories: []
date: '2020-01-01'
lastmod: 2022-06-13T15:10:25+02:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2022-06-13T13:10:13.477252Z'
publication_types:
- '2'
abstract: Spoken words are highly variable and therefore listeners interpret speech
  sounds relative to the surrounding acoustic context, such as the speech rate of
  a preceding sentence. For instance, a vowel midway between short /ɑ/ and long /a:/
  in Dutch is perceived as short /ɑ/ in the context of preceding slow speech, but
  as long /a:/ if preceded by a fast context. Despite the well-established influence
  of visual articulatory cues on speech comprehension, it remains unclear whether
  visual cues to speech rate also influence subsequent spoken word recognition. In
  two ‘Go Fish’-like experiments, participants were presented with audio-only (auditory
  speech + fixation cross), visual-only (mute videos of talking head), and audiovisual
  (speech + videos) context sentences, followed by ambiguous target words containing
  vowels midway between short /ɑ/ and long /a:/. In Experiment 1, target words were
  always presented auditorily, without visual articulatory cues. Although the audio-only
  and audiovisual contexts induced a rate effect (i.e., more long /a:/ responses after
  fast contexts), the visual-only condition did not. When, in Experiment 2, target
  words were presented audiovisually, rate effects were observed in all three conditions,
  including visual-only. This suggests that visual cues to speech rate in a context
  sentence influence the perception of following visual target cues (e.g., duration
  of lip aperture), which at an audiovisual integration stage bias participants’ target
  categorization responses. These findings contribute to a better understanding of
  how what we see influences what we hear.
publication: '*Quarterly Journal of Experimental Psychology*'
doi: 10.1177/1747021820914564
---
