
@incollection{bosker_sibilants_2013,
	address = {Leiden},
	title = {Sibilants},
	volume = {III},
	booktitle = {Encyclopedia of {Hebrew} {Language} and {Linguistics}},
	publisher = {Brill},
	author = {Bosker, Hans Rutger},
	editor = {Kahn, G.},
	year = {2013},
	pages = {557--561},
	file = {Bosker - 2013 - Sibilants.pdf:D\:\\HRBfiles\\Zotero Files\\storage\\UWTLBGDX\\Bosker - 2013 - Sibilants.pdf:application/pdf},
}

@incollection{bosker_entrained_2017,
	title = {An entrained rhythm’s frequency, not phase, influences temporal sampling of speech},
	url = {doi.org/10.21437/Interspeech.2017-73},
	booktitle = {Proceedings of {Interspeech} 2017, {Stockholm}},
	author = {Bosker, Hans Rutger and Kösem, Anne},
	year = {2017},
	file = {Bosker en Kösem - 2017 - An entrained rhythm’s frequency, not phase, influe.PDF:D\:\\HRBfiles\\Zotero Files\\storage\\CQU6WJUG\\Bosker en Kösem - 2017 - An entrained rhythm’s frequency, not phase, influe.PDF:application/pdf},
}

@article{bosker_how_2020,
	title = {How visual cues to speech rate influence speech perception},
	volume = {73},
	doi = {10.1177/1747021820914564},
	abstract = {Spoken words are highly variable and therefore listeners interpret speech sounds relative to the surrounding acoustic context, such as the speech rate of a preceding sentence. For instance, a vowel midway between short /ɑ/ and long /a:/ in Dutch is perceived as short /ɑ/ in the context of preceding slow speech, but as long /a:/ if preceded by a fast context. Despite the well-established influence of visual articulatory cues on speech comprehension, it remains unclear whether visual cues to speech rate also influence subsequent spoken word recognition. In two ‘Go Fish’-like experiments, participants were presented with audio-only (auditory speech + fixation cross), visual-only (mute videos of talking head), and audiovisual (speech + videos) context sentences, followed by ambiguous target words containing vowels midway between short /ɑ/ and long /a:/. In Experiment 1, target words were always presented auditorily, without visual articulatory cues. Although the audio-only and audiovisual contexts induced a rate effect (i.e., more long /a:/ responses after fast contexts), the visual-only condition did not. When, in Experiment 2, target words were presented audiovisually, rate effects were observed in all three conditions, including visual-only. This suggests that visual cues to speech rate in a context sentence influence the perception of following visual target cues (e.g., duration of lip aperture), which at an audiovisual integration stage bias participants’ target categorization responses. These findings contribute to a better understanding of how what we see influences what we hear.},
	language = {en},
	number = {10},
	journal = {Quarterly Journal of Experimental Psychology},
	author = {Bosker, Hans Rutger and Peeters, David and Holler, Judith},
	year = {2020},
	keywords = {visual, ratenorm},
	pages = {1523--1536},
	file = {Bosker et al. - 2020 - How visual cues to speech rate influence speech pe.pdf:D\:\\HRBfiles\\Zotero Files\\storage\\CIS58R7G\\Bosker et al. - 2020 - How visual cues to speech rate influence speech pe.pdf:application/pdf},
}

@article{bosker_using_2021,
	title = {Using fuzzy string matching for automated assessment of listener transcripts in speech intelligibility studies},
	volume = {53},
	doi = {10.3758/s13428-021-01542-4},
	abstract = {Many studies of speech perception assess the intelligibility of spoken sentence stimuli by means of transcription tasks (‘type out what you hear’). The intelligibility of a given stimulus is then often expressed in terms of percentage of words correctly reported from the target sentence. Yet scoring the participants’ raw responses for words correctly identified from the target sentence is a time-consuming task, and hence resource-intensive. Moreover, there is no consensus among speech scientists about what specific protocol to use for the human scoring, limiting the reliability of human scores. The present paper evaluates various forms of fuzzy string matching between participants’ responses and target sentences, as automated metrics of listener transcript accuracy. We demonstrate that one particular metric, the Token Sort Ratio, is a consistent, highly efficient, and accurate metric for automated assessment of listener transcripts, as evidenced by high correlations with human-generated scores (best correlation: r = 0.940) and a strong relationship to acoustic markers of speech intelligibility. Thus, fuzzy string matching provides a practical tool for assessment of listener transcript accuracy in large-scale speech intelligibility studies. See https://tokensortratio.netlify.app for an online implementation.},
	number = {5},
	journal = {Behavior Research Methods},
	author = {Bosker, Hans Rutger},
	year = {2021},
	pages = {1945--1953},
	file = {Bosker - 2021 - Using fuzzy string matching for automated assessme.pdf:D\:\\HRBfiles\\Zotero Files\\storage\\F33HGLRM\\Bosker - 2021 - Using fuzzy string matching for automated assessme.pdf:application/pdf},
}

@inproceedings{bujok_visible_2022,
	title = {Visible lexical stress cues on the face do not influence audiovisual speech perception},
	url = {https://www.isca-speech.org/archive/speechprosody_2022/bujok22_speechprosody.html},
	doi = {10.21437/SpeechProsody.2022-53},
	abstract = {Producing lexical stress leads to visible changes on the face, such as longer duration and greater size of the opening of the mouth. Research suggests that these visual cues alone can inform participants about which syllable carries stress (i.e., lipreading silent videos). This study aims to determine the influence of visual articulatory cues on lexical stress perception in more naturalistic audiovisual settings.},
	language = {en},
	urldate = {2022-06-02},
	booktitle = {Speech {Prosody} 2022},
	publisher = {ISCA},
	author = {Bujok, Ronny and Meyer, Antje and Bosker, Hans Rutger},
	editor = {Frota, Sónia and Cruz, Marisa and Vigário, Marina},
	month = may,
	year = {2022},
	keywords = {audiovisual, articulation, prosody, beat gestures, lexical stress, gesture, lipreading, 2afc, lips},
	pages = {259--263},
	file = {Bujok et al. - 2022 - Visible lexical stress cues on the face do not inf.pdf:D\:\\HRBfiles\\Zotero Files\\storage\\GKKEYXDK\\Bujok et al. - 2022 - Visible lexical stress cues on the face do not inf.pdf:application/pdf},
}

@inproceedings{severijnen_acoustic_2022,
	title = {Acoustic correlates of {Dutch} lexical stress re-examined: {Spectral} tilt is not always more reliable than intensity},
	shorttitle = {Acoustic correlates of {Dutch} lexical stress re-examined},
	url = {https://www.isca-speech.org/archive/speechprosody_2022/severijnen22_speechprosody.html},
	doi = {10.21437/SpeechProsody.2022-57},
	abstract = {The present study examined two acoustic cues in the production of lexical stress in Dutch: spectral tilt and overall intensity. Sluijter and Van Heuven (1996) reported that spectral tilt is a more reliable cue to stress than intensity. However, that study included only a small number of talkers (10) and only syllables with the vowels /aː/ and /ɔ/.},
	language = {en},
	urldate = {2022-06-02},
	booktitle = {Speech {Prosody} 2022},
	publisher = {ISCA},
	author = {Severijnen, Giulio and Bosker, Hans Rutger and McQueen, James},
	editor = {Frota, Sónia and Cruz, Marisa and Vigário, Marina},
	month = may,
	year = {2022},
	keywords = {prosody, production, spectral tilt, f0, lexical stress, duration, intensity, corpus, lda, linear discriminant analysis},
	pages = {278--282},
	file = {Severijnen et al. - 2022 - Acoustic correlates of Dutch lexical stress re-exa.pdf:D\:\\HRBfiles\\Zotero Files\\storage\\N5BT494L\\Severijnen et al. - 2022 - Acoustic correlates of Dutch lexical stress re-exa.pdf:application/pdf},
}
