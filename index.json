[{"authors":["giulio-severijnen"],"categories":null,"content":"Giulio Severijnen is a PhD student at the Donders Center for Cognition [DCC], part of the Donders Institute at Radboud University, Nijmegen, The Netherlands. His supervisors are Prof. James McQueen and Dr. Hans Rutger Bosker. His project investigates between-talker and within-talker variability in prosody production, with a specific focus on lexical stress. Moreover, he also tests how listeners flexibly adapt to this variability in order to successfully comprehend different talkers. The project is funded through a ‘Donders Internal PhD Round’ grant, awarded to Prof. James McQueen, Dr. Hans Rutger Bosker, and Dr. Ashley Lewis.\n","date":1640995200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1640995200,"objectID":"326756edd9bfed8d3d7aa32573218298","permalink":"https://hrbosker.github.io/author/giulio-severijnen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/giulio-severijnen/","section":"authors","summary":"Giulio Severijnen is a PhD student at the Donders Center for Cognition [DCC], part of the Donders Institute at Radboud University, Nijmegen, The Netherlands. His supervisors are Prof. James McQueen and Dr.","tags":null,"title":"Giulio Severijnen","type":"authors"},{"authors":null,"categories":null,"content":"Hans Rutger Bosker is Assistant Professor at the Donders Center for Cognition [DCC], part of the Donders Institute at Radboud University, Nijmegen, The Netherlands. He leads the Speech Perception in Audiovisual Communication [SPEAC] research group, funded by the ERC Starting Grant ‘HearingHands’ [101040276] that started in September 2022. This research grant investigates the contribution of simple up-and-down hand movements (‘beat gestures’) to audiovisual speech perception. Hans Rutger Bosker is also Senior Investigator at the Max Planck Institute for Psycholinguistics, Nijmegen.\nORCID iD: 0000-0002-2628-7738.\nWondering how to pronounce my name? I have a double first name “Hans Rutger” (no hyphen), “Bosker” is the surname. It’s pronounced /ɦɑns ˈʀʏt.xəɹ ˈbɔs.kəɹ/ in Dutch but for the less phonetically savvy among us, here’s Google pronouncing my name (with rather depressed prosody). But feel free to simply call me “Hans”.\n","date":1640995200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1640995200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://hrbosker.github.io/author/hans-rutger-bosker/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/hans-rutger-bosker/","section":"authors","summary":"Hans Rutger Bosker is Assistant Professor at the Donders Center for Cognition [DCC], part of the Donders Institute at Radboud University, Nijmegen, The Netherlands. He leads the Speech Perception in Audiovisual Communication [SPEAC] research group, funded by the ERC Starting Grant ‘HearingHands’ [101040276] that started in September 2022.","tags":null,"title":"Hans Rutger Bosker","type":"authors"},{"authors":["ronny-bujok"],"categories":null,"content":"Ronny Bujok is a PhD student at the Max Planck Institute for Psycholinguistics [MPI], Nijmegen, The Netherlands. His supervisors are Prof. Antje S. Meyer and Dr. Hans Rutger Bosker. His project investigates how simple up-and-down hand gestures (‘beat gestures’) influence low-level speech perception.\n","date":1640995200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1640995200,"objectID":"9ad7ef51d4fe5b9fe1fa4062f61d116c","permalink":"https://hrbosker.github.io/author/ronny-bujok/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ronny-bujok/","section":"authors","summary":"Ronny Bujok is a PhD student at the Max Planck Institute for Psycholinguistics [MPI], Nijmegen, The Netherlands. His supervisors are Prof. Antje S. Meyer and Dr. Hans Rutger Bosker. His project investigates how simple up-and-down hand gestures (‘beat gestures’) influence low-level speech perception.","tags":null,"title":"Ronny Bujok","type":"authors"},{"authors":["ivy-mok"],"categories":null,"content":"Ivy Mok is enrolled at Radboud University in the MA Linguistics program. She is currently writing her MA thesis at the SPEAC research group, under supervision of Dr. Hans Rutger Bosker and Ronny Bujok. Her project investigates audiovisual prosody perception in noise, with a focus on beat gestures.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"c3404aabaaa0d65aba38d717d8c4d911","permalink":"https://hrbosker.github.io/author/ivy-mok/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ivy-mok/","section":"authors","summary":"Ivy Mok is enrolled at Radboud University in the MA Linguistics program. She is currently writing her MA thesis at the SPEAC research group, under supervision of Dr. Hans Rutger Bosker and Ronny Bujok.","tags":null,"title":"Ivy Mok","type":"authors"},{"authors":["orhun-ulusahin"],"categories":null,"content":"Orhun Uluşahin is a PhD student at the Max Planck Institute for Psycholinguistics [MPI], Nijmegen, The Netherlands. His supervisors are Prof. Antje S. Meyer, Prof. James McQueen, and Dr. Hans Rutger Bosker. His project targets the production-perception interface, using phonetic convergence as the main paradigm. Specifically, he tests whether familiarity with a talker modulates the extent to which interlocutors converge towards each other on a phonetic level.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8bbf000eee2b19fa67856597ac935bd4","permalink":"https://hrbosker.github.io/author/orhun-ulusahin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/orhun-ulusahin/","section":"authors","summary":"Orhun Uluşahin is a PhD student at the Max Planck Institute for Psycholinguistics [MPI], Nijmegen, The Netherlands. His supervisors are Prof. Antje S. Meyer, Prof. James McQueen, and Dr. Hans Rutger Bosker.","tags":null,"title":"Orhun Uluşahin","type":"authors"},{"authors":null,"categories":null,"content":" Table of Contents NiCLS PiNCeR Other corpora Voice recordings are privacy-sensitive data; please use them respectfully and for academic purposes only. NiCLS Hans Rutger Bosker, Martin Cooke (2020). Enhanced amplitude modulations contribute to the Lombard intelligibility benefit: Evidence from the Nijmegen Corpus of Lombard Speech. The Journal of the Acoustical Society of America, 147(2), 721-730, doi:10.1121/10.0000646. PDF Cite Dataset DOI Nijmegen Corpus of Lombard Speech [NiCLS] Lead author: Hans Rutger Bosker. https://hdl.handle.net/1839/21ee5744-b5dc-4eed-9693-c37e871cdaf6 Dutch 42 talkers (37 F/5 M); each reading a folk story of 56 utterances; in both plain speech (read in quiet) and Lombard speech (read in noise over headphones) 3968 wav files (1984 Lombard-plain pairs) + forced-alignment TextGrids CC BY-NC-ND 4.0 license PiNCeR Joe Rodd, Hans Rutger Bosker, Mirjam Ernestus, Phillip M. Alday, Antje S. Meyer, Louis ten Bosch (2020). Control of speaking rate is achieved by switching between qualitatively distinct cognitive ‘gaits’: Evidence from simulation. Psychological Review 127*(2), 281-304, doi:10.1037/rev0000172. PDF Cite Dataset DOI Picture Naming at Cued Rates [PiNCeR] corpus Lead author: Joe Rodd. https://hdl.handle.net/1839/7c210d30-bb55-4cbe-9eeb-baf18570460c Dutch 25 talkers (21 F/4 M) naming disyllabic pictures arranged on a ‘clock face’; produced at three different cued rates: fast, medium, slow with (manual) word-level and (forced-aligned) syllable-level annotations; with eye-tracking data CC BY 4.0 license Other corpora Many other speech, video, and picture corpora are publicly available nowadays. Please see our Other resources for some examples that we ourselves have used in the past.\n","date":1657152000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657152000,"objectID":"ab1257ee950262edc01075a4ef751f4c","permalink":"https://hrbosker.github.io/resources/corpora/","publishdate":"2022-07-07T00:00:00Z","relpermalink":"/resources/corpora/","section":"resources","summary":"Open collections of speech recordings.","tags":null,"title":"Corpora","type":"book"},{"authors":null,"categories":null,"content":" Table of Contents Token Sort Ratio PraatVSCode POnSS Headphone screening tests Token Sort Ratio Table showing example TSR scores for various responses Hans Rutger Bosker (2021). Using fuzzy string matching for automated assessment of listener transcripts in speech intelligibility studies. Behavior Research Methods, 53(5), 1945-1953, doi:10.3758/s13428-021-01542-4. PDF Cite Dataset DOI Token Sort Ratio: automatically quantifying response accuracy for speech intelligiblity experiments. Author: Hans Rutger Bosker. https://tokensortratio.netlify.app The Token Sort Ratio [TSR] score is a fuzzy string matching metric that – at the most basic level – quantifies the orthographic match between a target string and a response string (value between 0 = no match and 100 = perfect match). The TSR score has been shown to strongly correlate with human-generated scores of percentage words correct (Bosker, 2021). It is an efficient, reliable, and accurate tool for use in speech perception research (e.g., studies that examine the perception of speech in adverse listening conditions, or degraded speech) or for generating listener intelligibility measures in clinical disciplines such as speech-language pathology or audiology. PraatVSCode PraatVSCode syntax highlighting Author: Orhun Uluşahin. https://github.com/orhunulusahin/praatvscode Praat is an excellent software package for speech analysis, annotation, and manipulation. However, it’s scripting interface is - let’s put it this way - ‘suboptimal’. PraatVSCode is an extension for Visual Studio Code (see screenshot) that provides syntax highlighting, autocompletion, and even an array of code snippets that writes itself. Moreover, it allows running and debugging of the script by Praat from inside Visual Studio Code. How to install: Download and install Visual Studio Code. Under View \u0026gt; Extensions, search for PraatVSCode, and click install. MIT license POnSS POnSS annotation workflow screenshots Joe Rodd, Caitlin Decuyper, Hans Rutger Bosker, Louis ten Bosch (2021). A tool for efficient and accurate segmentation of speech data: Announcing POnSS. Behavior Research Methods, 53, 744-756, doi:10.3758/s13428-020-01449-6. PDF Cite Dataset DOI Pipeline for Online Speech Segmentation [POnSS] Lead author: Joe Rodd. https://git.io/Jexj3 POnSS is a browser-based system that is specialized for the task of segmenting the onsets and offsets of words, that combines automatic speech recognition (WebMAUS) with limited human input. MIT license Headphone screening tests When running experiments online, you may want your participants to use headphones or in-ear buds (i.e., no speakers). Moreover, you may want to verify that they are wearing them ’the right way around’: [L] in their left ear, [R] in their right ear. This is particularly important when testing multitalker listening conditions and/or virtual auditory environments. Several tools exist to verify whether participants are using headphones (as instructed) or not (exclude ’m!), based on different psychoacoustic binaural phenomena. We have implemented these on Gorilla and PsyToolkit. Tone attenuation based on phase-cancellation Woods, K. J. P., Siegel, M. H., Traer, J., \u0026amp; McDermott, J. H. (2017). Headphone screening to facilitate web-based auditory experiments. Attention, Perception, \u0026amp; Psychophysics, 79(7), 2064–2072. doi:10.3758/s13414-017-1361-2\nGeneral idea: binaural tones are played and participants are asked to indicate which tone out of three is quietest. Some binaural tones are played 180° out-of-phase, attenuating perceived loudness if using speakers, but not when using headphones/in-ear buds. 3min test, shared by authors on Github We have implemented this headphone screening test on Gorilla and PsyToolkit. Send us an email and we’d be happy to share! Huggins Pitch illusion Milne, A. E., Bianco, R., Poole, K. C., Zhao, S., Oxenham, A. J., Billig, A. J., \u0026amp; Chait, M. (2021). An online headphone screening test based on dichotic pitch. Behavior Research Methods, 53(4), 1551–1562. doi:10.3758/s13428-020-01514-0\nGeneral idea: participants are played white noise in one ear and the same white noise but with a phase shift of 180° over a narrow frequency band to the other ear. This results in the perception of a faint tone embedded in the noise but only when using headphones/in-ear buds. Otherwise, listeners only perceive white noise (i.e., without the faint embedded tone). 3min test, shared by authors on Gorilla We have also implemented this headphone screening test on PsyToolkit. Send us an email and we’d be happy to share! ITD and ILD manipulations General idea: participants are played six trials of three binaural white noise sounds. Interaural time differences (ITDs) and interaural level differences (ILDs) are applied to the L/R channels of the stereo stimuli such that two noise sounds are perceived as coming from the left, and one as coming from the right. Participants indicate which out of the three white noise sounds comes from the …","date":1657152000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657152000,"objectID":"e33bb1b9ecf7be816c053c63be363458","permalink":"https://hrbosker.github.io/resources/tools/","publishdate":"2022-07-07T00:00:00Z","relpermalink":"/resources/tools/","section":"resources","summary":"Open research tools for data collection, annotation, and analysis.","tags":null,"title":"Research tools","type":"book"},{"authors":null,"categories":null,"content":" Table of Contents Praat syntax List of available scripts Helpful links License The Praat scripts shared here are first and foremost intended as a lab archive, providing script snippets we frequently use. This means they require customization for each individual new project. Use at your own risk! Praat syntax Over the years, Praat has had three types of syntax:\nExtract part... 0 0.1 rectangular 1 no (before Praat 5.3.44, Apr 2013) do(\u0026#34;Extract part\u0026#34;, 0, 0.1, \u0026#34;rectangular\u0026#34;, 1, \u0026#34;no\u0026#34;) (before Praat 5.3.63, Jan 2014) Extract part: 0, 0.1, \u0026#34;rectangular, 1, \u0026#34;no\u0026#34; (current) Current Praat versions are compatible with older and newer syntax types, and mixes thereof. The scripts shared here primarily use the first type of syntax, with occasional lines using the latest type of syntax. [ADVERTISEMENT:] Did you know the syntax highlighting in PraatVSCode works with either syntax?\nList of available scripts Save all Praat can only save one object at a time for you. If you have multiple objects in your object window you’d like to save in one go, you can use this script.\nAnnotate This script streamlines an annotation workflow: it presents a TextGrid for manual annotation to the user, you perform some changes, and when you click Next, it automatically saves the changes and presents the next TextGrid, and so on.\nBatch processing This script is an in-house template / starting point for batch processing multiple files. Adapt it to your own needs to apply a particular function to multiple files or multiple time intervals within each file.\nHelpful links Not finding what you were looking for? There are thousands of other Praat scripts available online. Three resources with particularly useful code are:\nVocal Toolkit plugin is a plugin for Praat. When installed, you can call various new functions from a button within Praat. However, it’s a little risky if you don’t know the ins-and-outs of a particular function, so always check the raw code here: [WINDOWS] “C:\\Users\\(Username)\\Praat\\plugin_VocalToolkit” [MAC] “/Users/(UserName)/Library/Preferences/Praat Prefs/” Matt Winn’s Listen Lab with some really fun Youtube Praat tutorials Holger Mitterer’s website License All scripts are shared under an MIT license.\n2022, Hans Rutger Bosker\n","date":1657152000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657152000,"objectID":"3c695000c9960ffc206501e907fecded","permalink":"https://hrbosker.github.io/resources/scripts/","publishdate":"2022-07-07T00:00:00Z","relpermalink":"/resources/scripts/","section":"resources","summary":"Open Praat scripts for speech analysis, manipulation, and synthesis","tags":null,"title":"Scripts","type":"book"},{"authors":null,"categories":null,"content":" Table of Contents Placeholder1 List of available how-to’s Placeholder2 License The how-to’s shared here are first and foremost intended as a lab archive, providing some starting ground for tasks and documents we often deal with. This means they require customization for each individual new project. Use at your own risk! Placeholder1 Body text.\nList of available how-to’s First how-to This is the subtitle. Second how-to This is the subtitle. Third how-to This is the subtitle. Placeholder2 Body text.\nLicense All documents are shared under an MIT license.\n2022, Hans Rutger Bosker\n","date":1657152000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657152000,"objectID":"177cb2ca635117e123be4e5ebd30ec44","permalink":"https://hrbosker.github.io/resources/how-to/","publishdate":"2022-07-07T00:00:00Z","relpermalink":"/resources/how-to/","section":"resources","summary":"Guidelines and templates for research-related tasks","tags":null,"title":"How to's","type":"book"},{"authors":null,"categories":null,"content":" Table of Contents Video/audio editing Corpora Writing tools Online experimenting Mailing lists Video/audio editing Praat praat.org the number 1 speech-editing software in academia supports speech measurements, annotation in TextGrids, manipulation, synthesis scripting interface supports batch processing recommended scripting interface: PraatVSCode ffmpeg ffmpeg.org a command line tool for batch video processing ’lives’ in the terminal (e.g., command prompt) Adobe Premiere Pro is a great video-editor when working on individual files, but is not the best solution for batch processing. ffmpeg is great at efficiently and quickly extracting the audio channels from a large set of video files, converting mpg to mp4, manipulating audio/video temporal alignment (asynchrony), etc. MediaPipe mediapipe.dev 2D video motion-tracking tool in Python, developed by Google input: video file of a single person (OpenPose is preferred for multi-person tracking). output: x, y, (estimated) z coordinates of body landsmarks + video with superimposed tracking skeleton. here’s a great tutorial by Wim Pouw and James Trujillo at Envision Bootcamp. WebMAUS https://clarin.phonetik.uni-muenchen.de/BASWebServices/interface/WebMAUSBasic forced-alignment tool taking wav files and txt files with orthographic transcripts as input, providing TextGrids as output ’lives’ online: you upload wav and txt files and download TextGrids large set of languages available annotations at word-level and at phone-level (not syllable-level) EasyAlign http://latlcui.unige.ch/phonetique/easyalign.php forced-alignment tool taking wav files and txt files with orthographic transcripts as input, providing TextGrids as output ’lives’ in Praat (plugin) French, English, Spanish, Brazilian Portuguese, Taiwan Min annotations at word-level, syllable-level and phone-level Corpora MultiPic https://www.bcbl.eu/databases/multipic/ standardized set of 750 drawings with multilingual name agreement and visual complexity norms in color; 300 x 300 pixels; DPI=96 pixels/inch Spanish, English (British), German, Italian, French, Dutch (Belgium), Dutch (Netherlands) also useful when looking for ‘imageable’ words Severens et al. https://www.ugent.be/pp/experimentele-psychologie/en/research/documents/pnn/overview.htm timed naming norms for 590 pictures in Belgian Dutch black-and-white line drawings name agreement, freq, aoa, h-statistic, naming latencies SUBTLEX http://crr.ugent.be/programs-data/subtitle-frequencies/subtlex-nl database of Dutch word frequencies based on 44 million words from film and television subtitles also available for other languages, including US English (SUBTLEX-US), UK English (SUBTLEX-UK), Mandarin Chinese (SUBTLEX-CH), Spanish (SUBTLEX-ESP), German (SUBTLEX-DE), Greek (SUBTLEX-GR), Polish (SUBTLEX-PL), Italian (SUBTLEX-IT), Brazilian Portuguese (SUBTLEX-PT-BR) reliable predictor of lexical decision reaction times, outperforming Google Books Ngram=1 (Brysbaert et al., 2011) ANW Algemeen Nederlands Woordenboek Dutch word list, allowing searching with regular expressions (“spraa*”) and with particular word characteristics (number of syllables, stress on syllable n, etc.) NOTE. I’ve found that the search lists are not exhaustive. Failure to find certain words in ANW does not necessarily mean they do not exist. Lombard speech corpora Acted clear speech corpus: English; 1 male talker; ’normal’ sentences; 25 items; babble-modulated noise; Mayo et al. (2012), doi:10.7488/ds/138. Hurricane natural speech corpus: English; 1 male talker; Harvard sentences (720 items) and MRT sentences (300 items); speech-modulated noise; Cooke et al. (2013), doi:10.7488/ds/140. DELNN: L1 Dutch and L2 English from 30 native speakers of Dutch (+9 native speakers of US English as control); speech-shaped noise; Marcoux (2022, PhD thesis). RaLoCo: Dutch; 78 talkers; 48 sentences; speech-shaped noise; Shen (2022, PhD thesis). Additional info, including human listening effort ratings and HEGP scores (spectral glimpsing metric of intelligibility). Also see our very own NiCLS corpus of Lombard speech. Writing tools Thesaurus thesaurus.com for looking up synonyms indispensable gizmo when scribbling palimpsests, particularly useful for L2 writers of English like myself also gives antonyms, example sentences, and related words links to definitions at dictionary.com Zotero zotero.org reference manager use the Zotero Connector in your browser to store a paper, including fulltext and all bibliographic specs use the Zotero Word Plugin to cite papers in a Word document, automatically generating a bibliography at the end of the document easily change bibliography styles (from author-year in APA to numbered-lists in IEEE) supports open fulltext search, notes and tags, organize in folders, etc. preferred over other reference managers because Zotero is institute-independent, free, open-source, and very flexible. Overleaf Overleaf Online LaTeX editor it’s like Google Docs but then in LaTeX collaborative …","date":1657152000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657152000,"objectID":"45016db0825c98667af5f5e841b09e50","permalink":"https://hrbosker.github.io/resources/other-resources/","publishdate":"2022-07-07T00:00:00Z","relpermalink":"/resources/other-resources/","section":"resources","summary":"Open research resources that others have shared and we frequently use.","tags":null,"title":"Other resources","type":"book"},{"authors":null,"categories":null,"content":"Do you hear VOORnaam or voorNAAM? In the video below, you’ll see a (randomly selected…) talker say a Dutch word. Is he saying VOORnaam (Eng. “first name”, with stress on the first syllable VOOR-) or voorNAAM (Eng. “respectable”, with stress on the second syllable -NAAM).\nIn other words: where do you hear the stress?\nSpoiler: click here to reveal the video specs Audio: ambiguous; midway between VOORnaam – voorNAAM Lips: head taken from a recording of VOORnaam Hands: beat gesture aligned to the first syllable Now play the video below. Where do you hear the stress now?\nSpoiler: click here to reveal the video specs Audio: ambiguous; midway between VOORnaam – voorNAAM Lips: head taken from a recording of VOORnaam Hands: beat gesture aligned to the second syllable Explanation The audio in these videos is perfectly identical: it has been manipulated to be ambiguous, falling roughly midway between VOORnaam and voorNAAM. The head of the talker is also the same: it has been copy-pasted from a video recording of the talker saying VOORnaam. The only difference between these two videos is the timing of the hand gesture. In the first clip, the talker produces a beat gesture on the first syllable, while in the second video the talker gestures on the second syllable. Our experiments show that this slight change in timing has major consequences for perception. When we ask a group of Dutch participants to indicate what word they hear the talker say, the majority reports hearing VOORnaam in the first clip, but voorNAAM in the second clip.\nReally? Convince me… This is Figure 1 from Bosker \u0026amp; Peeters (2021). In the bottom left panel, you see the proportion of ‘I hear stress on the first syllable’ responses for when the beat gesture falls on the first syllable (blue line) or on the second syllable (red line). The blue line lies above the red line, indicating an overall bias to report more ‘stress on first syllable’ responses when the gesture falls on the first vs. second syllable. The difference between the lines is sizable, averaging around 20%.\nFigure 1 in Bosker \u0026amp; Peeters (2021) How hands help us hear When we have a face-to-face conversation, we don’t only exchange sounds. We also move our head, hands, and body to the rhythm of the speech. Beat gestures are relatively ‘simple’ up-and-down hand gestures that are closely aligned to the rhythm of speech. They tend to fall on the stressed syllable in free-stress languages, such as English and Dutch. These videos demonstrate that people are sensitive to the timing of beat gestures, influencing lexical stress perception. In Bosker \u0026amp; Peeters (2021), this effect was termed the manual McGurk effect. That is, just like seeing a talker close their lips can make you hear the sound /b/ in the classic McGurk effect (McGurk \u0026amp; McDonald, 1976), so can the timing of hand gestures influence speech perception in the manual McGurk effect.\nWhy is this important? The manual McGurk effect is the first demonstration of how the timing of hand gestures influences low-level speech perception. Even the simplest flicks-of-the-hands that do not convey any particular meaning of themselves can shape what words you hear. This promises that these seemingly unimportant hand gestures contribute meaningfully to audiovisual speech comprehension. Perhaps ’enriching’ our speech with carefully timed gestures can help our audience understand our spoken message, particularly in challenging listening conditions, such as in noise.\nRelevant papers Hans Rutger Bosker, David Peeters (2021). Beat gestures influence which speech sounds you hear. Proceedings of the Royal Society B: Biological Sciences, 288, 20202419, doi:10.1098/rspb.2020.2419. PDF Cite Dataset DOI Ronny Bujok, Antje S. Meyer, Hans Rutger Bosker (2022). Visible lexical stress cues on the face do not influence audiovisual speech perception. In Proceedings of Speech Prosody 2022 (ed. S. Frota, M. Cruz, and M. Vigário), 259-263, doi:10.21437/SpeechProsody.2022-53. PDF Cite Dataset DOI Ronny Bujok, Antje S. Meyer, and Hans Rutger Bosker (2022). Audiovisual perception of lexical stress: Beat gestures are stronger visual cues for lexical stress than visible articulatory cues on the face. PsyArXiv Preprints, doi:10.31234/osf.io/y9jck, data:https://osf.io/4d9w5/\n","date":1657584000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657584000,"objectID":"f4f8cbb39ecc8b613476e3bda59a298d","permalink":"https://hrbosker.github.io/demos/manual-mcgurk/","publishdate":"2022-07-12T00:00:00Z","relpermalink":"/demos/manual-mcgurk/","section":"demos","summary":"How hands help us hear...","tags":null,"title":"Manual McGurk effect","type":"demos"},{"authors":null,"categories":null,"content":"This is the subtitle.\n1-2 hours per week, for 8 weeks\npraatcode = 1 Some disclaimer\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3d26ec3a92970bcc0c9447401568700e","permalink":"https://hrbosker.github.io/resources/how-to/howto1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resources/how-to/howto1/","section":"resources","summary":"This is the subtitle.\n","tags":null,"title":"First how-to","type":"book"},{"authors":null,"categories":null,"content":"Praat can only save one object at a time for you. If you have multiple objects in your object window you’d like to save in one go, you can use this script. It can either save objects by their object name or by their id number.\nYou can also download the script as a .praat file.\n################################################################################ ### Hans Rutger Bosker, Radboud University ### HansRutger.Bosker@ru.nl ### Date: 23 June 2022, run in Praat 6.2.12 on Windows 10 ### License: CC BY-NC 4.0 ################################################################################ ###\u0026gt;\u0026gt; This script saves all selected objects to the directory \u0026#39;dir_out$\u0026#39; ###\u0026gt;\u0026gt;\twith either: ###\u0026gt;\u0026gt; - their object name (e.g., \u0026#34;sentence1.wav\u0026#34;) ###\u0026gt;\u0026gt;\t\u0026gt; set variable \u0026#39;save_method$\u0026#39; to \u0026#34;name\u0026#34; [default] ###\u0026gt;\u0026gt; - their id number in the Praat object window (e.g., \u0026#34;42.wav\u0026#34;) ###\u0026gt;\u0026gt;\t\u0026gt; set variable \u0026#39;save_method$\u0026#39; to \u0026#34;id\u0026#34; ###\u0026gt;\u0026gt; ###\u0026gt;\u0026gt; Sounds are saved as .wav files, ###\u0026gt;\u0026gt; other object types (TextGrids, Spectrum, etc.) are saved ###\u0026gt;\u0026gt; with their own extension type (.TextGrid, .Spectrum). ###\u0026gt;\u0026gt; ###\u0026gt;\u0026gt; Default: the script will overwrite pre-existing files. ###\u0026gt;\u0026gt; Set variable \u0026#39;overwrite$\u0026#39; to \u0026#34;no\u0026#34; if you want Praat ###\u0026gt;\u0026gt; to throw an error instead. ################################################################################ ### Variables you will definitely need to customize: ################################################################################ ### Where should the selected objects be saved? dir_out$ = \u0026#34;C:\\Users\\hanbos\\mysounds\u0026#34; ### Should Praat overwrite pre-existing files? overwrite$ = \u0026#34;yes\u0026#34; #overwrite$ = \u0026#34;no\u0026#34; ### Do you want to save each object by its object name or by its id number? ### If object name, then use \u0026#34;name\u0026#34; (e.g., \u0026#34;sentence1.wav\u0026#34;). ### If object id number, then use \u0026#34;id\u0026#34; (e.g., \u0026#34;42.wav\u0026#34;). save_method$ = \u0026#34;name\u0026#34; #save_method$ = \u0026#34;id\u0026#34; ################################################################################ ### Before we start, let\u0026#39;s check whether you\u0026#39;ve entered sensible ### input for the variables above... ################################################################################ ### Let\u0026#39;s check if the output directory exists. ### This script will throw an error if the directory doesn\u0026#39;t exist ### (i.e., it won\u0026#39;t write to a mysterious temp directory). ### First check whether the input directory ends in a backslash (if so, removed) if right$(dir_out$,1)=\u0026#34;/\u0026#34; dir_out$ = left$(dir_out$,length(dir_out$)-1) elsif right$(dir_out$,1)=\u0026#34;\\\u0026#34; dir_out$ = left$(dir_out$,length(dir_out$)-1) endif ### Then create a temporary txt file in the folder ### and try to write it to the output folder. ### NOTE: The \u0026#34;nocheck\u0026#34; below asks Praat not to complain if the folder ### does *not* exist. We\u0026#39;ll manually check whether the saving of this ### temp txt file has succeeded or not further down below. temp_filename$ = dir_out$ + \u0026#34;/\u0026#34; + \u0026#34;my_temporary_Praat_file.txt\u0026#34; nocheck writeFileLine: temp_filename$, \u0026#34;This is just to check if the directory exists\u0026#34; ### Can the file be found? file_exists_yesno = fileReadable(temp_filename$) if file_exists_yesno = 1 # if you *could* read that temp txt file, # this confirms that the directory is valid. # Then you can delete it. deleteFile: temp_filename$ else # if that file wasn\u0026#39;t readable, that means that the directory wasn\u0026#39;t valid. printline The folder \u0026#39;dir_out$\u0026#39; was not found exit Your directory doesn\u0026#39;t exist. Check spelling. The directory must *already* exist. endif ################################################################################ ################################################################################ ################################# SCRIPT ################################# ################################################################################ ################################################################################ ### Make sure you\u0026#39;ve selected the objects you\u0026#39;d like to save in ### the Praat object window. If nothing is selected, the script exits. nSelected = numberOfSelected() if nSelected = 0 exit No objects selected. endif ### Store the object id numbers in an array for thisObject to nSelected objectArray [\u0026#39;thisObject\u0026#39;] = selected(\u0026#39;thisObject\u0026#39;) endfor ### Loop through this array and for each id number ### select the corresponding object and save it. for thisArrayNumber to nSelected objectId = objectArray [\u0026#39;thisArrayNumber\u0026#39;] select \u0026#39;objectId\u0026#39; type$ = extractWord$(selected$(), \u0026#34;\u0026#34;) name$ = extractLine$(selected$(), \u0026#34; \u0026#34;) if save_method$ = \u0026#34;name\u0026#34; if type$ = \u0026#34;Sound\u0026#34; does_file_exist = fileReadable(\u0026#34;\u0026#39;dir_out$\u0026#39;\\\u0026#39;name$\u0026#39;.wav\u0026#34;) if does_file_exist = 1 if overwrite$ = \u0026#34;no\u0026#34; exit The file \u0026#39;dir_out$\u0026#39;\\\u0026#39;name$\u0026#39;.wav\u0026#39; already exists! If you wish to overwrite, set the variable overwrite$ to \u0026#34;yes\u0026#34;. endif endif Write to WAV file... \u0026#39;dir_out$\u0026#39;\\\u0026#39;name$\u0026#39;.wav else does_file_exist = fileReadable(\u0026#34;\u0026#39;dir_out$\u0026#39;\\\u0026#39;name$\u0026#39;.\u0026#39;type$\u0026#39;\u0026#34;) if does_file_exist = 1 if overwrite$ = \u0026#34;no\u0026#34; exit The file \u0026#39;dir_out$\u0026#39;\\\u0026#39;name$\u0026#39;.\u0026#39;type$\u0026#39; already exists! If you wish to overwrite, set the variable overwrite$ to \u0026#34;yes\u0026#34;. …","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c3295f7839cdac0638abc97e77bc14ea","permalink":"https://hrbosker.github.io/resources/scripts/save-all/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resources/scripts/save-all/","section":"resources","summary":"Praat can only save one object at a time for you. If you have multiple objects in your object window you’d like to save in one go, you can use this script.","tags":null,"title":"Save all","type":"book"},{"authors":null,"categories":null,"content":"Remember this one? Do you remember this famous quote, starting at 00:15s?\nThis was Neil Armstrong, landing on the moon on July 20, 1969. But hold on, what is he saying exactly? Let’s listen to the first part of his famous quote:\nIs it:\n“one small step for man”, or: “one small step for a man”? Both transcripts are grammatically viable, but they mean different things. In (1), “man” is used as a synonym of “mankind”, while in (2) “man” is used with the meaning of “person”. Only the second transcript actually fits the remainder of the quote (\u0026#34;…one giant leap for mankind\u0026#34;) and Neil Armstrong himself also claimed that he had uttered the second version (Baese-Berk et al., 2016). But how come people miss the “a” in the recording?\nSpeech is messy When we talk, we don’t produce ‘spaces’ between words. Instead, we join all the words together, producing a connected stream of sound. This is especially true for function words, like “a”, “or”, “for”, and “and”. It is quite likely that Neil Armstrong intended to say “for a man”, but in stringing together the sounds for the words, the “a” was ’lost’ in articulation. This is what speech scientists call ‘reductions’ in speech.\nTaking speech rate into account OK, so the “a” is ‘reduced’ in the original pronunciation. But human perception is not only determined by the input signal alone. Instead, it is heavily context-dependent, taking into account such contextual factors as who is talking, why he is talking, and even how fast the speech is likely to come in!\nEvidence for this comes from ‘context effects’, whereby for instance the acoustic characteristics of a preceding sentence can influence what you hear next. Let’s take speech rate for example. If you hear someone say “That’s one small step…” at a really fast tempo, it is very likely that the next few words will be spoken at a fast rate too. And conversely: if someone happens to speak at a slow rate, the next few sounds will likely be slow too. This means people are likely to interpret the next few sounds in line with the speech rate of the preceding sentence.\nConjuring up the “a” Now let’s see if we can make the “a” in Neil Armstrong’s quote appear and disappear by playing around with the speech rate of only the surrounding speech. Note that we’re not changing anything about the “for (a)” part of the audio clip (highlighted in red in video below). All we’ll do is speed up (compressed by 2) or slow down (compressed by 0.5) the surrounding parts of the recording. Do you hear “for man” or “for a man”?\nClick here to access the audio from the video ORIGINAL CLIP\nCONTEXT SLOWED DOWN\nCONTEXT SPED UP\nWhat happened there? Most listeners will report hearing “for man” in the bottom clip, with the slowed-down context, but “for a man” in the top clip, with the sped-up context. Notably, these clips have the exact same “for a” part; they only differ in the speech rate of the surrounding context. The slow context makes listeners predict that the “for (a)” part was uttered at a slow rate too. But this critical “for (a)” does not contain a really slow “a”, so people ‘miss’ the function word. However, in a fast context, listeners expect the critical “for (a)” to be uttered at a fast rate. This “for (a)” does indeed (kinda) match a really fast and short “a”, so people are more likely to report hearing “for a man”.\nWhy is this important? Context effects abound in perception, and so they also surface in speech perception. People interpret speech sounds differently depending on the surrounding speech rate, formants, and perceived pitch. But even non-acoustic aspects of the context are taken into account: people hear a sound differently depending on the talker’s gender, the talker’s hand gestures, one’s own preceding speech, and there’s even reports that a stuffed toy seen in the background can change what you hear (Hay \u0026amp; Drager, 2010). All these phenomena together shape human speech perception. So if we want Automatic Speech Recognition (the Siri’s, Cortana’s, and Alexa’s of this world) to approach human-like behavior, we need to know each and every aspect that defines what words we (think we) hear. Oh, and apparently it also helps extraterrestrial communication.\nRelevant papers Hans Rutger Bosker (2017). How our own speech rate influences our perception of others. Journal of Experimental Psychology: Learning, Memory, and Cognition, 43(8), 1225-1238, doi:10.1037/xlm0000381. PDF Cite DOI Hans Rutger Bosker (2017). Accounting for rate-dependent category boundary shifts in speech perception. Attention, Perception \u0026amp; Psychophysics, 79, 333-343, doi:10.3758/s13414-016-1206-4. PDF Cite DOI ","date":1657497600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657497600,"objectID":"685c4d1fd8b797d0268cf144ce7ae724","permalink":"https://hrbosker.github.io/demos/conjuring-words/","publishdate":"2022-07-11T00:00:00Z","relpermalink":"/demos/conjuring-words/","section":"demos","summary":"\\\"*That's one small step for (a?) man...*\\\"","tags":null,"title":"Conjuring up words that were never spoken","type":"demos"},{"authors":null,"categories":null,"content":"This script streamlines an annotation workflow: it presents a TextGrid for manual annotation to the user, you perform some changes, and when you click Next, it automatically saves the changes and presents the next TextGrid, and so on. This is particularly useful for when you have forced aligned TextGrids (e.g., from WebMAUS or EasyAlign) that you’d like to manually evaluate and edit.\nMoreover, the script keeps track of who annotated what, can continue where you left off yesterday, allows users to enter comments about their annotations, and blinds file names to avoid human annotation biases. The script can be updated to present new empty TextGrids (instead of any pre-existing ones, in case you only have .wav files) or to automatically perform changes to TextGrid tiers/intervals before presenting them for manual annotation.\nYou can also download the script as a .praat file.\n################################################################################ ### Hans Rutger Bosker, Radboud University ### HansRutger.Bosker@ru.nl ### Date: 30 June 2022, run in Praat 6.2.12 on Windows 10 ### License: CC BY-NC 4.0 ################################################################################ ###\u0026gt;\u0026gt; This script reads a directory containing sound files with pre-existing TextGrids, ###\u0026gt;\u0026gt;\tfor instance resulting from a forced aligner (e.g., WebMAUS or EasyAlign). ###\u0026gt;\u0026gt;\tIMPORTANT: Every Sound should have a pre-existing TextGrid file **with the same name**! ###\u0026gt;\u0026gt;\tIt opens every Sound + Textgrid combination, presents it to the user for editing, ###\u0026gt;\u0026gt;\tallows the user to enter comments about the annotations, and then saves the ###\u0026gt;\u0026gt;\tedited TextGrid with \u0026#34;_edited\u0026#34; suffix in the subfolder \u0026#39;edited_textgrids\u0026#39;. ###\u0026gt;\u0026gt;\tUser comments are tracked in the file \u0026#39;annotation_log.txt\u0026#39; in the same subfolder. ###\u0026gt;\u0026gt;\t###\u0026gt;\u0026gt;\t\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; If you **do not** yet have pre-existing TextGrids (i.e., only sound files),\t\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; ###\u0026gt;\u0026gt;\t\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; you can adjust the script to read all .wav files, create new empty TextGrids,\t\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; ###\u0026gt;\u0026gt;\t\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; and present those for editing and saving...\t\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; ###\u0026gt;\u0026gt;\t\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; See the line with \u0026#34;CREATE EMPTY TEXTGRIDS\u0026#34;\t\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; ###\u0026gt;\u0026gt; ###\u0026gt;\u0026gt; This script can be run by multiple users simultaneously, for instance when ###\u0026gt;\u0026gt; multiple annotators are working on the same shared folder. It keeps track ###\u0026gt;\u0026gt; of what files have already been edited: it only presents TextGrids for editing ###\u0026gt;\u0026gt; that do not yet have an \u0026#34;_edited\u0026#34; version pre-existing in the subfolder. ###\u0026gt;\u0026gt; This also means that users can close the script or Praat at anytime ###\u0026gt;\u0026gt; without losing data. Then, next time someone runs the script, it will ###\u0026gt;\u0026gt; start with the files that are \u0026#39;left over\u0026#39; from the previous run. ###\u0026gt;\u0026gt;\tNOTE: This checking of which files already exist can slow the script down ###\u0026gt;\u0026gt;\twhen working with folders with \u0026gt;5000 files... ###\u0026gt;\u0026gt; ###\u0026gt;\u0026gt; At present, the script **only** presents pre-existing tiers and intervals ###\u0026gt;\u0026gt; for editing (e.g., adding boundaries, dragging boundaries around, etc.). ###\u0026gt;\u0026gt; This script can be augmented by automatically adding tiers or intervals ###\u0026gt;\u0026gt; before the TextGrid is presented for editing, so users can annotate ###\u0026gt;\u0026gt; new tiers/intervals. See the line with \u0026#34;ADD/REMOVE TIERS HERE\u0026#34;. ################################################################################ ### Variables you will definitely need to customize: ################################################################################ ### Where can the Sound and TextGrid files be found? dir_in$ = \u0026#34;C:\\Users\\hanbos\\mysounds\u0026#34; ### Do you want to use \u0026#39;blinded\u0026#39; objects in Praat to avoid human biases in annotation? ### Default value: \u0026#34;yes\u0026#34; ### Change to \u0026#34;no\u0026#34; if you want to use original object names. blinded$ = \u0026#34;yes\u0026#34; ################################################################################ ### Before we start, let\u0026#39;s check whether you\u0026#39;ve entered sensible ### input for the variables above... ################################################################################ ### Let\u0026#39;s check if the input directory exists. ### This script will throw an error if the directory doesn\u0026#39;t exist ### (i.e., it won\u0026#39;t write to a mysterious temp directory). ### First check whether the input directory ends in a backslash (if so, removed) if right$(dir_in$,1)=\u0026#34;/\u0026#34; dir_in$ = left$(dir_in$,length(dir_in$)-1) elsif right$(dir_in$,1)=\u0026#34;\\\u0026#34; dir_in$ = left$(dir_in$,length(dir_in$)-1) endif ### Then create a temporary txt file in the folder ### and try to write it to the input folder. ### NOTE: The \u0026#34;nocheck\u0026#34; below asks Praat not to complain if the folder ### does *not* exist. We\u0026#39;ll manually check whether the saving of this ### temp txt file has succeeded or not further down below. temp_filename$ = dir_in$ + \u0026#34;/\u0026#34; + \u0026#34;my_temporary_Praat_file.txt\u0026#34; nocheck writeFileLine: temp_filename$, \u0026#34;This is just to check if the directory exists\u0026#34; ### Can the file be found? file_exists_yesno = fileReadable(temp_filename$) if file_exists_yesno = 1 # if you *could* read that temp txt file, # this confirms that the directory is valid. # Then you can …","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"87e2dde545e138e121782d0c7c595fac","permalink":"https://hrbosker.github.io/resources/scripts/annotate/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resources/scripts/annotate/","section":"resources","summary":"This script streamlines an annotation workflow: it presents a TextGrid for manual annotation to the user, you perform some changes, and when you click Next, it automatically saves the changes and presents the next TextGrid, and so on.","tags":null,"title":"Annotate","type":"book"},{"authors":null,"categories":null,"content":"This is the subtitle.\n1-2 hours per week, for 8 weeks\nfirstobject = 2 Some disclaimer\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"eea908ef4e679ffd106ce0e4720f2339","permalink":"https://hrbosker.github.io/resources/how-to/howto2/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resources/how-to/howto2/","section":"resources","summary":"This is the subtitle.\n","tags":null,"title":"Second how-to","type":"book"},{"authors":null,"categories":null,"content":"How fast can your ears go? Listen to this clip of a talker saying the telephone number 496-0356…\n496-0356 – [original] You’ll probably have no problem understanding the same digits when it’s compressed by a factor of 2 (i.e., twice as fast)…\n496-0356 – compressed by 2 And when it’s compressed by a factor of 3?\n496-0356 – compressed by 3 And compressed by 4?\n496-0356 – compressed by 4 Or even by 5?\n496-0356 – compressed by 5 Research has demonstrated that compression rates up to 3 are still doable (kinda…) but intelligibility breaks down quite dramatically for higher compression rates (e.g., Ghitza, 2014; Bosker \u0026amp; Ghitza, 2018). So the last two clips are unintelligible to most listeners.\nThis has been suggested to be due to how our brain works. Our brain is known to ’track’ incoming speech by aligning its ‘brain waves’ (neural oscillations in the theta range, 3-9 Hz) to the syllable rhythm of the speech (amplitude modulations in the temporal envelope). But when the syllables come in too rapidly (\u0026gt;9 Hz), the brain waves can’t keep up, resulting in poor intelligibility.\nMaking unintelligible speech intelligible again But there’s a trick to help the brain keep up. Let’s take the unintelligible clip with the telephone number 496-0356 compressed by a factor of 5. Here it is again:\n496-0356 – compressed by 5 That’s tough, right? No wonder with a syllable rate of over 12 Hz!\nNow let’s chop this clip up into short snippets of 66 ms (“packages”, cf. top panel in the figure at the top of this page) and space them apart by 100 ms (i.e., inserting silent intervals). This brings the package rate down to around 6 Hz. Can your brain keep up with that?\n496-0356 – repackaged What wizardry! What was unintelligible before is made (more…) intelligible by adding some ‘breathing space’ for the brain. Note that the speech signal itself did not change: it is the same acoustic content as before, but just presented at a slower pace so your brain can keep up!\nAnd now the exam! Here is a new telephone number, also consisting of 7 digits. Can you tell me what the last four digits are?\nClick here to see the correct answer… 0592\nAnd what about this one?\nClick here to see the correct answer… 0137\nPresumably everybody has a hard time correctly hearing these digits, because these are again recordings that have been compressed by a factor of 5!\nBut now try these:\nClick here to see the correct answer… 0723\nClick here to see the correct answer… 0164\nThat probably sounded much more intelligible. These are two examples of ‘repackaged speech’: first compressed by a factor of 5, chopped up into 66 ms snippets, and then spaced apart by 100 ms. And your brain was presumably very grateful for that extra breathing space (“my pleasure, brain…”).\nWhy is this important? This phenomenon can tell us something about what acoustic aspects support speech intelligibility. If we know what aspects of speech are critical for proper intelligibility, then that knowledge would be helpful, for instance, (i) for speech synthesizers, such as Automatic Announcement Systems in public transport, to generate speech signals that human listeners can understand well, (ii) for hearing aids to ’enrich’ incoming speech signals and present those optimized signals to the listening brain, or (iii) for communication with the elderly who often experience difficulty with speech perception, especially in noise.\nRelevant papers Hans Rutger Bosker, Oded Ghitza (2018). Entrained theta oscillations guide perception of subsequent speech: Behavioral evidence from rate normalization. Language, Cognition and Neuroscience,33(8), 955-967, doi:10.1080/23273798.2018.1439179. PDF Cite DOI ","date":1657238400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657238400,"objectID":"bc474ee88d85da53d3377d1b86d37eb5","permalink":"https://hrbosker.github.io/demos/repackaging/","publishdate":"2022-07-08T00:00:00Z","relpermalink":"/demos/repackaging/","section":"demos","summary":"Making unintelligible speech intelligible again...","tags":null,"title":"Repackaging speech","type":"demos"},{"authors":null,"categories":null,"content":"This script is an in-house template / starting point for batch processing multiple files. Adapt it to your own needs to apply a particular function to multiple files or multiple time intervals within each file.\nIn its current form, the script reads each .wav file plus accompanying TextGrid in a given input directory, extracts all non-empty intervals individually, and then loops over those to find the ones labelled “vowel”. It then allows the user to apply a particular function to those intervals (such as Scale intensity: 65), after which it concatenates the individual intervals back together, and saves the output in an output directory.\nNOTE: In its current form, the script does not run any function on its input. It really only serves as a starting point, including snippets of code we regularly use and now do not need to look up every time we want to do batch processing in Praat.\nYou can also download the script as a .praat file.\n################################################################################ ### Hans Rutger Bosker, Radboud University ### HansRutger.Bosker@ru.nl ### Date: 6 July 2022, run in Praat 6.2.12 on Windows 10 ### License: CC BY-NC 4.0 ################################################################################ ###\u0026gt;\u0026gt; This script is a starting point for batch processing a set of files. ###\u0026gt;\u0026gt;\tThe script basically reads files in an input directory and runs a ###\u0026gt;\u0026gt;\ta to-be-defined function [see \u0026#39;Perform your function here\u0026#39; below] ###\u0026gt;\u0026gt;\tand writes the output to an output directory . This saves me having ###\u0026gt;\u0026gt;\tto look up how to create a file list, how to loop over files, etc. ###\u0026gt;\u0026gt;\t###\u0026gt;\u0026gt; Since this was basically intended for in-house use, I\u0026#39;ve added in bits ###\u0026gt;\u0026gt;\tand pieces that I find useful to have ready-to-go, such as: ###\u0026gt;\u0026gt;\t\u0026#39;beginPause\u0026#39; for manually specifying variables. ################################################################################ ### Variables you will definitely need to customize: ################################################################################ ### Where can the files be found? dir_in$ = \u0026#34;C:\\Users\\hanbos\\mysounds\u0026#34; ### Where should the output files be saved? dir_out$ = \u0026#34;C:\\Users\\hanbos\\mysounds\\output\u0026#34; ################################################################################ ### Let\u0026#39;s check whether the directories specified above exist... ################################################################################ ### Let\u0026#39;s check if the input directory exists. ### This script will throw an error if the directory doesn\u0026#39;t exist ### (i.e., it won\u0026#39;t write to a mysterious temp directory). ### First check whether the input directory ends in a backslash (if so, removed) if right$(dir_in$,1)=\u0026#34;/\u0026#34; dir_in$ = left$(dir_in$,length(dir_in$)-1) elsif right$(dir_in$,1)=\u0026#34;\\\u0026#34; dir_in$ = left$(dir_in$,length(dir_in$)-1) endif ### Then create a temporary txt file in the folder ### and try to write it to the input folder. ### NOTE: The \u0026#34;nocheck\u0026#34; below asks Praat not to complain if the folder ### does *not* exist. We\u0026#39;ll manually check whether the saving of this ### temp txt file has succeeded or not further down below. temp_filename$ = dir_in$ + \u0026#34;/\u0026#34; + \u0026#34;my_temporary_Praat_file.txt\u0026#34; nocheck writeFileLine: temp_filename$, \u0026#34;This is just to check if the directory exists\u0026#34; ### Can the file be found? file_exists_yesno = fileReadable(temp_filename$) if file_exists_yesno = 1 # if you *could* read that temp txt file, # this confirms that the directory is valid. # Then you can delete it. deleteFile: temp_filename$ else # if that file wasn\u0026#39;t readable, that means that the directory wasn\u0026#39;t valid. printline The folder \u0026#39;dir_in$\u0026#39; was not found exit Your input directory doesn\u0026#39;t exist. Check spelling. The directory must *already* exist. endif ## Now re-do this for the output directory: if right$(dir_out$,1)=\u0026#34;/\u0026#34; dir_out$ = left$(dir_out$,length(dir_out$)-1) elsif right$(dir_out$,1)=\u0026#34;\\\u0026#34; dir_out$ = left$(dir_out$,length(dir_out$)-1) endif ### Then create a temporary txt file in the folder ### and try to write it to the input folder. ### NOTE: The \u0026#34;nocheck\u0026#34; below asks Praat not to complain if the folder ### does *not* exist. We\u0026#39;ll manually check whether the saving of this ### temp txt file has succeeded or not further down below. temp_filename$ = dir_out$ + \u0026#34;/\u0026#34; + \u0026#34;my_temporary_Praat_file.txt\u0026#34; nocheck writeFileLine: temp_filename$, \u0026#34;This is just to check if the directory exists\u0026#34; ### Can the file be found? file_exists_yesno = fileReadable(temp_filename$) if file_exists_yesno = 1 # if you *could* read that temp txt file, # this confirms that the directory is valid. # Then you can delete it. deleteFile: temp_filename$ else # if that file wasn\u0026#39;t readable, that means that the directory wasn\u0026#39;t valid. printline The folder \u0026#39;dir_out$\u0026#39; was not found exit Your output directory doesn\u0026#39;t exist. Check spelling. The directory must *already* exist. endif ########################################################################### ##\tFORM TO MANUALLY SPECIFY VARIABLES …","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3b365b10c943f6e7d7f2a5d9dd414149","permalink":"https://hrbosker.github.io/resources/scripts/batch-processing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resources/scripts/batch-processing/","section":"resources","summary":"This script is an in-house template / starting point for batch processing multiple files. Adapt it to your own needs to apply a particular function to multiple files or multiple time intervals within each file.","tags":null,"title":"Batch processing","type":"book"},{"authors":null,"categories":null,"content":"This is the subtitle.\n1-2 hours per week, for 8 weeks\nfirstobject = 3 Some disclaimer\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3ff40f2e8673682469ca91c7ea3788f7","permalink":"https://hrbosker.github.io/resources/how-to/howto3/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resources/how-to/howto3/","section":"resources","summary":"This is the subtitle.\n","tags":null,"title":"Third how-to","type":"book"},{"authors":null,"categories":null,"content":"A very dull cocktail party In everday life, we often encounter situations where there’s more than just one talker speaking, like having a conversation in a busy bar, listening to a presenter at a crowded poster session, or talking to someone on the phone while walking on the street. Somehow our brain has little trouble honing in on that one talker we want to attend to, while ignoring others. How do we that?\nSpeech researchers like ourselves tend to investigate this research question by creating terribly dull ‘cocktail parties’. These cocktail parties include one listener, two talkers (A and B), and a lamentable lack of any cocktails. Still, such a situation does allow the researcher to play with particular auditory and visual cues to see if that helps or hinders the listener to attend to Talker A and ignore Talker B.\nWhy call this a ‘cocktail party’ in the first place? Colin Cherry came up with ’the cocktail party problem’ in 1953: “how do we recognize what one person is saying when others are speaking at the same time”. His experiments involved playing speech from two different talkers over the same speaker, covering such topics as “the really complex nature of the causes and uses of birds’ colors” and about how “religious convictions, legal systems, and politics have been so successful in accomplishing their aims”. How prof. Cherry arrived at the term ‘cocktail party’ in light of these topics remains an outright mystery. Scientists and parties…\nThe time between your ears One such cue is the interaural time difference (ITD). This is the difference in arrival time of a sound between two ears. Imagine Talker A is talking on your left, while Talker B is talking on your right. Their speech needs to travel through the air before reaching your ears, which takes (a very short amount of) time. Their opposite locations in space mean that the speech of Talker A will hit your left ear a fraction of a second earlier than your right ear. Likewise, the speech of Talker B will hit your right ear earlier than your left ear. Remarkably, your brain can use this difference in arrival time (ITD) to locate speakers in space, helping you to separate the speech from Talker A from the speech from Talker B.\nWe can try to construct a situation where ITD is the only cue to speakers’ locations in space, creating a virtual auditory scene:\nPut on your headphones/ear buds Do not use speakers; this works better with headphones Let’s take two recordings: one from a female Google, another from a male Alexa FEMALE GOOGLE MALE ALEXA Now let’s simply mix these two recordings: GOOGLE/ALEXA MIX When listening to this mixture, we can already notice three things:\nThey’re lying. In defiance of their words, the positioning of the two talkers in (virtual) space is acutally identical. Most listeners would judge the two talkers as being positioned ‘right in front of them’, or even ’talking inside their heads’. That is because both voices reach both of your ears instantaneously. The audio clip is a mono file with only one channel, containing the mixed speech from both talkers. The browser sends this single channel to both sides of your headphones (if all went well…), so the female speech reaches your left ear at the same point in time as it reaches your right ear (ITD = 0 ms), and the same for the male speech. In reality, this hardly ever happens (if at all), unless a speaker is standing perfectly in front of you.\nOur brains are remarkable. Despite the unrealistic virtual positions of the two talkers, we can still freely direct our attention to the female talker (and ignore the male talker), or to the male talker (and ignore the female talker). Apparently, we can use other cues (i.e., other than ITDs) to attend one talker and ignore others, such as their (markedly different) pitch.\nThis truly is a dull party…\nVirtual reality Now let’s see if we can move these two talkers around in virtual space. Imagine the female talker is on your left and the male is on your right. This would mean that the female speech reaches your left ear before it reaches your right ear. And vice versa: the male speech would reach your right ear before reaching your left.\nWe can mimick this by applying ITDs to the individual speech signals. First, we take the mono clip above and copy it to another channel, resulting in a stereo clip with two identical channels (see illustration in the clip below). Second, we take the female speech (given in red below) and delay it in the right channel by 0.0006 seconds = 0.6 milliseconds = 600 μs. Finally, we take the male speech (in blue) and delay it in the left channel by the same minute amount of time. Now we have a stereo clip with opposite ITDs for the two talkers!\nIn the clip below, you’ll first hear the ‘old’ mixture we had initially, followed by the ’new’ clip with manipulated ITDs. Can you hear the difference?\nBut I unexpectedly hear the talkers in the wrong locations! If you do hear the difference between the two clips in the video, …","date":1657152000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657152000,"objectID":"e70ded1bbf3c419375fe80859deb4bec","permalink":"https://hrbosker.github.io/demos/cocktail-party/","publishdate":"2022-07-07T00:00:00Z","relpermalink":"/demos/cocktail-party/","section":"demos","summary":"Trying to attend one talker, while ignoring others...","tags":null,"title":"Cocktail party listening","type":"demos"},{"authors":null,"categories":null,"content":"Listening test In the video below, you’ll see a simple display with four objects. First, see if you know each of the four objects. Then play the video. You’ll hear a female voice asking you to press a button for one of the objects (i.e., click on it). While watching and listening, try to keep track of where your eyes go in the display…\nDo you have any idea what the next word of the speaker will be? Probably not, right? Did you notice anything particular about where in the display your gaze was at? Since you probably didn’t know what object the speaker was going to name, chances are your eyes were all over the place.\nOK, next video. It’s the same display, but with a new audio recording. Have a look and see if you can tell which of the four objects the speaker selects…\nWell, in this case, you may already have a slight hunch, right? The speaker was hesitating at the end of her utterance, wasn’t she? Well, chances are this native speaker of British English won’t have much trouble naming common objects, like lion, ear, or bike, would she? So could it be that she’ll refer to the Italian moka pot in the top left?\nDisfluencies help you predict what’s coming up Natural speech is messy. We stumble over words, lose our line of thought, and produce tons of uhm’s and uh’s. Still, these kinds of disfluencies don’t occur randomly throughout an utterance. We are much more likely to stumble before rarely occurring (low-frequency), novel (not mentioned before), and complex (long) words than we are before common and simple words.\nInterestingly, human listeners seem to be aware of this. In our experiments, we presented listeners with displays like the ones above together with spoken instructions to click on one of the objects. While people were watching/listening, we recorded where they were looking on the screen using eye-tracking (see lab photo below). This allowed us to track their gaze on a millisecond time scale as the utterance unfolds. Results showed that when people heard the speaker hesitate, they were much more likely to look at a low-frequency object, like moka pot, compared to high-frequency objects (Bosker et al., 2014).\nEye-tracking lab at the MPI. (C) Max-Planck-Gesellschaft, https://www.mpi.nl/page/mpi-labs OK, let’s try again… Here’s another video, again with the same display, but another audio recording. Once again, have a listen and see if you can tell which object the speaker will name:\nIn the last few milliseconds of the clip, you may have discovered a glimpse of the object. Did she say “Now press the button for the li-…”? Does that mean we’ve finally figured out that it’ll be the lion after all?\nLet’s find out:\nAargh!\nFiller words can be misleading As mentioned before, speech is messy. We don’t only produce hesitations and disfluencies, but also litter our speech with seemingly meaningless filler words, such as ‘you know’, ‘well’, and (worst of all) ’like’. Our audience, in turn, is tasked with distilling from this chaos what we actually want to communicate.\nAnd that can be hard. Filler words share their sounds (phonology) with many other words. The filler ’like’ shares its initial sounds with words such as ’lion’, ’lime’, ’lice’, lightbulb’, etc. Our experiments have shown that listeners are actually considering these similar-sounding words (cohort competitor) when encountering ’like’. When presented with displays with one ‘cohort competitor’ (e.g., lion) and three distractors, participants were biased towards looking at the lion upon hearing “…for the like…”. This suggests that filler words, like “like” (see what I did there?), have an impact on the efficiency of word recognition (Bosker et al., 2021).\nWhy is this important? Eye-tracking can reveal the time-course of speech processing. It allows tracking people’s gaze with millisecond precision, often without participants themselves being aware of their own looking behavior. As such, it can show when in time certain acoustic and/or visual cues influence speech perception. That kind of temporal information has for instance been used to discriminate between different models of word recognition.\nRelevant papers Merel Maslowski, Antje S. Meyer, Hans Rutger Bosker (2020). Eye-tracking the time course of distal and global speech rate effects. Journal of Experimental Psychology: Human Perception and Performance, 46(10), 1148-1163, doi:10.1037/xhp0000838. PDF Cite Dataset DOI Hans Rutger Bosker, Esperanza Badaya, Martin Corley (2021). Discourse markers activate their, like, cohort competitors. Discourse Processes, 58(9), 837-851, doi:10.1080/0163853X.2021.1924000. PDF Cite Dataset DOI Hans Rutger Bosker, Marjolein van Os,, Rik Does, Geertje van Bergen (2019). Counting ‘uhm’s: how tracking the distribution of native and non-native disfluencies influences online language comprehension. Journal of Memory and Language, 106, 189-202, doi:10.1016/j.jml.2019.02.006. PDF Cite Dataset DOI Hans Rutger Bosker, Geertje van Bergen (2018). Linguistic expectation management …","date":1657065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657065600,"objectID":"e4e9399826b7013e174c41cdead91ce2","permalink":"https://hrbosker.github.io/demos/visual-world-paradigm/","publishdate":"2022-07-06T00:00:00Z","relpermalink":"/demos/visual-world-paradigm/","section":"demos","summary":"How your eyes betray what you think you hear...","tags":null,"title":"Look and listen","type":"demos"},{"authors":null,"categories":null,"content":"Same audio, different perception In May 2018, social media exploded after the surfacing of an audio clip that some perceived as Laurel, but others as Yanny. Listen and decide for yourself:\nLaurel/Yanny – [original] #Laurelgate was quickly seen as the auditory version of #TheDress, a photo going viral in 2015 of a white and gold dress, or was it black and blue? But how fixed is this divide between individuals? Can we turn #Yannists into #Laurelites, and vice versa?\nHigher vs. lower frequencies Acoustic analysis of the original clip suggests that the higher frequencies (\u0026gt;1000 Hz) resembled the word Yanny, but the lower frequencies (\u0026lt;1000 Hz) are more like Laurel. This can be seen in the figure at the top of this page, where the upper part of the middle panel (Original) is more like the right panel (Yanny), but the lower part is more like the left panel (Laurel). This is best demonstrated by artificially emphasizing/attenuating the higher vs. lower frequencies in the audio clip.\nIn these sounds below, we gradually attenuate (~turn down) the higher frequencies while we simultaneously emphasize (~turn up) the lower frequencies. Play the sounds below, can you hear Laurel turning into Yanny?\nClick here for audio specs Middle clip: original Laurel/Yanny clip Manipulation: filtered by 10 bandpass filters (with center frequencies: 31.5, 63, 125, 250, 500, 1000, 2000, 4000, 8000, 16000 Hz; using a Hann window with a roll-off width of 20, 20, 40, 80, 100, 100, 100, 100, 100, 100 Hz, respectively). Inverse intensity manipulation for high (\u0026gt;1000 Hz) vs. low (\u0026lt;1000 Hz) frequency bands in steps of 6 dB. Top clip: -18 dB attenuation for higher frequency bands, +18 dB emphasis for lower frequency bands. Bottom clip: +18 dB emphasis for higher frequency bands, -18 dB attenuation for lower frequency bands. OK, so we can guide what people hear by artificially editing the higher vs. lower frequencies in the clip. But can we also make someone hear one and the same clip differently?\nLet’s add Laurel’s telephone number The perception of speech sounds is influenced by the surrounding acoustic context. The same sound can be perceived differently when, for instance, the acoustics of a preceding sentence are changed. Below, you will hear the original Laurel/Yanny clip, but this time preceded by a telephone number: 496-0356. In the first clip, we filtered out (~removed) the lower frequencies in the telephone number leaving only the high frequency content. In the second clip, we filtered out the higher frequencies leaving only the low frequency content. Note: the Laurel/Yanny clip itself is identical in the two audios. Do you hear a different name after each telephone number?\nHigh-pass filtered telephone number + Laurel/Yanny Low-pass filtered telephone number + Laurel/Yanny Numbing your ears In a crowd-sourced experiment with \u0026gt;500 online participants, we found that the same people were more likely to report hearing Laurel for the first clip, but Yanny for the second clip. This is because the high-frequency content in the telephone number in the first clip ’numbs your ears’ for any following high-frequency content, thus making the lower frequencies stand out more, biasing perception towards Laurel. And vice versa, the low-frequency content in the telephone number in the second clip ’numbs your ears’ for any following low-frequency content, thus making the higher frequencies stand out more, biasing perception towards Yanny.\nReally? Convince me… This is Figure 1 from Bosker (2018, JASA) showing people’s responses in panel C. The blue line shows the proportion of Yanny responses after a high-pass filtered telephone number (~first clip above), which is higher than the red line illustrating people’s responses for the same Laurel/Yanny clips after a low-pass filtered telephone number (~second clip above).\nWhy is this important? These social media phenomena are great examples of how our perception of the world is strongly context-dependent. What we perceive is not wholly determined by the input signal alone, but also by the context in which the signal is perceived, including the sounds heard previously, our prior expectations, who is talking, etc. etc. As such, they highlight the subtle intricacies of human perception.\nRelevant papers Hans Rutger Bosker (2018). Putting Laurel and Yanny in context. The Journal of the Acoustic Society of America, 144(6), EL503-EL508, doi:10.1121/1.5070144. PDF Cite Dataset DOI ","date":1656979200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656979200,"objectID":"e501aee23aa3d94c5466896220f32ba6","permalink":"https://hrbosker.github.io/demos/laurel-yanny/","publishdate":"2022-07-05T00:00:00Z","relpermalink":"/demos/laurel-yanny/","section":"demos","summary":"Making you hear *Yanny* when previously you heard *Laurel*...","tags":null,"title":"Laurel or Yanny?","type":"demos"},{"authors":null,"categories":null,"content":"Let’s do a little test… First, let’s take care of your audio settings:\nput on your headphones/ear buds/speakers turn your volume way down play this sound… SOME WHITE NOISE… …and adjust your volume until it’s at a loud but still comfortable level. OK, now we’ll do a short reading test:\nplay the video below you’ll see a counter counting down from 3… …and then it will present a simple sentence on screen your task is simply to read out the sentence aloud. Ready? Go!\nWhy thank you! OK, now let’s do this again. Make sure to keep wearing your headphones, play the next video, and read out the sentence aloud.\nSurprise!\nWhat’s going on? Perhaps you noticed your voice sounding somewhat different the second time, when there was loud babble from 16 other talkers playing in your ears, compared to the first time (in quiet). This phenomenon is called Lombard speech (or: Lombard effect; Lombard reflex). It’s the type of speech people produce when speaking in noise.\nBut perhaps you didn’t quite hear yourself all too well because it’s hard to listen to your own voice when there’s other sounds around. So here’s two clips from a male speaker of British English (and a rather posh one, if I may say so…) giving you some really useful dietary advice. The first is from when he was speaking in quiet: this is called ‘plain speech’. The second clip is a recording of the same sentence but this time the talker heard loud noise over headphones, speaking in noise: ‘Lombard speech’.\nPLAIN SPEECH LOMBARD SPEECH retrieved from the Acted clear speech corpus\nWhat is Lombard speech? In the clips above, you can clearly hear the difference between ‘plain speech’ and ‘Lombard speech’. Lombard speech sounds louder, higher pitched, is a little slower, with more pronounced higher frequencies, and clearer vowels. In our own research, we demonstrated that Lombard speech is also more rhythmic, having a stronger ‘beat’ to it compared to plain speech (see refs below).\nLombard speech rulez Speech perception studies have demonstrated that these ‘vocal adjustments’ people make when speaking in noise actually have a purpose: they make you more intelligible! When you take the plain and Lombard clips above, scale their intensities to be exactly the same, and then mix them with loud babble, this is what you get:\nPLAIN SPEECH in BABBLE (SNR = -6 dB) LOMBARD SPEECH in BABBLE (SNR = -6 dB) You may experience that it’s easier to pick out the male target talker from the babble in the second (Lombard) clip than in the first (plain) clip. Apparently, ‘speaking up’ actually helps!\nWhy is this important? Lombard speech is more intelligible in noise than plain speech. This means that speech researchers can ‘borrow’ acoustic aspects of Lombard speech to boost speech intelligibility, for instance in hearing aids. So next time you wanna make sure your message comes across in that busy bar, you’d better boost your F0, raise your spectral tilt, and increase your vowel dispersion; got it?!\nRelevant papers Hans Rutger Bosker, Martin Cooke (2018). Talkers produce more pronounced amplitude modulations when speaking in noise. The Journal of the Acoustical Society of America, 143(2), EL121-EL126, doi:10.1121/1.5024404. PDF Cite DOI Hans Rutger Bosker, Martin Cooke (2020). Enhanced amplitude modulations contribute to the Lombard intelligibility benefit: Evidence from the Nijmegen Corpus of Lombard Speech. The Journal of the Acoustical Society of America, 147(2), 721-730, doi:10.1121/10.0000646. PDF Cite Dataset DOI ","date":1656892800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656892800,"objectID":"7d509f542847ddd8835f927d980b7ab2","permalink":"https://hrbosker.github.io/demos/lombard-speech/","publishdate":"2022-07-04T00:00:00Z","relpermalink":"/demos/lombard-speech/","section":"demos","summary":"How speaking up changes your voice...","tags":null,"title":"Lombard speech","type":"demos"},{"authors":["Giulio Severijnen","Hans Rutger Bosker","James M. McQueen"],"categories":null,"content":" ","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"c9b859ec7e1a4a133fec2bb356a2cba0","permalink":"https://hrbosker.github.io/publication/severijnen-etal-2022-speechprosody/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/severijnen-etal-2022-speechprosody/","section":"publication","summary":"The present study examined two acoustic cues in the production of lexical stress in Dutch: spectral tilt and overall intensity. Sluijter and Van Heuven (1996) reported that spectral tilt is a more reliable cue to stress than intensity. However, that study included only a small number of talkers (10) and only syllables with the vowels /aː/ and /ɔ/. The present study re-examined this issue in a larger and more variable dataset. We recorded 38 native speakers of Dutch (20 females) producing 744 tokens of Dutch segmentally overlapping words (e.g., *VOORnaam* vs. *voorNAAM*, “first name” vs. “respectable”), targeting 10 different vowels, in variable sentence contexts. For each syllable, we measured overall intensity and spectral tilt following Sluijter and Van Heuven (1996). Results from Linear Discriminant Analyses showed that, for the vowel /aː/ alone, spectral tilt showed an advantage over intensity, as evidenced by higher stressed/unstressed syllable classification accuracy scores for spectral tilt. However, when all vowels were included in the analysis, the advantage disappeared. These findings confirm that spectral tilt plays a larger role in signaling stress in Dutch /aː/ but show that, for a larger sample of Dutch vowels, overall intensity and spectral tilt are equally important.","tags":["lexical stress","spectral tilt","intensity","speech production","cue weighting","prosody"],"title":"Acoustic correlates of Dutch lexical stress re-examined: Spectral tilt is not always more reliable than intensity","type":"publication"},{"authors":["Hans Rutger Bosker"],"categories":null,"content":" ","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"f36be0149803aa3dc83a74b788b4123e","permalink":"https://hrbosker.github.io/publication/bosker-2022-ls/index/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-2022-ls/index/","section":"publication","summary":"Individuals vary in how they produce speech. This variability affects both the segments (vowels  and consonants) and the suprasegmental properties of their speech (prosody). Previous literature  has demonstrated that listeners can adapt to variability in how different talkers pronounce the  segments of speech. This study shows that listeners can also adapt to variability in how talkers  produce lexical stress. Experiment 1 demonstrates a selective adaptation effect in lexical stress  perception- repeatedly hearing Dutch trochaic words biased perception of a subsequent lexical stress continuum towards more iamb responses. Experiment 2 demonstrates a recalibrationeffect in lexical stress perception- when ambiguous suprasegmental cues to lexical stress were disambiguated by lexical orthographic context as signaling a trochaic word in an exposure phase, Dutch participants categorized a subsequent test continuum as more trochee-like. Moreover, the selective adaptation and recalibration effects generalized to novel words, not encountered during exposure. Together, the experiments demonstrate that listeners also flexibly adapt to variability in the suprasegmental properties of speech, thus expanding our understanding of the utility of listener adaptation in speech perception. Moreover, the combined outcomes speak for an architecture of spoken word recognition involving abstract prosodic representations at a prelexical level of analysis.","tags":["lexical stress","recalibration","selective adaptation","suprasegmental cues","prosody"],"title":"Evidence for selective adaptation and recalibration in the perception of lexical stress","type":"publication"},{"authors":["Hans Rutger Bosker"],"categories":null,"content":" ","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"95113500c13f59c48a820768e429702f","permalink":"https://hrbosker.github.io/publication/bosker-dummy/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-dummy/","section":"publication","summary":"Individuals vary in how they produce speech. This variability affects both the segments (vowels  and consonants) and the suprasegmental properties of their speech (prosody). Previous literature  has demonstrated that listeners can adapt to variability in how different talkers pronounce the  segments of speech. This study shows that listeners can also adapt to variability in how talkers  produce lexical stress. Experiment 1 demonstrates a selective adaptation effect in lexical stress  perception- repeatedly hearing Dutch trochaic words biased perception of a subsequent lexical stress continuum towards more iamb responses. Experiment 2 demonstrates a recalibrationeffect in lexical stress perception- when ambiguous suprasegmental cues to lexical stress were disambiguated by lexical orthographic context as signaling a trochaic word in an exposure phase, Dutch participants categorized a subsequent test continuum as more trochee-like. Moreover, the selective adaptation and recalibration effects generalized to novel words, not encountered during exposure. Together, the experiments demonstrate that listeners also flexibly adapt to variability in the suprasegmental properties of speech, thus expanding our understanding of the utility of listener adaptation in speech perception. Moreover, the combined outcomes speak for an architecture of spoken word recognition involving abstract prosodic representations at a prelexical level of analysis.","tags":["lexical stress","recalibration","selective adaptation","suprasegmental cues","prosody"],"title":"Evidence for selective adaptation and recalibration in the perception of lexical stress","type":"publication"},{"authors":["Ronny Bujok","Antje S. Meyer","Hans Rutger Bosker"],"categories":null,"content":" ","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"a1c6d07ddf19db624006b31cb28d4b6f","permalink":"https://hrbosker.github.io/publication/bujok-etal-2022-sp/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bujok-etal-2022-sp/","section":"publication","summary":"Producing lexical stress leads to visible changes on the face, such as longer duration and greater size of the opening of the mouth. Research suggests that these visual cues alone can inform participants about which syllable carries stress (i.e., lip-reading silent videos). This study aims to determine the influence of visual articulatory cues on lexical stress perception in more naturalistic audiovisual settings. Participants were presented with seven disyllabic, Dutch minimal stress pairs (e.g., VOORnaam [first name] \u0026 voorNAAM [respectable]) in audio-only (phonetic lexical stress continua without video), video-only (lip-reading silent videos), and audiovisual trials (e.g., phonetic lexical stress continua with video of talker saying VOORnaam or voorNAAM). Categorization data from video-only trials revealed that participants could distinguish the minimal pairs above chance from seeing the silent videos alone. However, responses in the audiovisual condition did not differ from the audio-only condition. We thus conclude that visual lexical stress information on the face, while clearly perceivable, does not play a major role in audiovisual speech perception. This study demonstrates that clear unimodal effects do not always generalize to more naturalistic multimodal communication, advocating that speech prosody is best considered in multimodal settings.","tags":["lexical stress","audiovisual speech","articulatory cues","spoken-word recognition","prosody"],"title":"Visible lexical stress cues on the face do not influence audiovisual speech perception","type":"publication"},{"authors":["Joe Rodd","Caitlin Decuyper","Hans Rutger Bosker","Louis ten Bosch"],"categories":null,"content":" ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"a946f68c4cabe846a081b5edb6eef52a","permalink":"https://hrbosker.github.io/publication/rodd-etal-2021-brm/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/rodd-etal-2021-brm/","section":"publication","summary":"Despite advances in automatic speech recognition (ASR), human input is still essential for producing research-grade segmentations of speech data. Conventional approaches to manual segmentation are very labor-intensive. We introduce POnSS, a browser-based system that is specialized for the task of segmenting the onsets and offsets of words, which combines aspects of ASR with limited human input. In developing POnSS, we identified several sub-tasks of segmentation, and implemented each of these as separate interfaces for the annotators to interact with to streamline their task as much as possible. We evaluated segmentations made with POnSS against a baseline of segmentations of the same data made conventionally in Praat. We observed that POnSS achieved comparable reliability to segmentation using Praat, but required 23% less annotator time investment. Because of its greater efficiency without sacrificing reliability, POnSS represents a distinct methodological advance for the segmentation of speech data.","tags":["speech data","segmentation","research tool"],"title":"A tool for efficient and accurate segmentation of speech data: Announcing POnSS","type":"publication"},{"authors":["Hans Rutger Bosker","David Peeters"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"394cafb43835bfa97485f69172aac2cd","permalink":"https://hrbosker.github.io/publication/bosker-etal-2021-procroysocb/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2021-procroysocb/","section":"publication","summary":"Beat gestures—spontaneously produced biphasic movements of the hand are among the most frequently encountered co-speech gestures in human communication. They are closely temporally aligned to the prosodic characteristics of the speech signal, typically occurring on lexically stressed syllables. Despite their prevalence across speakers of the world’s languages, how beat gestures impact spoken word recognition is unclear. Can these simple ‘flicks of the hand’ influence speech perception? Across a range of experiments, we demonstrate that beat gestures influence the explicit and implicit perception of lexical stress (e.g. distinguishing ‘OBject’ from ‘obJECT’), and in turn can influence what vowels listeners hear. Thus, we provide converging evidence for a manual McGurk effect-relatively simple and widely occurring hand movements influence which speech sounds we hear.","tags":["audiovisual speech perception","manual McGurk effect","multimodal communication","beat gestures","lexical stress"],"title":"Beat gestures influence which speech sounds you hear","type":"publication"},{"authors":["Hans Rutger Bosker","Esperanza Badaya","Martin Corley"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"5be7cb5e80af5432428c7848acf38c78","permalink":"https://hrbosker.github.io/publication/bosker-etal-2021-discproc/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2021-discproc/","section":"publication","summary":"Speech in everyday conversations is riddled with discourse markers (DMs), such as well, you know, and like. However, in many lab-based studies of speech comprehension, such DMs are typically absent from the carefully articulated and highly controlled speech stimuli. As such, little is known about how these DMs influence online word recognition. The present study specifically investigated the online processing of DM like and how it influences the activation of words in the mental lexicon. We specifically targeted the cohort competitor (CC) effect in the Visual World Paradigm- Upon hearing spoken instructions to “pick up the beaker,” human listeners also typically fixate—next to the target object—referents that overlap phonologically with the target word (cohort competitors such as beetle; CCs). However, several studies have argued that CC effects are constrained by syntactic, semantic, pragmatic, and discourse constraints. Therefore, the present study investigated whether DM like influences online word recognition by activating its cohort competitors (e.g., lightbulb). In an eye-tracking  experiment using the Visual World Paradigm, we demonstrate that when participants heard spoken instructions such as “Now press the button for the, like... unicycle,” they showed anticipatory looks to the CC referent (lightbulb) well before hearing the target. This CC effect was sustained for a relatively long period of time, even despite hearing disambiguating information (i.e., the /k/ in like). Analysis of the reaction times also showed that participants were significantly faster to select CC targets (lightbulb) when preceded by DM like. These findings suggest that seemingly trivial DMs, such as like, activate their CCs, impacting online word recognition. Thus, we advocate a more holistic perspective on spoken language comprehension in naturalistic communication, including the processing of DMs.","tags":["cohort competitor effect","fillers","disfluency","word recognition","eye-tracking"],"title":"Discourse markers activate their, like, cohort competitors","type":"publication"},{"authors":["Giulio Severijnen","Hans Rutger Bosker","Piai Vitoria","James M. McQueen"],"categories":null,"content":" ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"cb4f47f2920130ed3a45ed6cf95e641f","permalink":"https://hrbosker.github.io/publication/severijnen-etal-2021-brainres/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/severijnen-etal-2021-brainres/","section":"publication","summary":"One of the challenges in speech perception is that listeners must deal with considerable segmental and suprasegmental variability in the acoustic signal due to differences between talkers. Most previous studies have focused on how listeners deal with segmental variability. In this EEG experiment, we investigated whether listeners track talker-specific usage of suprasegmental cues to lexical stress to recognize spoken words correctly. In a three-day training phase, Dutch participants learned to map non-word minimal stress pairs onto different object referents (e.g., USklot meant “lamp”; usKLOT meant “train”). These non-words were produced by two male talkers. Critically, each talker used only one suprasegmental cue to signal stress (e.g., Talker A used only F0 and Talker B only intensity). We expected participants to learn which talker used which cue to signal stress. In the test phase, participants indicated whether spoken sentences including these non-words were correct (“The word for lamp is…”). We found that participants were slower to indicate that a stimulus was correct if the non-word was produced with the unexpected cue (e.g., Talker A using intensity). That is, if in training Talker A used F0 to signal stress, participants experienced a mismatch between predicted and perceived phonological word-forms if, at test, Talker A unexpectedly used intensity to cue stress. In contrast, the N200 amplitude, an event-related potential related to phonological prediction, was not modulated by the cue mismatch. Theoretical implications of these contrasting results are discussed. The behavioral findings illustrate talker-specific prediction of prosodic cues, picked up through perceptual learning during training.","tags":["prosody","perceptual learning","lexical stress","phonological prediction","N200"],"title":"Listeners track talker-specific prosody to deal with talker-variability","type":"publication"},{"authors":["Hans Rutger Bosker"],"categories":null,"content":" ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"afe100f101d4e4afb230d571951c139b","permalink":"https://hrbosker.github.io/publication/bosker-2021-bookchapter/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-2021-bookchapter/","section":"publication","summary":"Speech contains pronounced amplitude modulations in the 1–9 Hz range, correlating with the syllabic rate of speech. Recent models of speech perception propose that this rhythmic nature of speech is central to speech recognition and has beneficial effects on language processing. Here, we investigated the contribution of amplitude modulations to the subjective impression listeners have of public speakers. The speech from US presidential candidates Hillary Clinton and Donald Trump in the three TV debates of 2016 was acoustically analyzed by means of modulation spectra. These indicated that Clinton’s speech had more pronounced amplitude modulations than Trump’s speech, particularly in the 1–9 Hz range. A subsequent perception experiment, with listeners rating the perceived charisma of (low-pass filtered versions of) Clinton’s and Trump’s speech, showed that more pronounced amplitude modulations (i.e., more ‘rhythmic’ speech) increased perceived charisma ratings. These outcomes highlight the important contribution of speech rhythm to charisma perception.","tags":["amplitude modulations","speech rhythm","modulation spectrum","charisma perception","temporal envelope","amplitude envelope"],"title":"The contribution of amplitude modulations in speech to perceived charisma","type":"publication"},{"authors":["Hans Rutger Bosker"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"cb9629ab6e5f9f3fab618c91c8136a05","permalink":"https://hrbosker.github.io/publication/bosker-2021-brm/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-2021-brm/","section":"publication","summary":"Many studies of speech perception assess the intelligibility of spoken sentence stimuli by means of transcription tasks (‘type out what you hear’). The intelligibility of a given stimulus is then often expressed in terms of percentage of words correctly reported from the target sentence. Yet scoring the participants’ raw responses for words correctly identified from the target sentence is a time-consuming task, and hence resource-intensive. Moreover, there is no consensus among speech scientists about what specific protocol to use for the human scoring, limiting the reliability of human scores. The present paper evaluates various forms of fuzzy string matching between participants’ responses and target sentences, as automated metrics of listener transcript accuracy. We demonstrate that one particular metric, the token sort ratio, is a consistent, highly efficient, and accurate metric for automated assessment of listener transcripts, as evidenced by high correlations with human-generated scores (best correlation r = 0.940) and a strong relationship to acoustic markers of speech intelligibility. Thus, fuzzy string matching provides a practical tool for assessment of listener transcript accuracy in large-scale speech intelligibility studies. See https//tokensortratio.netlify.app for an online implementation.","tags":["speech intelligibility","fuzzy string matching","transcription accuracy","automated assessment","token sort ratio"],"title":"Using fuzzy string matching for automated assessment of listener transcripts in speech intelligibility studies","type":"publication"},{"authors":["Anne Kosem","Hans Rutger Bosker","Ole Jensen","Peter Hagoort","Lars Riecke"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"7e338625612b44260276d9898b481815","permalink":"https://hrbosker.github.io/publication/kosem-etal-2020-jcn/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/kosem-etal-2020-jcn/","section":"publication","summary":"Recent neuroimaging evidence suggests that the frequency of entrained oscillations in auditory cortices influences the perceived duration of speech segments, impacting word perception [Kösem, A., Bosker, H. R., Takashima, A., Meyer, A., Jensen, O., \u0026 Hagoort, P. Neural entrainment determines the words we hear. Current Biology, 28, 2867–2875, 2018]. We further tested the causal influence of neural entrainment frequency during speech processing, by manipulating entrainment with continuous transcranial alternating current stimulation (tACS) at distinct oscillatory frequencies (3 and 5.5 Hz) above the auditory cortices. Dutch participants listened to speech and were asked to report their percept of a target Dutch word, which contained a vowel with an ambiguous duration. Target words were presented either in isolation (first experiment) or at the end of spoken sentences (second experiment). We predicted that the tACS frequency would influence neural entrainment and therewith how speech is perceptually sampled, leading to a perceptual overestimation or underestimation of the vowel’s duration. Whereas results from Experiment 1 did not confirm this prediction, results from Experiment 2 suggested a small effect of tACS frequency on target word perception- Faster tACS leads to more long-vowel word percepts, in line with the previous neuroimaging findings. Importantly, the difference in word perception induced by the different tACS frequencies was significantly larger in Experiment 1 versus Experiment 2, suggesting that the impact of tACS is dependent on the sensory context. tACS may have a stronger effect on spoken word perception when the words are presented in continuous speech as compared to when they are isolated, potentially because prior (stimulus-induced) entrainment of brain oscillations might be a prerequisite for tACS to be effective.","tags":null,"title":"Biasing the perception of spoken words with transcranial alternating current stimulation","type":"publication"},{"authors":["Greta Kaufeld","Wibke Naumann","Antje S. Meyer","Hans Rutger Bosker","Andrea E. Martin}"],"categories":null,"content":" ","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"5f8bd0f6ae39380aa7439c7cbd900666","permalink":"https://hrbosker.github.io/publication/kaufeld-etal-2020-lcn/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/kaufeld-etal-2020-lcn/","section":"publication","summary":"Understanding spoken language requires the integration and weighting of multiple cues, and may call on cue integration mechanisms that have been studied in other areas of perception. In the current study, we used eye-tracking (visual-world paradigm) to examine how contextual speech rate (a lower-level, perceptual cue) and morphosyntactic knowledge (a higher-level, linguistic cue) are iteratively combined and integrated. Results indicate that participants used contextual rate information immediately, which we interpret as evidence of perceptual inference and the generation of predictions about upcoming morphosyntactic information. Additionally, we observed that early rate effects remained active in the presence of later conflicting lexical information. This result demonstrates that (1) contextual speech rate functions as a cue to morphosyntactic inferences, even in the presence of subsequent disambiguating information; and (2) listeners iteratively use multiple sources of information to draw inferences and generate predictions during speech comprehension. We discuss the implication of these demonstrations for theories of language processing.","tags":["language comprehension","speech perception","cue integration","morphology","rate normalization"],"title":"Contextual speech rate influences morphosyntactic prediction and integration","type":"publication"},{"authors":["Joe Rodd","Hans Rutger Bosker","Mirjam Ernestus","Phillip M. Alday","Antje S. Meyer","Louis ten Bosch"],"categories":null,"content":" ","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"af62ce577b2d57d5d13dcbc0db40028f","permalink":"https://hrbosker.github.io/publication/rodd-etal-2020-psychreview/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/rodd-etal-2020-psychreview/","section":"publication","summary":"That speakers can vary their speaking rate is evident, but how they accomplish this has hardly been studied. Consider this analogy: When walking, speed can be continuously increased, within limits, but to speed up further, humans must run. Are there multiple qualitatively distinct speech “gaits” that resemble walking and running? Or is control achieved by continuous modulation of a single gait? This study investigates these possibilities through simulations of a new connectionist computational model of the cognitive process of speech production, EPONA, that borrows from Dell, Burger, and Svec’s (1997) model. The model has parameters that can be adjusted to fit the temporal characteristics of speech at different speaking rates. We trained the model on a corpus of disyllabic Dutchwords produced at different speaking rates. During training, different clusters of parameter values (regimes) were identified for different speaking rates. In a 1-gait system, the regimes used to achieve fast and slow speech are qualitatively similar, but quantitatively different. In a multiple gait system, there is no linear relationship between the parameter settings associated with each gait, resulting in an abrupt shift in parameter values to move from speaking slowly to speaking fast. After training, the model achieved good fits in all three speaking rates. The parameter settings associated with each speaking rate were not linearly related, suggesting the presence of cognitive gaits. Thus, we provide the first computationally explicit account of the ability to modulate the speech production system to achieve different speaking styles.","tags":["speech production","speech rate","connectionist models","executive control","lexical access"],"title":"Control of speaking rate is achieved by switching between qualitatively distinct cognitive ‘gaits’: Evidence from simulation","type":"publication"},{"authors":["Hans Rutger Bosker","Martin Cooke"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"a41a7cc0e2d07199ea693b60e5b3d01e","permalink":"https://hrbosker.github.io/publication/bosker-etal-2020-jasa/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2020-jasa/","section":"publication","summary":"Speakers adjust their voice when talking in noise, which is known as Lombard speech. These acoustic adjustments facilitate speech comprehension in noise relative to plain speech (i.e., speech produced in quiet). However, exactly which characteristics of Lombard speech drive this intelligibility benefit in noise remains unclear. This study assessed the contribution of enhanced amplitude modulations to the Lombard speech intelligibility benefit by demonstrating that (1) native speakers of Dutch in the Nijmegen Corpus of Lombard Speech produce more pronounced amplitude modulations in noise vs in quiet; (2) more enhanced amplitude modulations correlate positively with intelligibility in a speech-in-noise perception experiment; (3) transplanting the amplitude modulations from Lombard speech onto plain speech leads to an intelligibility improvement, suggesting that enhanced amplitude modulations in Lombard speech contribute towards intelligibility in noise. Results are discussed in light of recent neurobiological models of speech perception with reference to neural oscillators phase-locking to the amplitude modulations in speech, guiding the processing of speech.","tags":["Lombard speech","speech in noise","amplitude modulations","prosody transplantation","neural entrainment"],"title":"Enhanced amplitude modulations contribute to the Lombard intelligibility benefit: Evidence from the Nijmegen Corpus of Lombard Speech","type":"publication"},{"authors":["Merel Maslowski","Antje S. Meyer","Hans Rutger Bosker"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"076057a4bbefc492977ad8053af494e8","permalink":"https://hrbosker.github.io/publication/maslowski-etal-2020-jephpp/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/maslowski-etal-2020-jephpp/","section":"publication","summary":"To comprehend speech sounds, listeners tune in to speech rate information in the proximal (immediately adjacent), distal (nonadjacent), and global context (further removed preceding and following sentences). Effects of global contextual speech rate cues on speech perception have been shown to follow constraints not found for proximal and distal speech rate. Therefore, listeners may process such global cues at distict time points during word recognition. We conducted a printed-word eye-tracking experiment to compare the time courses of distal and global rate effects. Results indicated that the distal rate effect emerged immediately after target sound presentation, in line with a general-auditory account. The global rate effect, however, arose more than 200 ms later than the distal rate effect, indicating that distal and global context effects involve distinct processing mechanisms. Results are interpreted in a 2-stage model of acoustic context effects. This model posits that distal context effects involve very early perceptual processes, while global context effects arise at a later stage, involving cognitive adjustments conditioned by higher-level information.","tags":["rate normalization","distal context","global context","two-stage model","eye-tracking"],"title":"Eye-tracking the time course of distal and global speech rate effects","type":"publication"},{"authors":["Marjolein van Os","Nivja H. de Jong","Hans Rutger Bosker"],"categories":null,"content":" ","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"f2da57a002b37c1192f3369a211f909d","permalink":"https://hrbosker.github.io/publication/vanos-etal-2020-ll/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/vanos-etal-2020-ll/","section":"publication","summary":"Fluency is an important part of research on second language learning, but most research on language proficiency typically has not included oral fluency as part of interactions even though natural communication usually occurs in conversations. The present study considered aspects of turn-taking behavior as part of the construct of fluency and investigated whether these aspects differentially influence perceived fluency ratings of native and nonnative speech. Results from two experiments using acoustically manipulated speech showed that, in native speech, too “eager” answers (interrupting a question with a fast answer) and too “reluctant” answers (answering slowly after a long turn gap) negatively affected fluency ratings. However, in nonnative speech, only too “reluctant” answers led to lower fluency ratings. Thus, we demonstrated that acoustic properties of dialogue are perceived as part of fluency. By adding to the current understanding of dialogue fluency, these lab-based findings carry implications for language teaching and assessment.","tags":["fluency","turn-taking","turn gaps","dialogue","ratings"],"title":"Fluency in dialogue: Turn‐taking behavior shapes perceived fluency in native and nonnative speech","type":"publication"},{"authors":["Hans Rutger Bosker","David Peeters","Judith Holler"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"160b1f6e0e16f8ef5cb2034fad1960c7","permalink":"https://hrbosker.github.io/publication/bosker-etal-2020-qjep/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2020-qjep/","section":"publication","summary":"Spoken words are highly variable and therefore listeners interpret speech sounds relative to the surrounding acoustic context, such as the speech rate of a preceding sentence. For instance, a vowel midway between short /ɑ/ and long /a/ in Dutch is perceived as short /ɑ/ in the context of preceding slow speech, but as long /a/ if preceded by a fast context. Despite the well-established influence of visual articulatory cues on speech comprehension, it remains unclear whether visual cues to speech rate also influence subsequent spoken word recognition. In two “Go Fish”–like experiments, participants were presented with audio-only (auditory speech + fixation cross), visual-only (mute videos of talking head), and audiovisual (speech + videos) context sentences, followed by ambiguous target words containing vowels midway between short /ɑ/ and long /a/. In Experiment 1, target words were always presented auditorily, without visual articulatory cues. Although the audio-only and audiovisual contexts induced a rate effect (i.e., more long /a/ responses after fast contexts), the visual-only condition did not. When, in Experiment 2, target words were presented audiovisually, rate effects were observed in all three conditions, including visual-only. This suggests that visual cues to speech rate in a context sentence influence the perception of following visual target cues (e.g., duration of lip aperture), which at an audiovisual integration stage bias participants’ target categorisation responses. These findings contribute to a better understanding of how what we see influences what we hear.","tags":["speech rate","neural entrainment","audiovisual speech perception","rate-dependent perception","rate normalisation","supramodal perception"],"title":"How visual cues to speech rate influence speech perception","type":"publication"},{"authors":["Greta Kaufeld","Anna Ravenschlag","Antje S. Meyer","Andrea E. Martin","Hans Rutger Bosker"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"8cd3aa663a7d5a88707fa58237e3d081","permalink":"https://hrbosker.github.io/publication/kaufeld-etal-2020-jeplmc/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/kaufeld-etal-2020-jeplmc/","section":"publication","summary":"During spoken language comprehension, listeners make use of both knowledge-based and signal-based sources of information, but little is known about how cues from these distinct levels of representational hierarchy are weighted and integrated online. In an eye-tracking experiment using the visual world paradigm, we investigated the flexible weighting and integration of morphosyntactic gender marking (a knowledge-based cue) and contextual speech rate (a signal-based cue). We observed that participants used the morphosyntactic cue immediately to make predictions about upcoming referents, even in the presence of uncertainty about the cue’s reliability. Moreover, we found speech rate normalization effects in participants’ gaze patterns even in the presence of preceding morphosyntactic information. These results demonstrate that cues are weighted and integrated flexibly online, rather than adhering to a strict hierarchy. We further found rate normalization effects in the looking behavior of participants who showed a strong behavioral preference for the morphosyntactic gender cue. This indicates that rate normalization effects are robust and potentially automatic. We discuss these results in light of theories of cue integration and the two-stage model of acoustic context effects.","tags":["language comprehension","speech perception","cue integration","rate normalization"],"title":"Knowledge-based and signal-based cues are weighted flexibly during spoken language comprehension","type":"publication"},{"authors":["Greta Kaufeld","Hans Rutger Bosker","Sanne ten Oever","Philip M. Alday","Antje S. Meyer","Andrea E. Martin"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"9e40ccf211a40da78fd81c04a173df3b","permalink":"https://hrbosker.github.io/publication/kaufeld-etal-2020-jneuro/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/kaufeld-etal-2020-jneuro/","section":"publication","summary":"Neural oscillations track linguistic information during speech comprehension (Ding et al., 2016; Keitel et al., 2018), and are known to be modulated by acoustic landmarks and speech intelligibility (Doelling et al., 2014; Zoefel and VanRullen, 2015). However, studies investigating linguistic tracking have either relied on non-naturalistic isochronous stimuli or failed to fully control for prosody. Therefore, it is still unclear whether low-frequency activity tracks linguistic structure during natural speech, where linguistic structure does not follow such a palpable temporal pattern. Here, we measured electroencephalography (EEG) and manipulated the presence of semantic and syntactic information apart from the timescale of their occurrence, while carefully controlling for the acoustic-prosodic and lexical-semantic information in the signal. EEG was recorded while 29 adult native speakers (22 women, 7 men) listened to naturally spoken Dutch sentences, jabberwocky controls with morphemes and sentential prosody, word lists with lexical content but no phrase structure, and backward acoustically matched controls. Mutual information (MI) analysis revealed sensitivity to linguistic content: MI was highest for sentences at the phrasal (0.8–1.1 Hz) and lexical (1.9–2.8 Hz) timescales, suggesting that the delta-band is modulated by lexically driven combinatorial processing beyond prosody, and that linguistic content (i.e., structure and meaning) organizes neural oscillations beyond the timescale and rhythmicity of the stimulus. This pattern is consistent with neurophysiologically inspired models of language comprehension (Martin, 2016, 2020; Martin and Doumas, 2017) where oscillations encode endogenously generated linguistic content over and above exogenous or stimulus-driven timing and rhythm information.","tags":["combinatorial processing","lexical semantics","mutual information","neural oscillations","prosody","sentence comprehension"],"title":"Linguistic structure and meaning organize neural oscillations into a content-specific hierarchy","type":"publication"},{"authors":["Hans Rutger Bosker","Matthias J. Sjerps","Eva Reinisch"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"b61bf53d11c1ab28b52639de46e622f0","permalink":"https://hrbosker.github.io/publication/bosker-etal-2020-app/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2020-app/","section":"publication","summary":"Speech sounds are perceived relative to spectral properties of surrounding speech. For instance, target words ambiguous between /bɪt/ (with low F1) and /bɛt/ (with high F1) are more likely to be perceived as “bet” after a ‘low F1’ sentence, but as “bit” after a ‘high F1’ sentence. However, it is unclear how these spectral contrast effects (SCEs) operate in multi-talker listening conditions. Recently, Feng and Oxenham [(2018b). J.Exp.Psychol.-Hum.Percept.Perform. 44(9), 1447–1457] reported that selective attention affected SCEs to a small degree, using two simultaneously presented sentences produced by a single talker. The present study assessed the role of selective attention in more naturalistic ‘cocktail party’ settings, with 200 lexically unique sentences, 20 target words, and different talkers. Results indicate that selective attention to one talker in one ear (while ignoring another talker in the other ear) modulates SCEs in such a way that only the spectral properties of the attended talker influences target perception. However, SCEs were much smaller in multi-talker settings (Experiment 2) than those in single-talker settings (Experiment 1). Therefore, the influence of SCEs on speech comprehension in more naturalistic settings (i.e., with competing talkers) may be smaller than estimated based on studies without competing talkers.","tags":["spectral contrast effects","spectral normalization","selective attention","stream segregation","cocktail party listening"],"title":"Spectral contrast effects are modulated by selective attention in ‘cocktail party’ settings","type":"publication"},{"authors":["Hans Rutger Bosker","Matthias J. Sjerps","Eva Reinisch"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"9441a5b9cbe776f21a2688c67fdd3031","permalink":"https://hrbosker.github.io/publication/bosker-etal-2020-scirep/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2020-scirep/","section":"publication","summary":"Two fundamental properties of perception are selective attention and perceptual contrast, but how these two processes interact remains unknown. Does an attended stimulus history exert a larger contrastive influence on the perception of a following target than unattended stimuli? Dutch listeners categorized target sounds with a reduced prefix “ge-” marking tense (e.g., ambiguous between gegaan-gaan “gone-go”). In ‘single talker’ Experiments 1–2, participants perceived the reduced syllable (reporting gegaan) when the target was heard after a fast sentence, but not after a slow sentence (reporting gaan). In ‘selective attention’ Experiments 3–5, participants listened to two simultaneous sentences from two different talkers, followed by the same target sounds, with instructions to attend only one of the two talkers. Critically, the speech rates of attended and unattended talkers were found to equally influence target perception – even when participants could watch the attended talker speak. In fact, participants’ target perception in ‘selective attention’ Experiments 3–5 did not differ from participants who were explicitly instructed to divide their attention equally across the two talkers (Experiment 6). This suggests that contrast effects of speech rate are immune to selective attention, largely operating prior to attentional stream segregation in the auditory processing hierarchy.","tags":["temporal contrast effects","rate normalization","selective attention","stream segregation","cocktail party listening"],"title":"Temporal contrast effects in human speech perception are immune to selective attention","type":"publication"},{"authors":["Hans Rutger Bosker","Marjolein van Os,","Rik Does","Geertje van Bergen"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"d9f3a515881dbb1b8aa2785fbac0fb35","permalink":"https://hrbosker.github.io/publication/bosker-etal-2019-jml/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2019-jml/","section":"publication","summary":"Disfluencies, like uh, have been shown to help listeners anticipate reference to low-frequency words. The associative account of this ‘disfluency bias’ proposes that listeners learn to associate disfluency with low-frequency referents based on prior exposure to non-arbitrary disfluency distributions (i.e., greater probability of low-frequency words after disfluencies). However, there is limited evidence for listeners actually tracking disfluency distributions online. The present experiments are the first to show that adult listeners, exposed to a typical or more atypical disfluency distribution (i.e., hearing a talker unexpectedly say uh before high-frequency words), flexibly adjust their predictive strategies to the disfluency distribution at hand (e.g., learn to predict high-frequency referents after disfluency). However, when listeners were presented with the same atypical disfluency distribution but produced by a non-native speaker, no adjustment was observed. This suggests pragmatic inferences can modulate distributional learning, revealing the flexibility of, and constraints on, distributional learning in incremental language comprehension.","tags":["distributional learning","pragmatic inferences","disfluencies","non-native speech","prediction","eye-tracking"],"title":"Counting ‘uhm’s: how tracking the distribution of native and non-native disfluencies influences online language comprehension","type":"publication"},{"authors":["Joe Rodd","Hans Rutger Bosker","Louis ten Bosch","Mirjam Ernestus"],"categories":null,"content":" ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"cbf0fcb25c0b05fa2e75c569cb77e8af","permalink":"https://hrbosker.github.io/publication/rodd-etal-2019-jasa/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/rodd-etal-2019-jasa/","section":"publication","summary":"Many psycholinguistic models of speech sequence planning make claims about the onset and offset times of planning units, such as words, syllables, and phonemes. These predictions typically go untested, however, since psycholinguists have assumed that the temporal dynamics of the speech signal is a poor index of the temporal dynamics of the underlying speech planning process. This article argues that this problem is tractable, and presents and validates two simple metrics that derive planning unit onset and offset times from the acoustic signal and articulatographic data.","tags":null,"title":"Deriving the onset and offset times of planning units from acoustic and articulatory measurements","type":"publication"},{"authors":["Merel Maslowski","Antje S. Meyer","Hans Rutger Bosker"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"58667e15717a4744f554472fbc50754e","permalink":"https://hrbosker.github.io/publication/maslowski-etal-2019-jeplmc/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/maslowski-etal-2019-jeplmc/","section":"publication","summary":"Listeners are known to track statistical regularities in speech. Yet, which temporal cues are encoded is unclear. This study tested effects of talker-specific habitual speech rate and talker-independent average speech rate (heard over a longer period of time) on the perception of the temporal Dutch vowel contrast /ɑ/–/a:/. First, Experiment 1 replicated that slow local (surrounding) speech contexts induce fewer long /a:/ responses than faster contexts. Experiment 2 tested effects of long-term habitual speech rate. A high-rate group listened to ambiguous vowels embedded in “neutral” speech from Talker A, intermixed with fast speech from Talker B. A low-rate group listened to the same neutral speech from Talker A, and/but to Talker B speaking at a slow rate. Between-groups comparison of the neutral trials showed that the high-rate group demonstrated a lower proportion of /a:/ responses, indicating that Talker A’s habitual speech rate sounded slower when B was faster. In Experiment 3, both talkers produced speech at both rates, removing the different habitual speech rates of Talkers A and B, while maintaining the average rates differing between groups. In Experiment 3, no global rate effect was observed. Taken together, the present experiments show that a talker’s habitual rate is encoded relative to the habitual rate of another talker, carrying implications for episodic and constraint-based models of speech perception.","tags":["speech rate","rate-dependent perception","rate normalization","habitual speech rate"],"title":"How the tracking of habitual rate influences speech perception","type":"publication"},{"authors":["Merel Maslowski","Antje S. Meyer","Hans Rutger Bosker"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"abe220dc07750360b49269cfd402f1da","permalink":"https://hrbosker.github.io/publication/maslowski-etal-2019-jasa/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/maslowski-etal-2019-jasa/","section":"publication","summary":"Speech can be produced at different rates. Listeners take this rate variation into account by normalizing vowel duration for contextual speech rate: An ambiguous Dutch word /m?t/ is perceived as short /mAt/ when embedded in a slow context, but long /ma:t/ in a fast context. While some have argued that this rate normalization involves low-level automatic perceptual processing, there is also evidence that it arises at higher-level cognitive processing stages, such as decision making. Prior research on rate-dependent speech perception has only used explicit recognition tasks to investigate the phenomenon, involving both perceptual processing and decision making. This study tested whether speech rate normalization can be observed without explicit decision making, using a crossmodal repetition priming paradigm. Results show that a fast precursor sentence makes an embedded ambiguous prime (/m?t/) sound (implicitly) more /a:/-like, facilitating lexical access to the long target word “maat” in a (explicit) lexical decision task. This result suggests that rate normalization is automatic, taking place even in the absence of an explicit recognition task. Thus, rate normalization is placed within the realm of everyday spoken conversation, where explicit categorization of ambiguous sounds is rare.","tags":["speech rate","rate-dependent perception","rate normalization","habitual speech rate"],"title":"Listeners normalize speech for contextual speech rate even without an explicit recognition task","type":"publication"},{"authors":["Hans Rutger Bosker","Oded Ghitza"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"b50c266bfb995a1cad9fecefa2f893f9","permalink":"https://hrbosker.github.io/publication/bosker-etal-2018-lcn/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2018-lcn/","section":"publication","summary":"This psychoacoustic study provides behavioural evidence that neural entrainment in the theta range (3–9 Hz) causally shapes speech perception. Adopting the “rate normalization” paradigm (presenting compressed carrier sentences followed by uncompressed target words), we show that uniform compression of a speech carrier to syllable rates inside the theta range influences perception of subsequent uncompressed targets, but compression outside theta range does not. However, the influence of carriers – compressed outside theta range – on target perception is salvaged when carriers are “repackaged” to have a packet rate inside theta. This suggests that the brain can only successfully entrain to syllable/packet rates within theta range, with a causal influence on the perception of subsequent speech, in line with recent neuroimaging data. Thus, this study points to a central role for sustained theta entrainment in rate normalisation and contributes to our understanding of the functional role of brain oscillations in speech perception.","tags":["neural entrainment","theta oscillations","speech rate","rate normalization"],"title":"Entrained theta oscillations guide perception of subsequent speech: Behavioral evidence from rate normalization","type":"publication"},{"authors":["Hans Rutger Bosker","Geertje van Bergen"],"categories":null,"content":" ","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"c2f7ff39a6e36571db77d276fabb2f04","permalink":"https://hrbosker.github.io/publication/bosker-etal-2018-jml/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2018-jml/","section":"publication","summary":"Interpersonal discourse particles (DPs), such as Dutch inderdaad (≈‘indeed’) and eigenlijk (≈‘actually’) are highly frequent in everyday conversational interaction. Despite extensive theoretical descriptions of their polyfunctionality, little is known about how they are used by language comprehenders. In two visual world eyetracking experiments involving an online dialogue completion task, we asked to what extent inderdaad, confirming an inferred expectation, and eigenlijk, contrasting with an inferred expectation, influence real-time understanding of dialogues. Answers in the dialogues contained a DP or a control adverb, and a critical discourse referent was replaced by a beep; participants chose the most likely dialogue completion by clicking on one of four referents in a display. Results show that listeners make rapid and fine-grained situation-specific inferences about the use of DPs, modulating their expectations about how the dialogue will unfold. Findings further specify and constrain theories about the conversation-managing function and polyfunctionality of DPs.","tags":["discourse particles","discourse processing","dialogue","polyfunctionality","visual world eye-tracking"],"title":"Linguistic expectation management in online discourse processing: An investigation of Dutch inderdaad ‘indeed’ and eigenlijk ‘actually’","type":"publication"},{"authors":["Merel Maslowski","Antje S. Meyer","Hans Rutger Bosker"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"c060fd26083040da32ed45e4402daef5","permalink":"https://hrbosker.github.io/publication/maslowski-etal-2018-plosone/index/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/maslowski-etal-2018-plosone/index/","section":"publication","summary":"Listeners are known to use adjacent contextual speech rate in processing temporally ambiguous speech sounds. For instance, an ambiguous vowel between short /α/ and long /a:/ in Dutch sounds relatively long (i.e., as /a:/) embedded in a fast precursor sentence, but short in a slow sentence. Besides the local speech rate, listeners also track talker-specific global speech rates. However, it is yet unclear whether other talkers’ global rates are encoded with reference to a listener’s self-produced rate. Three experiments addressed this question. In Experiment 1, one group of participants was instructed to speak fast, whereas another group had to speak slowly. The groups were compared on their perception of ambiguous /α/-/a:/ vowels embedded in neutral rate speech from another talker. In Experiment 2, the same participants listened to playback of their own speech and again evaluated target vowels in neutral rate speech. Neither of these experiments provided support for the involvement of self-produced speech in perception of another talker’s speech rate. Experiment 3 repeated Experiment 2 but with a new participant sample that was unfamiliar with the participants from Experiment 2. This experiment revealed fewer /a:/ responses in neutral speech in the group also listening to a fast rate, suggesting that neutral speech sounds slow in the presence of a fast talker and vice versa. Taken together, the findings show that selfproduced speech is processed differently from speech produced by others. They carry implications for our understanding of rate-dependent speech perception in dialogue settings, suggesting that both perceptual and cognitive mechanisms are involved.","tags":["rate normalization","self perception","speech rate"],"title":"Listening to yourself is special: Evidence from global speech rate tracking","type":"publication"},{"authors":["Anne Kösem","Hans Rutger Bosker","Atsuko  Takashima","Antje S. Meyer","Ole Jensen","Peter Hagoort"],"categories":null,"content":" ","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"00158eb073cf11d7662c3887ab8a0cac","permalink":"https://hrbosker.github.io/publication/kosem-etal-2018-currbiol/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/kosem-etal-2018-currbiol/","section":"publication","summary":"Low-frequency neural entrainment to rhythmic input has been hypothesized as a canonical mechanismthat shapes sensory perception in time. Neural entrainment is deemed particularly relevant for speech analysis, as it would contribute to the extraction of discrete linguistic elements from continuous acoustic signals. However, its causal influence in speech perception has been difficult to establish. Here, we provide evidence that oscillations build temporal predictions about the duration of speech tokens that affect perception. Using magnetoencephalography (MEG), we studied neural dynamics during listening to sentences that changed in speech rate. We observed neural entrainment to preceding speech rhythms persisting for several cycles after the change in rate. The sustained entrainment was associated with changes in the perceived duration of the last word’s vowel, resulting in the perception of words with different meanings. These findings support oscillatory models of speech processing, suggesting that neural oscillations actively shape speech perception.","tags":["speech","rhythm","temporal prediction","neural oscillations","MEG","rate normalization"],"title":"Neural entrainment determines the words we hear","type":"publication"},{"authors":["Hans Rutger Bosker"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"5277515244fb2c112e399c24f290a106","permalink":"https://hrbosker.github.io/publication/bosker-2018-jasa/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-2018-jasa/","section":"publication","summary":"Recently, the world’s attention was caught by an audio clip that was perceived as “Laurel” or “Yanny.” Opinions were sharply split. Many could not believe others heard something different from their perception. However, a crowd-source experiment with \u003e500 participants shows that it is possible to make people hear Laurel, where they previously heard Yanny, by manipulating preceding acoustic context. This study is not only the first to reveal within-listener variation in Laurel/ Yanny percepts, but also to demonstrate contrast effects for global spectral information in larger frequency regions. Thus, it highlights the intricacies of human perception underlying these social media phenomena.","tags":["speech perception","crowd-source experiment","acoustic context effects","contrast effects","social media"],"title":"Putting Laurel and Yanny in context","type":"publication"},{"authors":["Hans Rutger Bosker","Martin Cooke"],"categories":null,"content":" ","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"d7f61337abb55475eab648c185c44c33","permalink":"https://hrbosker.github.io/publication/bosker-etal-2018-jasa/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2018-jasa/","section":"publication","summary":"Speakers adjust their voice when talking in noise (known as Lombard speech), facilitating speech comprehension. Recent neurobiological models of speech perception emphasize the role of amplitude modulations in speech-in-noise comprehension, helping neural oscillators to “track” the attended speech. This study tested whether talkers produce more pronounced amplitude modulations in noise. Across four different corpora, modulation spectra showed greater power in amplitude modulations below 4 Hz in Lombard speech compared to matching plain speech. This suggests that noise-induced speech contains more pronounced amplitude modulations, potentially helping the listening brain to entrain to the attended talker, aiding comprehension.","tags":["Lombard speech","speech in noise","amplitude modulations","rhythmicity","rhythm"],"title":"Talkers produce more pronounced amplitude modulations when speaking in noise","type":"publication"},{"authors":["Hans Rutger Bosker"],"categories":null,"content":" ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"9cafbdbc17484ba87e2d37f66fca2de9","permalink":"https://hrbosker.github.io/publication/bosker-2017-app/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-2017-app/","section":"publication","summary":"The perception of temporal contrasts in speech is known to be influenced by the speech rate in the surrounding context. This rate-dependent perception is suggested to involve general auditory processes because it is also elicited by non-speech contexts, such as pure tone sequences. Two general auditory mechanisms have been proposed to underlie rate-dependent perception, durational contrast and neural entrainment. This study compares the predictions of these two accounts of rate-dependent speech perception by means of four experiments, in which participants heard tone sequences followed by Dutch target words ambiguous between /ɑs/ 'ash' and /a:s/ 'bait'. Tone sequences varied in the duration of tones (short vs. long) and in the presentation rate of the tones (fast vs. slow). Results show that the duration of preceding tones did not influence target perception in any of the experiments, thus challenging durational contrast as explanatory mechanism behind rate-dependent perception. Instead, the presentation rate consistently elicited a category boundary shift, with faster presentation rates inducing more /a:s/ responses, but only if the tone sequence was isochronous. Therefore, this study proposes an alternative, neurobiologically plausible account of rate-dependent perception involving neural entrainment of endogenous oscillations to the rate of a rhythmic stimulus","tags":["Speech rate","Rate-dependent perception","Rate normalization","Durational contrast","Neural entrainment"],"title":"Accounting for rate-dependent category boundary shifts in speech perception","type":"publication"},{"authors":["Hans Rutger Bosker","Anne Kösem"],"categories":null,"content":" ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"8c924dd435401c21b24ccd086ce3b67c","permalink":"https://hrbosker.github.io/publication/bosker-kosem-2017-interspeech/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-kosem-2017-interspeech/","section":"publication","summary":"Brain oscillations have been shown to track the slow amplitude fluctuations in speech during comprehension. Moreover, there is evidence that these stimulus-induced cortical rhythms may persist even after the driving stimulus has ceased. However, how exactly this neural entrainment shapes speech perception remains debated. This behavioral study investigated whether and how the frequency and phase of an entrained rhythm would influence the temporal sampling of subsequent speech. In two behavioral experiments, participants were presented with slow and fast isochronous tone sequences, followed by Dutch target words ambiguous between as /ɑs/ “ash” (with a short vowel) and aas /a:s/ “bait” (with a long vowel). Target words were presented at various phases of the entrained rhythm. Both experiments revealed effects of the frequency of the tone sequence on target word perception: fast sequences biased listeners to more long /a:s/ responses. However, no evidence for phase effects could be discerned. These findings show that an entrained rhythm’s frequency, but not phase, influences the temporal sampling of subsequent speech. These outcomes are compatible with theories suggesting that sensory timing is evaluated relative to entrained frequency. Furthermore, they suggest that phase tracking of (syllabic) rhythms by theta oscillations plays a limited role in speech parsing.","tags":["neural entrainment","phase-locking","temporal sampling","speech parsing","rate normalization","speech rate"],"title":"An entrained rhythm’s frequency, not phase, influences temporal sampling of speech","type":"publication"},{"authors":["Hans Rutger Bosker","Eva Reinisch","Matthias J. Sjerps"],"categories":null,"content":" ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"99e0771d11bbe42e8f80a12590f2fd84","permalink":"https://hrbosker.github.io/publication/bosker-etal-2017-jml/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2017-jml/","section":"publication","summary":"In natural situations, speech perception often takes place during the concurrent execution of other cognitive tasks, such as listening while viewing a visual scene. The execution of a dual task typically has detrimental effects on concurrent speech perception, but how exactly cognitive load disrupts speech encoding is still unclear. The detrimental effect on speech representations may consist of either a general reduction in the robustness of processing of the speech signal (‘noisy encoding’), or, alternatively it may specifically influence the temporal sampling of the sensory input, with listeners missing temporal pulses, thus underestimating segmental durations (‘shrinking of time’). The present study investigated whether and how spectral and temporal cues in a precursor sentence that has been processed under high vs. low cognitive load influence the perception of a subsequent target word. If cognitive load effects are implemented through ‘noisy encoding’, increasing cognitive load during the  recursor should attenuate the encoding of both its temporal and spectral cues, and hence reduce the contextual effect that these cues can have on subsequent target sound perception. However, if cognitive load effects are expressed as ‘shrinking of time’, context effects should not be modulated by load, but a main effect would be expected on the perceived duration of the speech signal. Results from two experiments indicate that increasing cognitive load (manipulated through a secondary visual search task) did not modulate temporal (Experiment 1) or spectral context effects (Experiment 2). However, a consistent main effect of cognitive load was found: increasing cognitive load during the precursor induced a perceptual increase in its perceived speech rate, biasing the perception of a following target word towards longer durations. This finding suggests that cognitive load effects in speech perception are implemented via ‘shrinking of time’, in line with a temporal sampling framework. In addition, we argue that our results align with a model in which early (spectral and temporal) normalization is unaffected by attention but later adjustments may be attention-dependent.","tags":["cognitive load","acoustic context","rate normalization","spectral normalization"],"title":"Cognitive load makes speech sound fast, but does not modulate acoustic context effects","type":"publication"},{"authors":["Hans Rutger Bosker","Eva Reinisch"],"categories":null,"content":" ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"00f81cb6138ee650e2c0f2326d0d4b2e","permalink":"https://hrbosker.github.io/publication/bosker-etal-2017-frontiers/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2017-frontiers/","section":"publication","summary":"Anecdotal evidence suggests that unfamiliar languages sound faster than one’s native language. Empirical evidence for this impression has, so far, come from explicit rate judgments. The aim of the present study was to test whether such perceived rate differences between native and foreign languages (FLs) have effects on implicit speech processing. Our measure of implicit rate perception was “normalization for speech rate”: an ambiguous vowel between short /a/ and long /a:/ is interpreted as /a:/ following a fast but as /a/ following a slow carrier sentence. That is, listeners did not judge speech rate itself; instead, they categorized ambiguous vowels whose perception was implicitly affected by the rate of the context. We asked whether a bias towards long /a:/ might be observed when the context is not actually faster but simply spoken in a FL. A fully symmetrical experimental design was used: Dutch and German participants listened to rate matched (fast and slow) sentences in both languages spoken by the same bilingual speaker. Sentences were followed by non-words that contained vowels from an /a-a:/ duration continuum. Results from Experiments 1 and 2 showed a consistent effect of rate normalization for both listener groups. Moreover, for German listeners, across the two experiments, foreign sentences triggered more /a:/ responses than (rate matched) native sentences, suggesting that foreign sentences were indeed perceived as faster. Moreover, this FL effect was modulated by participants’ ability to understand the FL. Those participants that scored higher on a FL translation task showed less of a FL effect. However, opposite effects were found for the Dutch listeners. For them, their native rather than the FL induced more /a:/ responses. Nevertheless, this reversed effect could be reduced when additional spectral properties of the context were controlled for. Experiment 3, using explicit rate judgments, replicated the effect for German but not Dutch listeners. We therefore conclude that the subjective impression that FLs sound fast may have an effect on implicit speech processing, with implications for how language learners perceive spoken segments in a FL.","tags":["speech rate","speech segmentation","rate normalization","second language acquisition","L2 speech","perception","‘Gabbling Foreigner Illusion’"],"title":"Foreign languages sound fast: evidence from implicit rate normalization","type":"publication"},{"authors":["Hans Rutger Bosker"],"categories":null,"content":" ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"8749e7e636e8805f6986e895daf6bc51","permalink":"https://hrbosker.github.io/publication/bosker-2017-jeplmc/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-2017-jeplmc/","section":"publication","summary":"In conversation, our own speech and that of others follow each other in rapid succession. Effects of the surrounding context on speech perception are well documented but, despite the ubiquity of the sound of our own voice, it is unknown whether our own speech also influences our perception of other talkers. This study investigated context effects induced by our own speech through 6 experiments, specifically targeting rate normalization (i.e., perceiving phonetic segments relative to surrounding speech rate). Experiment 1 revealed that hearing prerecorded fast or slow context sentences altered the perception of ambiguous vowels, replicating earlier work. Experiment 2 demonstrated that talking at a fast or slow rateprior to target presentation also altered target perception, though the effect of preceding speech rate was reduced. Experiment 3 showed that silent talking (i.e., inner speech) at fast or slow rates did not modulatethe perception of others, suggesting that the effect of self-produced speech rate in Experiment 2 arose through monitoring of the external speech signal. Experiment 4 demonstrated that, when participants were played back their own (fast/slow) speech, no reduction of the effect of preceding speech rate was observed, suggesting that the additional task of speech production may be responsible for the reduced effect in Experiment 2. Finally, Experiments 5 and 6 replicate Experiments 2 and 3 with new participant samples. Taken together, these results suggest that variation in speech production may induce variation in speech perception, thus carrying implications for our understanding of spoken communication in dialogue settings.","tags":["speech rate normalization","self-monitoring","covert speech","phonetic convergence","speaking induced suppression"],"title":"How our own speech rate influences our perception of others","type":"publication"},{"authors":["Hans Rutger Bosker"],"categories":null,"content":" ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"06ac0e50637b077c7b550ac8c8485cd9","permalink":"https://hrbosker.github.io/publication/bosker-2017-interspeech/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-2017-interspeech/","section":"publication","summary":"Speech is an acoustic signal with inherent amplitude modulations in the 1-9 Hz range. Recent models of speech perception propose that this rhythmic nature of speech is central to speech recognition. Moreover, rhythmic amplitude modulations have been shown to have beneficial effects on language processing and the subjective impression listeners have of the speaker. This study investigated the role of amplitude modulations in the political arena by comparing the speech produced by Hillary Clinton and Donald Trump in the three presidential debates of 2016. Inspection of the modulation spectra, revealing the spectral content of the two speakers’ amplitude envelopes after matching for overall intensity, showed considerably greater power in Clinton’s modulation spectra (compared to Trump’s) across the three debates, particularly in the 1-9 Hz range. The findings suggest that Clinton’s speech had a more pronounced temporal envelope with rhythmic amplitude modulations below 9 Hz, with a preference for modulations around 3 Hz. This may be taken as evidence for a more structured temporal organization of syllables in Clinton’s speech, potentially due to more frequent use of preplanned utterances. Outcomes are interpreted in light of the potential beneficial effects of a rhythmic temporal envelope on intelligibility and speaker perception.","tags":["amplitude envelope","amplitude modulations","speech rhythm","modulation spectrum"],"title":"The role of temporal amplitude modulations in the political arena: Hillary Clinton vs. Donald Trump","type":"publication"},{"authors":["Merel Maslowski","Antje S. Meyer","Hans Rutger Bosker"],"categories":null,"content":" ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"22af9b2ebe89dfcf6d7ef0bde3ee4f4c","permalink":"https://hrbosker.github.io/publication/maslowski-etal-2017-interspeech/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/maslowski-etal-2017-interspeech/","section":"publication","summary":"Speech rate is known to modulate perception of temporally ambiguous speech sounds. For instance, a vowel may be perceived as short when the immediate speech context is slow, but as long when the context is fast. Yet, effects of long-term tracking of speech rate are largely unexplored. Two experiments tested whether long-term tracking of rate influences perception of the temporal Dutch vowel contrast /A/-/a:/. In Experiment 1, one low-rate group listened to ‘neutral’ rate speech from talker A and to slow speech from talker B. Another high-rate group was exposed to the same neutral speech from A, but to fast speech from B. Between-group comparison of the ‘neutral’ trials revealed that the low-rate group reported a higher proportion of /a:/ in A’s ‘neutral’ speech, indicating that A sounded faster when B was slow. Experiment 2 tested whether one’s own speech rate also contributes to effects of long-term tracking of rate. Here, talker B’s speech was replaced by playback of participants’ own fast or slow speech. No evidence was found that one’s own voice affected perception of talker A in larger speech contexts. These results carry implications for our understanding of the mechanisms involved in rate-dependent speech perception and of dialogue.","tags":["speech rate","rate-dependent speech perception","rate normalization","global context effects","self-produced speech"],"title":"Whether long-term tracking of speech rate affects perception depends on who is talking","type":"publication"},{"authors":["Hans Rutger Bosker","Eva Reinisch","Matthias J. Sjerps"],"categories":null,"content":" ","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"424c11fa7b6d0b6f593af263db793167","permalink":"https://hrbosker.github.io/publication/bosker-etal-2016-spire/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2016-spire/","section":"publication","summary":"Listeners interpret local temporal cues (e.g., vowel durations) relative to the surrounding speech rate. For instance, an ambiguous Dutch vowel midway between short /ɑ/ and long /a:/ may be perceived as long /a:/ when presented in a fast context, but as short /ɑ/ in a slow context [1]. It is widely assumed that this process known as rate normalization is an early general auditory process [1, 2], and as such would operate independent from other higher level influences such as attention. However, when the perceptual system is taxed by the concurrent execution of another task, the encoding of the incoming speech signal is known to be negatively affected [3]. Therefore, listening to, for example, fast speech under cognitive load may result in impoverished encoding of the fast speech rate, reducing the effect that a fast context may have on the perception of subsequent speech (i.e., a reduction of the rate effect; cf. [4]). Alternatively, an increase in cognitive load has been shown to speed up time perception (the “shrinking of time”, [5]), potentially increasing the perceived rate of concurrent speech. This argument has, for instance, been used to explain why foreign-accented speech sounds faster than native speech [6]. Here we attempt to distinguish between these alternatives by testing Dutch /ɑ/-/a:/ categorization as a function of (1) the rate of the preceding carrier sentence and (2) the difficulty of a dual task (visual search) performed during carrier presentation.","tags":["rate normalization","spectral normalization","cognitive load","dual-tasking"],"title":"Listening under cognitive load makes speech sound fast","type":"publication"},{"authors":["Hans Rutger Bosker"],"categories":null,"content":" ","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"15e37b3ce74d6b207e79e6cb22e65d4c","permalink":"https://hrbosker.github.io/publication/bosker-2016-sp/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-2016-sp/","section":"publication","summary":"During conversation, spoken utterances occur in rich acoustic contexts, including speech produced by our interlocutor(s) and speech we produced ourselves. Prosodic characteristics of the acoustic context have been known to influence speech perception in a contrastive fashion: for instance, a vowel presented in a fast context is perceived to have a longer duration than the same vowel in a slow context. Given the ubiquity of the sound of our own voice, it may be that our own speech rate - a common source of acoustic context - also influences our perception of the speech of others. Two experiments were designed to test this hypothesis. Experiment 1 replicated earlier contextual rate effects by showing that hearing pre-recorded fast or slow context sentences alters the perception of ambiguous Dutch target words. Experiment 2 then extended this finding by showing that talking at a fast or slow rate prior to the presentation of the target words also altered the perception of those words. These results suggest that between-talker variation in speech rate production may induce between-talker variation in speech perception, thus potentially explaining why interlocutors tend to converge on speech rate in dialogue settings.","tags":["speech rate","rate normalization","self-monitoring","phonetic convergence"],"title":"Our own speech rate influences speech perception","type":"publication"},{"authors":["Hans Rutger Bosker","V. J. Tjiong","Hugo Quené","Ted Sanders","Nivja H. de Jong"],"categories":null,"content":" ","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"868b6fca440042fac1a08c511e23bcf1","permalink":"https://hrbosker.github.io/publication/bosker-etal-2015/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2015/","section":"publication","summary":"Disfluencies, such as uh and uhm, are known to help the listener in speech comprehension. For instance, disfluencies may elicit prediction of less accessible referents and may trigger listeners’ attention to the following word. However, recent work suggests differential processing of disfluencies in native and non-native speech. The current study investigated whether the beneficial effects of disfluencies on listeners’ attention are modulated by the (non-)native identity of the speaker. Using the Change Detection Paradigm, we investigated listeners’ recall accuracy for words presented in disfluent and fluent contexts, in native and non-native speech. We observed beneficial effects of both native and non-native disfluencies on listeners’ recall accuracy, suggesting that native and non-native disfluencies trigger listeners’ attention in a similar fashion.","tags":["disfluencies","attention","non-native speech","Change Detection paradigm"],"title":"Both native and non-native disfluencies trigger listeners’ attention","type":"publication"},{"authors":["Hans Rutger Bosker","Eva Reinisch"],"categories":null,"content":" ","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"ca90a1a2186a487023f22764791ca12a","permalink":"https://hrbosker.github.io/publication/bosker-reinisch-2015/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-reinisch-2015/","section":"publication","summary":"Speech perception involves a number of processes that deal with variation in the speech signal. One such process is normalization for speechrate: local temporal cues are perceived relative to the rate in the surrounding context. It is as yet unclear whether and how this perceptual effect interacts with higher level impressions of rate, such as a speaker’s nonnative identity. Nonnative speakers typically speak more slowly than natives, an experience that listeners take into account when explicitly judging the rate of nonnative speech. The present study investigatedwhether this is also reflected in implicit rate normalization. Results indicate that nonnative speech is implicitly perceived as faster than temporally-matched native speech, suggesting that the additional cognitive load of listening to an accent speeds up rate perception. Therefore, rate perception in speech is not dependent on syllable durations alone but also on the ease of processing of the temporal signal.","tags":["speech perception","speechrate","implicit processing","nonnative speech","cognitive load"],"title":"Normalization for speechrate in native and nonnative speech","type":"publication"},{"authors":["Hans Rutger Bosker","Hugo Quené","Ted Sanders","Nivja H. de Jong"],"categories":null,"content":" ","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"5745382d86a7db69495aae3569f1c2f6","permalink":"https://hrbosker.github.io/publication/bosker-etal-2014-jml/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2014-jml/","section":"publication","summary":"Speech comprehension involves extensive use of prediction. Linguistic prediction may be guided by the semantics or syntax, but also by the performance characteristics of the speech signal, such as disfluency. Previous studies have shown that listeners, when presented with the filler uh, exhibit a disfluency bias for discourse-new or unknown referents, drawing inferences about the source of the disfluency. The goal of the present study is to study the contrast between native and non-native disfluencies in speech comprehension. Experiment 1 presented listeners with pictures of high-frequency (e.g., a hand) and low frequency objects (e.g., a sewing machine) and with fluent and disfluent instructions. Listeners were found to anticipate reference to low-frequency objects when encountering disfluency, thus attributing disfluency to speaker trouble in lexical retrieval. Experiment 2  showed that, when participants listened to disfluent non-native speech, no anticipation of low-frequency referents was observed. We conclude that listeners can adapt their predictive strategies to the (non-native) speaker at hand, extending our understanding of the role of speaker identity in speech comprehension.","tags":["prediction","disfluency bias","disfluency","hesitation","non-native speech","speech comprehension"],"title":"Native ‘um’s elicit prediction of low-frequency referents, but non-native ‘um’s do not","type":"publication"},{"authors":["Anne-France Pinget","Hans Rutger Bosker","Hugo Quené","Nivja H. de Jong"],"categories":null,"content":" ","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"860b5116ff3a96ab1d5f234a309dab0e","permalink":"https://hrbosker.github.io/publication/pinget-etal-2014-lt/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/pinget-etal-2014-lt/","section":"publication","summary":"Oral fluency and foreign accent distinguish L2 from L1 speech production. In language testing practices, both fluency and accent are usually assessed by raters. This study investigates what exactly native raters of fluency and accent take into account when judging L2. Our aim is to explore the relationship between objectively measured temporal, segmental and suprasegmental properties of speech on the one hand, and fluency and accent as rated by native raters on the other hand. For 90 speech fragments from Turkish and English L2 learners of Dutch, several acoustic measures of fluency and accent were calculated. In Experiment 1, 20 native speakers of Dutch rated the L2 Dutch samples on fluency. In Experiment 2, 20 different untrained native speakers of Dutch judged the L2 Dutch samples on accentedness. Regression analyses revealed, first, that acousticmeasures of fluency were good predictors of fluency ratings. Second, segmental and suprasegmental measures of accent could predict some variance of accent ratings. Third, perceived fluency and perceived accent were only weakly related. In conclusion, this study shows that fluency and perceived foreign accent can be judged as separate constructs.","tags":["foreign accent","L2 specific fluency","native raters","perception of L2 speech","second language learners"],"title":"Native speakers’ perceptions of fluency and accent in L2 speech","type":"publication"},{"authors":["Katja Poellmann","Hans Rutger Bosker","James M. McQueen","Holger Mitterer"],"categories":null,"content":" ","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"6f2c18a699dffc4f95c19fc8a058167e","permalink":"https://hrbosker.github.io/publication/poellmann-etal-2014-jphon/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/poellmann-etal-2014-jphon/","section":"publication","summary":"This study investigates if and how listeners adapt to reductions in casual continuous speech. In a perceptual learning variant of the visual-world paradigm, two groups of Dutch participants were exposed to either segmental (/b/ → [ʋ]) or syllabic (ver- → [fː]) reductions in spoken Dutch sentences. In the test phase, both groups heard both kinds of reductions, but now applied to different words. In one of two experiments, the segmental reduction exposure group was better than the syllabic reduction exposure group in recognizing new reduced /b/-words.  In both experiments, the syllabic reduction group showed a greater target preference for new reduced ver-words. Learning about reductions was thus applied to previously unheard words. This lexical generalization suggests that mechanisms compensating for semental and syllabic reductions take place at a prelexical level, and hence that lexical access involves an abstractionist mode of processing. Existing abstractionist models need to be revised, however, as they do not include representations of sequences of segments (corresponding e.g. to ver-) at the prelexical level.","tags":["segmental reduction","syllabic reduction","adaptation","perceptual learning","reductions"],"title":"Perceptual adaptation to segmental and syllabic reductions in continuous spoken Dutch","type":"publication"},{"authors":["Hans Rutger Bosker","Hugo Quené","Ted Sanders","Nivja H. de Jong"],"categories":null,"content":" ","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"9f1e87355123fe8a62a02069a7185a56","permalink":"https://hrbosker.github.io/publication/bosker-etal-2014-ll/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2014-ll/","section":"publication","summary":"Where native speakers supposedly are fluent by default, nonnative speakers often have to strive hard to achieve a nativelike fluency level. However, disfluencies (such as pauses, fillers, repairs, etc.) occur in both native and nonnative speech and it is as yet unclear how fluency raters weigh the fluency characteristics of native and nonnative speech. Two rating experiments compared the way raters assess the fluency of native and nonnative speech. The fluency characteristics were controlled by using phonetic manipulations in pause (Experiment 1) and speed characteristics (Experiment 2). The results show that the ratings of manipulated native and nonnative speech were affected in a similar fashion. This suggests that there is no difference in the way listeners weigh the fluency characteristics of native and nonnative speakers.","tags":["fluency","disfluencies","hesitations","phonetic manipulations","nonnative speech"],"title":"The perception of fluency in native and nonnative speech","type":"publication"},{"authors":["Hans Rutger Bosker"],"categories":null,"content":" ","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"1b7308fe0760729e15bf8e89d926ad3e","permalink":"https://hrbosker.github.io/publication/bosker-2014-thesis/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-2014-thesis/","section":"publication","summary":" ","tags":null,"title":"The processing and evaluation of fluency in native and non-native speech","type":"publication"},{"authors":["Nivja H. de Jong","Hans Rutger Bosker"],"categories":null,"content":" ","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"6ad8b93a6b3ee427320da920d6033fba","permalink":"https://hrbosker.github.io/publication/dejong-bosker-2013/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/dejong-bosker-2013/","section":"publication","summary":"Second language (L2) research often involves analyses of acoustic measures of fluency. The studies investigating fluency, however, have been difficult to compare because the measures of fluency that were used differed widely. One of the differences between studies concerns the lower cut-off point for silent pauses, which has been set anywhere between 100 ms and 1000 ms. The goal of this paper is to find an optimal cut-off point. We calculate acoustic measures of fluency using different pause thresholds and then relate these measures to a measure of L2 proficiency and to ratings on fluency.","tags":["silent pauses","number of pauses","duration of pauses","silent pause threshold","second language speech"],"title":"Choosing a threshold for silent pauses to measure second language fluency","type":"publication"},{"authors":["Hans Rutger Bosker"],"categories":null,"content":" ","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"470925b5e95697e8be0a6841dfe55cc4","permalink":"https://hrbosker.github.io/publication/bosker-2013-ehll-juncture/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-2013-ehll-juncture/","section":"publication","summary":" ","tags":null,"title":"Juncture (prosodic)","type":"publication"},{"authors":["Hans Rutger Bosker"],"categories":null,"content":" ","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"0ee034120bdf164a6dde53c53d7c784a","permalink":"https://hrbosker.github.io/publication/bosker-2013-ehll-sibilants/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-2013-ehll-sibilants/","section":"publication","summary":" ","tags":null,"title":"Sibilant consonants","type":"publication"},{"authors":["Hans Rutger Bosker","Anne-France Pinget","Hugo Quené","Ted Sanders","Nivja H. de Jong"],"categories":null,"content":" ","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"a1709dfd10dc52b4fe5e5d6475ae7e43","permalink":"https://hrbosker.github.io/publication/bosker-etal-2013-lt/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2013-lt/","section":"publication","summary":"The oral fluency level of an L2 speaker is often used as a measure in assessing language proficiency. The present study reports on four experiments investigating the contributions of three fluency aspects (pauses, speed and repairs) to perceived fluency. In Experiment 1 untrained raters evaluated the oral fluency of L2 Dutch speakers. Using specific acoustic measures of pause, speed and repair phenomena, linear regression analyses revealed that pause and speed measures best predicted the subjective fluency ratings, and that repair measures contributed only very little. A second research question sought to account for these results by investigating perceptual sensitivity to acoustic pause, speed and repair phenomena, possibly accounting for the results from Experiment 1. In Experiments 2–4 three new groups of untrained raters rated the same L2 speech materials from Experiment 1 on the use of pauses, speed and repairs. A comparison of the results from perceptual sensitivity (Experiments 2–4) with fluency perception (Experiment 1) showed that perceptual sensitivity alone could not account for the contributions of the three aspects to perceived fluency. We conclude that listeners weigh the importance of the perceived aspects of fluency to come to an overall judgment.","tags":["perceived fluency","utterance fluency","fluency ratings","language proficiency"],"title":"What makes speech sound fluent? The contributions of pauses, speed and repairs","type":"publication"},{"authors":["Hans Rutger Bosker","Jeroen Briaire","Willemijn Heeren","Vincent J. van Heuven","Suzanne R. Jongman"],"categories":null,"content":" ","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"e169c4bcaf6d42512f725a50fca289a1","permalink":"https://hrbosker.github.io/publication/bosker-etal-2010/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2010/","section":"publication","summary":" ","tags":null,"title":"Whispered speech as input for cochlear implants","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://hrbosker.github.io/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9f5fb74c757270807d9c8ecf5259914e","permalink":"https://hrbosker.github.io/empty/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/empty/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://hrbosker.github.io/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f1d044c0738ab9f19347f15c290a71a1","permalink":"https://hrbosker.github.io/research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/research/","section":"","summary":"","tags":null,"title":"Research","type":"widget_page"}]