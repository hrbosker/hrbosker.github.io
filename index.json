[{"authors":["giulio-severijnen"],"categories":null,"content":"Giulio Severijnen is a PhD student at the Donders Center for Cognition [DCC], part of the Donders Institute at Radboud University, Nijmegen, The Netherlands. His supervisors are Prof. James McQueen and Dr. Hans Rutger Bosker. His project investigates between-talker and within-talker variability in prosody production, with a specific focus on lexical stress. Moreover, he also tests how listeners flexibly adapt to this variability in order to successfully comprehend different talkers. The project is funded through a ‘Donders Internal PhD Round’ grant, awarded to Prof. James McQueen, Dr. Hans Rutger Bosker, and Dr. Ashley Lewis.\n","date":1672531200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1672531200,"objectID":"326756edd9bfed8d3d7aa32573218298","permalink":"https://hrbosker.github.io/author/giulio-severijnen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/giulio-severijnen/","section":"authors","summary":"Giulio Severijnen is a PhD student at the Donders Center for Cognition [DCC], part of the Donders Institute at Radboud University, Nijmegen, The Netherlands. His supervisors are Prof. James McQueen and Dr.","tags":null,"title":"Giulio Severijnen","type":"authors"},{"authors":null,"categories":null,"content":"Hans Rutger Bosker is Assistant Professor at the Donders Center for Cognition [DCC], part of the Donders Institute at Radboud University, Nijmegen, The Netherlands. He leads the Speech Perception in Audiovisual Communication [SPEAC] research group, funded by the ERC Starting Grant ‘HearingHands’ [101040276] that started in September 2022. This research grant investigates the contribution of simple up-and-down hand movements (‘beat gestures’) to audiovisual speech perception. Hans Rutger Bosker is also Senior Investigator at the Max Planck Institute for Psycholinguistics, Nijmegen.\nORCID iD: 0000-0002-2628-7738.\nWondering how to pronounce my name? I have a double first name “Hans Rutger” (no hyphen), “Bosker” is the surname. It’s pronounced /ɦɑns ˈʀʏt.xəɹ ˈbɔs.kəɹ/ in Dutch but for the less phonetically savvy among us, here’s Google pronouncing my name (with rather depressed prosody). But feel free to simply call me “Hans”.\n","date":1672531200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1672531200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://hrbosker.github.io/author/hans-rutger-bosker/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/hans-rutger-bosker/","section":"authors","summary":"Hans Rutger Bosker is Assistant Professor at the Donders Center for Cognition [DCC], part of the Donders Institute at Radboud University, Nijmegen, The Netherlands. He leads the Speech Perception in Audiovisual Communication [SPEAC] research group, funded by the ERC Starting Grant ‘HearingHands’ [101040276] that started in September 2022.","tags":null,"title":"Hans Rutger Bosker","type":"authors"},{"authors":["ronny-bujok"],"categories":null,"content":"Ronny Bujok is a PhD student at the Max Planck Institute for Psycholinguistics [MPI], Nijmegen, The Netherlands. His supervisors are Prof. Antje S. Meyer and Dr. Hans Rutger Bosker. His project investigates how simple up-and-down hand gestures (‘beat gestures’) influence low-level speech perception.\n","date":1640995200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1640995200,"objectID":"9ad7ef51d4fe5b9fe1fa4062f61d116c","permalink":"https://hrbosker.github.io/author/ronny-bujok/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ronny-bujok/","section":"authors","summary":"Ronny Bujok is a PhD student at the Max Planck Institute for Psycholinguistics [MPI], Nijmegen, The Netherlands. His supervisors are Prof. Antje S. Meyer and Dr. Hans Rutger Bosker. His project investigates how simple up-and-down hand gestures (‘beat gestures’) influence low-level speech perception.","tags":null,"title":"Ronny Bujok","type":"authors"},{"authors":["ivy-mok"],"categories":null,"content":"Ivy Mok is enrolled at Radboud University in the MA Linguistics program. She is currently writing her MA thesis at the SPEAC research group, under supervision of Dr. Hans Rutger Bosker and Ronny Bujok. Her project investigates audiovisual prosody perception in noise, with a focus on beat gestures.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"c3404aabaaa0d65aba38d717d8c4d911","permalink":"https://hrbosker.github.io/author/ivy-mok/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ivy-mok/","section":"authors","summary":"Ivy Mok is enrolled at Radboud University in the MA Linguistics program. She is currently writing her MA thesis at the SPEAC research group, under supervision of Dr. Hans Rutger Bosker and Ronny Bujok.","tags":null,"title":"Ivy Mok","type":"authors"},{"authors":["orhun-ulusahin"],"categories":null,"content":"Orhun Uluşahin is a PhD student at the Max Planck Institute for Psycholinguistics [MPI], Nijmegen, The Netherlands. His supervisors are Prof. Antje S. Meyer, Prof. James McQueen, and Dr. Hans Rutger Bosker. His project targets the production-perception interface, using phonetic convergence as the main paradigm. Specifically, he tests whether familiarity with a talker modulates the extent to which interlocutors converge towards each other on a phonetic level.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8bbf000eee2b19fa67856597ac935bd4","permalink":"https://hrbosker.github.io/author/orhun-ulusahin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/orhun-ulusahin/","section":"authors","summary":"Orhun Uluşahin is a PhD student at the Max Planck Institute for Psycholinguistics [MPI], Nijmegen, The Netherlands. His supervisors are Prof. Antje S. Meyer, Prof. James McQueen, and Dr. Hans Rutger Bosker.","tags":null,"title":"Orhun Uluşahin","type":"authors"},{"authors":null,"categories":null,"content":" Table of Contents NiCLS PiNCeR Other corpora Voice recordings are privacy-sensitive data; please use them respectfully and for academic purposes only. NiCLS Hans Rutger Bosker, Martin Cooke (2020). Enhanced amplitude modulations contribute to the Lombard intelligibility benefit: Evidence from the Nijmegen Corpus of Lombard Speech. The Journal of the Acoustical Society of America, 147(2), 721-730, doi:10.1121/10.0000646. PDF Cite Dataset DOI Nijmegen Corpus of Lombard Speech [NiCLS] Lead author: Hans Rutger Bosker. https://hdl.handle.net/1839/21ee5744-b5dc-4eed-9693-c37e871cdaf6 Dutch 42 talkers (37 F/5 M); each reading a folk story of 56 utterances; in both plain speech (read in quiet) and Lombard speech (read in noise over headphones) 3968 wav files (1984 Lombard-plain pairs) + forced-alignment TextGrids CC BY-NC-ND 4.0 license PiNCeR Joe Rodd, Hans Rutger Bosker, Mirjam Ernestus, Phillip M. Alday, Antje S. Meyer, Louis ten Bosch (2020). Control of speaking rate is achieved by switching between qualitatively distinct cognitive ‘gaits’: Evidence from simulation. Psychological Review 127(2), 281-304, doi:10.1037/rev0000172. PDF Cite Dataset DOI Picture Naming at Cued Rates [PiNCeR] corpus Lead author: Joe Rodd. https://hdl.handle.net/1839/7c210d30-bb55-4cbe-9eeb-baf18570460c Dutch 25 talkers (21 F/4 M) naming disyllabic pictures arranged on a ‘clock face’; produced at three different cued rates: fast, medium, slow with (manual) word-level and (forced-aligned) syllable-level annotations; with eye-tracking data CC BY 4.0 license Other corpora Many other speech, video, and picture corpora are publicly available nowadays. Please see our Other resources for some examples that we ourselves have used in the past.\n","date":1657152000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657152000,"objectID":"ab1257ee950262edc01075a4ef751f4c","permalink":"https://hrbosker.github.io/resources/corpora/","publishdate":"2022-07-07T00:00:00Z","relpermalink":"/resources/corpora/","section":"resources","summary":"Open collections of speech recordings.","tags":null,"title":"Corpora","type":"book"},{"authors":null,"categories":null,"content":" Table of Contents Token Sort Ratio PraatVSCode POnSS Headphone screening tests Token Sort Ratio Table showing example TSR scores for various responses Hans Rutger Bosker (2021). Using fuzzy string matching for automated assessment of listener transcripts in speech intelligibility studies. Behavior Research Methods, 53(5), 1945-1953, doi:10.3758/s13428-021-01542-4. PDF Cite Dataset DOI Token Sort Ratio: automatically quantifying response accuracy for speech intelligiblity experiments. Author: Hans Rutger Bosker. https://tokensortratio.netlify.app The Token Sort Ratio [TSR] score is a fuzzy string matching metric that – at the most basic level – quantifies the orthographic match between a target string and a response string (value between 0 = no match and 100 = perfect match). The TSR score has been shown to strongly correlate with human-generated scores of percentage words correct (Bosker, 2021). It is an efficient, reliable, and accurate tool for use in speech perception research (e.g., studies that examine the perception of speech in adverse listening conditions, or degraded speech) or for generating listener intelligibility measures in clinical disciplines such as speech-language pathology or audiology. PraatVSCode PraatVSCode syntax highlighting Author: Orhun Uluşahin. https://github.com/orhunulusahin/praatvscode Praat is an excellent software package for speech analysis, annotation, and manipulation. However, it’s scripting interface is - let’s put it this way - ‘suboptimal’. PraatVSCode is an extension for Visual Studio Code (see screenshot) that provides syntax highlighting, autocompletion, and even an array of code snippets that writes itself. Moreover, it allows running and debugging of scripts by Praat from inside Visual Studio Code. How to install: Download and install Visual Studio Code. Under View \u0026gt; Extensions, search for ‘PraatVSCode’, and click install. See here for running Praat scripts from inside Visual Studio Code. MIT license POnSS POnSS annotation workflow screenshots Joe Rodd, Caitlin Decuyper, Hans Rutger Bosker, Louis ten Bosch (2021). A tool for efficient and accurate segmentation of speech data: Announcing POnSS. Behavior Research Methods, 53, 744-756, doi:10.3758/s13428-020-01449-6. PDF Cite Dataset DOI Pipeline for Online Speech Segmentation [POnSS] Lead author: Joe Rodd. https://git.io/Jexj3 POnSS is a browser-based system that is specialized for the task of segmenting the onsets and offsets of words, that combines automatic speech recognition (WebMAUS) with limited human input. MIT license Headphone screening tests When running experiments online, you may want your participants to use headphones or in-ear buds (i.e., no speakers). Moreover, you may want to verify that they are wearing them ’the right way around’: [L] in their left ear, [R] in their right ear. This is particularly important when testing multitalker listening conditions and/or virtual auditory environments. Several tools exist to verify whether participants are using headphones (as instructed) or not (exclude ’m!), based on different psychoacoustic binaural phenomena. We have implemented these on Gorilla and PsyToolkit. Tone attenuation based on phase-cancellation Woods, K. J. P., Siegel, M. H., Traer, J., \u0026amp; McDermott, J. H. (2017). Headphone screening to facilitate web-based auditory experiments. Attention, Perception, \u0026amp; Psychophysics, 79(7), 2064–2072. doi:10.3758/s13414-017-1361-2\nGeneral idea: binaural tones are played and participants are asked to indicate which tone out of three is quietest. Some binaural tones are played 180° out-of-phase, attenuating perceived loudness if using speakers, but not when using headphones/in-ear buds. 3min test, shared by authors on Github We have implemented this headphone screening test on Gorilla and PsyToolkit. Send us an email and we’d be happy to share! Huggins Pitch illusion Milne, A. E., Bianco, R., Poole, K. C., Zhao, S., Oxenham, A. J., Billig, A. J., \u0026amp; Chait, M. (2021). An online headphone screening test based on dichotic pitch. Behavior Research Methods, 53(4), 1551–1562. doi:10.3758/s13428-020-01514-0\nGeneral idea: participants are played white noise in one ear and the same white noise but with a phase shift of 180° over a narrow frequency band to the other ear. This results in the perception of a faint tone embedded in the noise but only when using headphones/in-ear buds. Otherwise, listeners only perceive white noise (i.e., without the faint embedded tone). 3min test, shared by authors on Gorilla We have also implemented this headphone screening test on PsyToolkit. Send us an email and we’d be happy to share! ITD and ILD manipulations General idea: participants are played six trials of three binaural white noise sounds. Interaural time differences (ITDs) and interaural level differences (ILDs) are applied to the L/R channels of the stereo stimuli such that two noise sounds are perceived as coming from the left, and one as coming from the right. Participants …","date":1657152000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657152000,"objectID":"e33bb1b9ecf7be816c053c63be363458","permalink":"https://hrbosker.github.io/resources/tools/","publishdate":"2022-07-07T00:00:00Z","relpermalink":"/resources/tools/","section":"resources","summary":"Open research tools for data collection, annotation, and analysis.","tags":null,"title":"Research tools","type":"book"},{"authors":null,"categories":null,"content":" Table of Contents Praat syntax List of available scripts Helpful links License The Praat scripts shared here are first and foremost intended as a lab archive, providing script snippets we frequently use. This means they require customization for each individual new project. Use at your own risk! Praat syntax Over the years, Praat has had three types of syntax:\nExtract part... 0 0.1 rectangular 1 no (Praat versions 5.3.43 and older; before April 2013) do(\u0026#34;Extract part\u0026#34;, 0, 0.1, \u0026#34;rectangular\u0026#34;, 1, \u0026#34;no\u0026#34;) (between 5.3.44 and 5.3.62; April 2013 - January 2014) Extract part: 0, 0.1, \u0026#34;rectangular, 1, \u0026#34;no\u0026#34; (5.3.63 and after; after January 2014) Current Praat versions are compatible with older and newer syntax types, and mixes thereof. The scripts shared here primarily use the first type of syntax (…shows my age), with occasional lines using the latest type of syntax. [ADVERTISEMENT:] Did you know the syntax highlighting in PraatVSCode works with either syntax?\nList of available scripts Save all Praat can only save one object at a time for you. If you have multiple objects in your object window you’d like to save in one go, you can use this script.\nAnnotate This script streamlines an annotation workflow: it presents a TextGrid for manual annotation to the user, you perform some changes, and when you click Next, it automatically saves the changes and presents the next TextGrid, and so on.\nBatch processing This script is an in-house template / starting point for batch processing multiple files. Adapt it to your own needs to apply a particular function to multiple files or multiple time intervals within each file.\nMove to zero-crossings This script automatically moves all boundaries in a given tier in a TextGrid file to zero-crossings, which is important for extracting sound intervals. Specifically, it adds a tier ’to0x’ at the top of the TextGrid that is identical to a given input tier, except that all boundaries are at zero-crossings.\nInterpolate F0 continuum This script creates an F0 continuum for two segmentally matching words (e.g., for a lexical stress pair, like SUBject vs. subJECT). First, it matches the two words in duration and then interpolates the F0 contour linearly in 11 steps (no.\nHelpful links Not finding what you were looking for? There are thousands of other Praat scripts available online. Three resources with particularly useful code are:\nVocal Toolkit plugin is a plugin for Praat. When installed, you can call various new functions from a button within Praat. However, it’s a little risky if you don’t know the ins-and-outs of a particular function, so always check the raw code here: [WINDOWS] “C:\\Users\\(Username)\\Praat\\plugin_VocalToolkit” [MAC] “/Users/(UserName)/Library/Preferences/Praat Prefs/” Matt Winn’s Listen Lab with some really fun Youtube Praat tutorials Holger Mitterer’s website Will Styler’s repo License All scripts are shared under an MIT license.\n2022, Hans Rutger Bosker\n","date":1657152000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657152000,"objectID":"3c695000c9960ffc206501e907fecded","permalink":"https://hrbosker.github.io/resources/scripts/","publishdate":"2022-07-07T00:00:00Z","relpermalink":"/resources/scripts/","section":"resources","summary":"Open Praat scripts for speech analysis, manipulation, and synthesis","tags":null,"title":"Scripts","type":"book"},{"authors":null,"categories":null,"content":" Table of Contents So how do I… License The how-to’s shared here are first and foremost intended as a lab archive, providing some starting ground for tasks and documents we often deal with. This means they require customization for each individual new project. Use at your own risk! So how do I… ...record audio We’ll cover making clean audio recordings in Audacity and in SpeechRecorder. Audacity is easy-to-use and perfect for making a single (long) recording of a speaker. SpeechRecorder presents individual word/sentence prompts to a speaker, saving each utterance separately.\n...annotate in Praat We’ll cover making TextGrid annotations in Praat. We’ll read a wav file in Praat, create an empty TextGrid with several tiers, add boundaries delimiting individual words and phonemes in the recording, and save the annotations to a TextGrid file.\n...script in Praat Here’s a brief intro into the Praat scripting language. We’ll cover how to write and run a script, point you to some tutorials, highlight some of the strange quirks in the Praat scripting language, and provide some scripts we find useful ourselves.\n...run a power analysis This R script runs a simulation-based power analysis for a simple 2AFC experimental design. This is by no means a one-size-fits-all solution to all your power needs. Use at your own risk!\n...write a paper Here’s a Word template that includes all basic sections of a paper, template statements (e.g., ethics, participants specs, etc.), ‘fields’ to automatically update figure/table numbers, heading styles, and Zotero for reference management.\n...get published This describes which journal to choose, which suggested (or dispreferred?) reviewers to mention, an example cover letter, how to read the submission system, when to contact the editorial office about your submission, and how to respond to reviewers.\nLicense All documents are shared under an MIT license.\n2022, Hans Rutger Bosker\n","date":1657152000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657152000,"objectID":"177cb2ca635117e123be4e5ebd30ec44","permalink":"https://hrbosker.github.io/resources/how-to/","publishdate":"2022-07-07T00:00:00Z","relpermalink":"/resources/how-to/","section":"resources","summary":"Guidelines and templates for research-related tasks","tags":null,"title":"How to's","type":"book"},{"authors":null,"categories":null,"content":" Table of Contents Video/audio editing Corpora Writing tools Online experimenting Mailing lists Video/audio editing Praat praat.org the number 1 speech-editing software in academia supports speech measurements, annotation in TextGrids, manipulation, synthesis scripting interface supports batch processing recommended scripting interface: PraatVSCode ffmpeg ffmpeg.org a command line tool for batch video processing ’lives’ in the terminal (e.g., command prompt) Adobe Premiere Pro is a great video-editor when working on individual files, but is not the best solution for batch processing. ffmpeg is great at efficiently and quickly extracting the audio channels from a large set of video files, converting mpg to mp4, manipulating audio/video temporal alignment (asynchrony), etc. MediaPipe mediapipe.dev 2D video motion-tracking tool in Python, developed by Google input: video file of a single person (OpenPose is preferred for multi-person tracking). output: x, y, (estimated) z coordinates of body landsmarks + video with superimposed tracking skeleton. here’s a great tutorial by Wim Pouw and James Trujillo at Envision Bootcamp. WebMAUS https://clarin.phonetik.uni-muenchen.de/BASWebServices/interface/WebMAUSBasic forced-alignment tool taking wav files and txt files with orthographic transcripts as input, providing TextGrids as output ’lives’ online: you upload wav and txt files and download TextGrids large set of languages available annotations at word-level and at phone-level (not syllable-level) EasyAlign http://latlcui.unige.ch/phonetique/easyalign.php forced-alignment tool taking wav files and txt files with orthographic transcripts as input, providing TextGrids as output ’lives’ in Praat (plugin) French, English, Spanish, Brazilian Portuguese, Taiwan Min annotations at word-level, syllable-level and phone-level Corpora MultiPic https://www.bcbl.eu/databases/multipic/ standardized set of 750 drawings with multilingual name agreement and visual complexity norms in color; 300 x 300 pixels; DPI=96 pixels/inch Spanish, English (British), German, Italian, French, Dutch (Belgium), Dutch (Netherlands) also useful when looking for ‘imageable’ words Severens et al. https://www.ugent.be/pp/experimentele-psychologie/en/research/documents/pnn/overview.htm timed naming norms for 590 pictures in Belgian Dutch black-and-white line drawings name agreement, freq, aoa, h-statistic, naming latencies SUBTLEX http://crr.ugent.be/programs-data/subtitle-frequencies/subtlex-nl database of Dutch word frequencies based on 44 million words from film and television subtitles also available for other languages, including US English (SUBTLEX-US), UK English (SUBTLEX-UK), Mandarin Chinese (SUBTLEX-CH), Spanish (SUBTLEX-ESP), German (SUBTLEX-DE), Greek (SUBTLEX-GR), Polish (SUBTLEX-PL), Italian (SUBTLEX-IT), Brazilian Portuguese (SUBTLEX-PT-BR) reliable predictor of lexical decision reaction times, outperforming Google Books Ngram=1 (Brysbaert et al., 2011) ANW Algemeen Nederlands Woordenboek Dutch word list, allowing searching with regular expressions (“spraa*”) and with particular word characteristics (number of syllables, stress on syllable n, etc.) NOTE. I’ve found that the search lists are not exhaustive. Failure to find certain words in ANW does not necessarily mean they do not exist. Lombard speech corpora Acted clear speech corpus: English; 1 male talker; ’normal’ sentences; 25 items; babble-modulated noise; Mayo et al. (2012), doi:10.7488/ds/138. Hurricane natural speech corpus: English; 1 male talker; Harvard sentences (720 items) and MRT sentences (300 items); speech-modulated noise; Cooke et al. (2013), doi:10.7488/ds/140. DELNN: L1 Dutch and L2 English from 30 native speakers of Dutch (+9 native speakers of US English as control); speech-shaped noise; Marcoux (2022, PhD thesis). RaLoCo: Dutch; 78 talkers; 48 sentences; speech-shaped noise; Shen (2022, PhD thesis). Additional info, including human listening effort ratings and HEGP scores (spectral glimpsing metric of intelligibility). Also see our very own NiCLS corpus of Lombard speech. Writing tools Thesaurus thesaurus.com for looking up synonyms indispensable gizmo when scribbling palimpsests, particularly useful for L2 writers of English like myself also gives antonyms, example sentences, and related words links to definitions at dictionary.com Zotero zotero.org reference manager use the Zotero Connector in your browser to store a paper, including fulltext and all bibliographic specs use the Zotero Word Plugin to cite papers in a Word document, automatically generating a bibliography at the end of the document easily change bibliography styles (from author-year in APA to numbered-lists in IEEE) supports open fulltext search, notes and tags, organize in folders, etc. preferred over other reference managers because Zotero is institute-independent, free, open-source, and very flexible. Overleaf Overleaf Online LaTeX editor it’s like Google Docs but then in LaTeX collaborative …","date":1657152000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1657152000,"objectID":"45016db0825c98667af5f5e841b09e50","permalink":"https://hrbosker.github.io/resources/other-resources/","publishdate":"2022-07-07T00:00:00Z","relpermalink":"/resources/other-resources/","section":"resources","summary":"Open research resources that others have shared and we frequently use.","tags":null,"title":"Other resources","type":"book"},{"authors":null,"categories":null,"content":"Do you hear VOORnaam or voorNAAM? In the video below, you’ll see a (randomly selected…) talker say a Dutch word. Is he saying VOORnaam (Eng. “first name”, with stress on the first syllable VOOR-) or voorNAAM (Eng. “respectable”, with stress on the second syllable -NAAM).\nIn other words: where do you hear the stress?\nSpoiler: click here to reveal the video specs Audio: ambiguous; midway between VOORnaam – voorNAAM Lips: head taken from a recording of VOORnaam Hands: beat gesture aligned to the first syllable Now play the video below. Where do you hear the stress now?\nSpoiler: click here to reveal the video specs Audio: ambiguous; midway between VOORnaam – voorNAAM Lips: head taken from a recording of VOORnaam Hands: beat gesture aligned to the second syllable Explanation The audio in these videos is perfectly identical: it has been manipulated to be ambiguous, falling roughly midway between VOORnaam and voorNAAM. The head of the talker is also the same: it has been copy-pasted from a video recording of the talker saying VOORnaam. The only difference between these two videos is the timing of the hand gesture. In the first clip, the talker produces a beat gesture on the first syllable, while in the second video the talker gestures on the second syllable. Our experiments show that this slight change in timing has major consequences for perception. When we ask a group of Dutch participants to indicate what word they hear the talker say, the majority reports hearing VOORnaam in the first clip, but voorNAAM in the second clip.\nReally? Convince me… This is Figure 1 from Bosker \u0026amp; Peeters (2021). In the bottom left panel, you see the proportion of ‘I hear stress on the first syllable’ responses for when the beat gesture falls on the first syllable (blue line) or on the second syllable (red line). The blue line lies above the red line, indicating an overall bias to report more ‘stress on first syllable’ responses when the gesture falls on the first vs. second syllable. The difference between the lines is sizable, averaging around 20%.\nFigure 1 in Bosker \u0026amp; Peeters (2021) How hands help us hear When we have a face-to-face conversation, we don’t only exchange sounds. We also move our head, hands, and body to the rhythm of the speech. Beat gestures are relatively ‘simple’ up-and-down hand gestures that are closely aligned to the rhythm of speech. They tend to fall on the stressed syllable in free-stress languages, such as English and Dutch. These videos demonstrate that people are sensitive to the timing of beat gestures, influencing lexical stress perception. In Bosker \u0026amp; Peeters (2021), this effect was termed the manual McGurk effect. That is, just like seeing a talker close their lips can make you hear the sound /b/ in the classic McGurk effect (McGurk \u0026amp; McDonald, 1976), so can the timing of hand gestures influence speech perception in the manual McGurk effect.\nWhy is this important? The manual McGurk effect is the first demonstration of how the timing of hand gestures influences low-level speech perception. Even the simplest flicks-of-the-hands that do not convey any particular meaning of themselves can shape what words you hear. This promises that these seemingly unimportant hand gestures contribute meaningfully to audiovisual speech comprehension. Perhaps ’enriching’ our speech with carefully timed gestures can help our audience understand our spoken message, particularly in challenging listening conditions, such as in noise.\nRelevant papers Hans Rutger Bosker, David Peeters (2021). Beat gestures influence which speech sounds you hear. Proceedings of the Royal Society B: Biological Sciences, 288, 20202419, doi:10.1098/rspb.2020.2419. PDF Cite Dataset DOI Ronny Bujok, Antje S. Meyer, Hans Rutger Bosker (2022). Visible lexical stress cues on the face do not influence audiovisual speech perception. In Proceedings of Speech Prosody 2022 (ed. S. Frota, M. Cruz, and M. Vigário), 259-263, doi:10.21437/SpeechProsody.2022-53. PDF Cite Dataset DOI Ronny Bujok, Antje S. Meyer, and Hans Rutger Bosker (2022). Audiovisual perception of lexical stress: Beat gestures are stronger visual cues for lexical stress than visible articulatory cues on the face. PsyArXiv Preprints, doi:10.31234/osf.io/y9jck, data:https://osf.io/4d9w5/\n","date":1657584000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657584000,"objectID":"f4f8cbb39ecc8b613476e3bda59a298d","permalink":"https://hrbosker.github.io/demos/manual-mcgurk/","publishdate":"2022-07-12T00:00:00Z","relpermalink":"/demos/manual-mcgurk/","section":"demos","summary":"How hands help us hear...","tags":null,"title":"Manual McGurk effect","type":"demos"},{"authors":null,"categories":null,"content":" We’ll cover making clean audio recordings in Audacity and in SpeechRecorder. Audacity is easy-to-use and perfect for making a single (long) recording of a speaker. SpeechRecorder presents individual word/sentence prompts to a speaker, saving each utterance separately. Audacity Audacity is a free, open-source, and cross-platform audio software package. It is easy to use, very intuitive, and fast. It is particularly suited for making a single (long) recording of a speaker, after which you manually extract the relevant words/sentences from this raw recording.\nDownload, install, and open Audacity: https://www.audacityteam.org/ First check the sampling frequency. Typically, 48 kHz or 44.1 kHz will do just fine. Audacity’s default is to make stereo recordings, with each recording containing two channels: In most cases, it’s better to make mono recordings (with only one channel) to make annotating and manipulating the speech a lot easier. Change the settings at the top to MONO: Then click the red button to start a recording… …which should look something like this: When you’re done recording, click the yellow square to stop. Do not SAVE projects in Audacity, but instead EXPORT audio files. Go to: File \u0026gt; Export Audio…, and then click the ‘Save as type’ drop-down menu to select the file format. Select .wav for uncompressed (raw) audio which is best suited for speech manipulations, or .mp3 for compressed (processed) audio. Audacity is also great for quickly converting a batch of .wav files to .mp3. Open Audacity, drag a selection of files from a folder into Audacity, and go to File \u0026gt; Export Multiple. In the pop-up window, select MP3 Files as Export format, choose an Export location, and keep the other defaults (in particular, Split files based on: Tracks; Name files: Using Label/Track Name). Click Export and voila, you now have an .mp3 file for each .wav file. Tips n tricks But how to make recordings that are ‘clean’: with little noise and of good quality?\nFor longer recordings, ask your speaker to sit in a relaxed position that they can keep up for a longer period of time. People typically tend to slouch during a recording, changing the distance between them and the table mic, thus changing the intensity of speech appearing early vs. later in your recording. If you ask your speaker to try to ‘already slouch a little’, this can help avoid large variability in intensity.\nBefore recording your items, start a dummy recording and ask the speaker to count to 100. Then adjust the volume/sensitivity/gain of the microphone. You want the average intensity of the speech to fall roughly in between 0.5 and -0.5 in Audacity. So this is good:\nThis is risky because it’s approaching 1 and -1: This is bad because it’s outside 1 and -1. This is called ‘clippings’ and it’s very well audible in the signal. These two audio clips contain the same telephone number, but the first is clean and the second has clippings: CLEAN WITH CLIPPINGS When boosting the volume/sensitivity/gain of the microphone, be aware that – by doing so – you may actually be boosting the background noise too. It’s often preferable to move the location of the mic closer to the speaker compared to amplifying the input signal.\nWhen you need the mic to be particularly close to the speaker (e.g., when using a head-mounted mic), make sure the mic is not too close to the speakers lips as that will introduce puffs of loud noise for many stop consonants. Better to aim for the chin!\nWhy not ask the speaker to go through your list of items from top to bottom first, and then repeat all items from bottom to top? This will give you two recordings of each item, allowing you to select the best one. Moreover, items at the top and bottom of lists typically sound different from the rest. First items typically have loud intensity and raised pitch, while last items have low intensity and pitch. By switching the order of items, you may be able to circumvent some of these order effects.\nChopping up the recording When you’ve finished recording with Audacity, and have extracted the audio as a .wav file, you will then need to isolate the individual items (e.g., words or sentences) from that one long recording. There’s different solutions for this task:\nDo it by hand. Read the .wav file into Praat, create an empty TextGrid, and insert boundaries at the onset and offset of every item. Once you’re done, you select the Sound and TextGrid objects together in Praat, and select an option from the Extract menu, such as Extract all intervals... or Extract non-empty intervals... and save the individual items.\nGo full automatic. You can use a forced-aligner to find the words inside your recording. We tend to use WebMAUS (for word- and phoneme-level annotations) or EasyAlign (for syllable-level annotations). In both cases, you provide the forced-aligner with the .wav file and a .txt file that contains the orthographic content of the recording (i.e., the prompts that your speaker was asked to read out). …","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5b04aa303363f6e4859c056f8f8bc3cd","permalink":"https://hrbosker.github.io/resources/how-to/record-audio/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resources/how-to/record-audio/","section":"resources","summary":"We’ll cover making clean audio recordings in Audacity and in SpeechRecorder. Audacity is easy-to-use and perfect for making a single (long) recording of a speaker. SpeechRecorder presents individual word/sentence prompts to a speaker, saving each utterance separately.","tags":null,"title":"...record audio","type":"book"},{"authors":null,"categories":null,"content":"Praat can only save one object at a time for you. If you have multiple objects in your object window you’d like to save in one go, you can use this script. It can either save objects by their object name or by their id number.\nYou can also download the script as a .praat file.\n################################################################################ ### Hans Rutger Bosker, Radboud University ### HansRutger.Bosker@ru.nl ### Date: 23 June 2022, run in Praat 6.2.12 on Windows 10 ### License: CC BY-NC 4.0 ################################################################################ ###\u0026gt;\u0026gt; This script saves all selected objects to the directory \u0026#39;dir_out$\u0026#39; ###\u0026gt;\u0026gt;\twith either: ###\u0026gt;\u0026gt; - their object name (e.g., \u0026#34;sentence1.wav\u0026#34;) ###\u0026gt;\u0026gt;\t\u0026gt; set variable \u0026#39;save_method$\u0026#39; to \u0026#34;name\u0026#34; [default] ###\u0026gt;\u0026gt; - their id number in the Praat object window (e.g., \u0026#34;42.wav\u0026#34;) ###\u0026gt;\u0026gt;\t\u0026gt; set variable \u0026#39;save_method$\u0026#39; to \u0026#34;id\u0026#34; ###\u0026gt;\u0026gt; ###\u0026gt;\u0026gt; Sounds are saved as .wav files, ###\u0026gt;\u0026gt; other object types (TextGrids, Spectrum, etc.) are saved ###\u0026gt;\u0026gt; with their own extension type (.TextGrid, .Spectrum). ###\u0026gt;\u0026gt; ###\u0026gt;\u0026gt; Default: the script will overwrite pre-existing files. ###\u0026gt;\u0026gt; Set variable \u0026#39;overwrite$\u0026#39; to \u0026#34;no\u0026#34; if you want Praat ###\u0026gt;\u0026gt; to throw an error instead. ################################################################################ ### Variables you will definitely need to customize: ################################################################################ ### Where should the selected objects be saved? dir_out$ = \u0026#34;C:\\Users\\hanbos\\mysounds\u0026#34; ### Should Praat overwrite pre-existing files? overwrite$ = \u0026#34;yes\u0026#34; #overwrite$ = \u0026#34;no\u0026#34; ### Do you want to save each object by its object name or by its id number? ### If object name, then use \u0026#34;name\u0026#34; (e.g., \u0026#34;sentence1.wav\u0026#34;). ### If object id number, then use \u0026#34;id\u0026#34; (e.g., \u0026#34;42.wav\u0026#34;). save_method$ = \u0026#34;name\u0026#34; #save_method$ = \u0026#34;id\u0026#34; ################################################################################ ### Before we start, let\u0026#39;s check whether you\u0026#39;ve entered sensible ### input for the variables above... ################################################################################ ### Let\u0026#39;s check if the output directory exists. ### This script will throw an error if the directory doesn\u0026#39;t exist ### (i.e., it won\u0026#39;t write to a mysterious temp directory). ### First check whether the input directory ends in a backslash (if so, removed) if right$(dir_out$,1)=\u0026#34;/\u0026#34; dir_out$ = left$(dir_out$,length(dir_out$)-1) elsif right$(dir_out$,1)=\u0026#34;\\\u0026#34; dir_out$ = left$(dir_out$,length(dir_out$)-1) endif ### Then create a temporary txt file in the folder ### and try to write it to the output folder. ### NOTE: The \u0026#34;nocheck\u0026#34; below asks Praat not to complain if the folder ### does *not* exist. We\u0026#39;ll manually check whether the saving of this ### temp txt file has succeeded or not further down below. temp_filename$ = dir_out$ + \u0026#34;/\u0026#34; + \u0026#34;my_temporary_Praat_file.txt\u0026#34; nocheck writeFileLine: temp_filename$, \u0026#34;This is just to check if the directory exists\u0026#34; ### Can the file be found? file_exists_yesno = fileReadable(temp_filename$) if file_exists_yesno = 1 # if you *could* read that temp txt file, # this confirms that the directory is valid. # Then you can delete it. deleteFile: temp_filename$ else # if that file wasn\u0026#39;t readable, that means that the directory wasn\u0026#39;t valid. printline The folder \u0026#39;dir_out$\u0026#39; was not found exit Your directory doesn\u0026#39;t exist. Check spelling. The directory must *already* exist. endif ################################################################################ ################################################################################ ################################# SCRIPT ################################# ################################################################################ ################################################################################ ### Make sure you\u0026#39;ve selected the objects you\u0026#39;d like to save in ### the Praat object window. If nothing is selected, the script exits. nSelected = numberOfSelected() if nSelected = 0 exit No objects selected. endif ### Store the object id numbers in an array for thisObject to nSelected objectArray [\u0026#39;thisObject\u0026#39;] = selected(\u0026#39;thisObject\u0026#39;) endfor ### Loop through this array and for each id number ### select the corresponding object and save it. for thisArrayNumber to nSelected objectId = objectArray [\u0026#39;thisArrayNumber\u0026#39;] select \u0026#39;objectId\u0026#39; type$ = extractWord$(selected$(), \u0026#34;\u0026#34;) name$ = extractLine$(selected$(), \u0026#34; \u0026#34;) if save_method$ = \u0026#34;name\u0026#34; if type$ = \u0026#34;Sound\u0026#34; does_file_exist = fileReadable(\u0026#34;\u0026#39;dir_out$\u0026#39;\\\u0026#39;name$\u0026#39;.wav\u0026#34;) if does_file_exist = 1 if overwrite$ = \u0026#34;no\u0026#34; exit The file \u0026#39;dir_out$\u0026#39;\\\u0026#39;name$\u0026#39;.wav\u0026#39; already exists! If you wish to overwrite, set the variable overwrite$ to \u0026#34;yes\u0026#34;. endif endif Write to WAV file... \u0026#39;dir_out$\u0026#39;\\\u0026#39;name$\u0026#39;.wav else does_file_exist = fileReadable(\u0026#34;\u0026#39;dir_out$\u0026#39;\\\u0026#39;name$\u0026#39;.\u0026#39;type$\u0026#39;\u0026#34;) if does_file_exist = 1 if overwrite$ = \u0026#34;no\u0026#34; exit The file \u0026#39;dir_out$\u0026#39;\\\u0026#39;name$\u0026#39;.\u0026#39;type$\u0026#39; already exists! If you wish to overwrite, set the variable overwrite$ to \u0026#34;yes\u0026#34;. …","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c3295f7839cdac0638abc97e77bc14ea","permalink":"https://hrbosker.github.io/resources/scripts/save-all/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resources/scripts/save-all/","section":"resources","summary":"Praat can only save one object at a time for you. If you have multiple objects in your object window you’d like to save in one go, you can use this script.","tags":null,"title":"Save all","type":"book"},{"authors":null,"categories":null,"content":"Remember this one? Do you remember this famous quote, starting at 00:15s?\nThis was Neil Armstrong, landing on the moon on July 20, 1969. But hold on, what is he saying exactly? Let’s listen to the first part of his famous quote:\nIs it:\n“one small step for man”, or: “one small step for a man”? Both transcripts are grammatically viable, but they mean different things. In (1), “man” is used as a synonym of “mankind”, while in (2) “man” is used with the meaning of “person”. Only the second transcript actually fits the remainder of the quote (\u0026#34;…one giant leap for mankind\u0026#34;) and Neil Armstrong himself also claimed that he had uttered the second version (Baese-Berk et al., 2016). But how come people miss the “a” in the recording?\nSpeech is messy When we talk, we don’t produce ‘spaces’ between words. Instead, we join all the words together, producing a connected stream of sound. This is especially true for function words, like “a”, “or”, “for”, and “and”. It is quite likely that Neil Armstrong intended to say “for a man”, but in stringing together the sounds for the words, the “a” was ’lost’ in articulation. This is what speech scientists call ‘reductions’ in speech.\nTaking speech rate into account OK, so the “a” is ‘reduced’ in the original pronunciation. But human perception is not only determined by the input signal alone. Instead, it is heavily context-dependent, taking into account such contextual factors as who is talking, why he is talking, and even how fast the speech is likely to come in!\nEvidence for this comes from ‘context effects’, whereby for instance the acoustic characteristics of a preceding sentence can influence what you hear next. Let’s take speech rate for example. If you hear someone say “That’s one small step…” at a really fast tempo, it is very likely that the next few words will be spoken at a fast rate too. And conversely: if someone happens to speak at a slow rate, the next few sounds will likely be slow too. This means people are likely to interpret the next few sounds in line with the speech rate of the preceding sentence.\nConjuring up the “a” Now let’s see if we can make the “a” in Neil Armstrong’s quote appear and disappear by playing around with the speech rate of only the surrounding speech. Note that we’re not changing anything about the “for (a)” part of the audio clip (highlighted in red in video below). All we’ll do is speed up (compressed by 2) or slow down (compressed by 0.5) the surrounding parts of the recording. Do you hear “for man” or “for a man”?\nClick here to access the audio from the video ORIGINAL CLIP\nCONTEXT SLOWED DOWN\nCONTEXT SPED UP\nWhat happened there? Most listeners will report hearing “for man” in the bottom clip, with the slowed-down context, but “for a man” in the top clip, with the sped-up context. Notably, these clips have the exact same “for a” part; they only differ in the speech rate of the surrounding context. The slow context makes listeners predict that the “for (a)” part was uttered at a slow rate too. But this critical “for (a)” does not contain a really slow “a”, so people ‘miss’ the function word. However, in a fast context, listeners expect the critical “for (a)” to be uttered at a fast rate. This “for (a)” does indeed (kinda) match a really fast and short “a”, so people are more likely to report hearing “for a man”.\nWhy is this important? Context effects abound in perception, and so they also surface in speech perception. People interpret speech sounds differently depending on the surrounding speech rate, formants, and perceived pitch. But even non-acoustic aspects of the context are taken into account: people hear a sound differently depending on the talker’s gender, the talker’s hand gestures, one’s own preceding speech, and there’s even reports that a stuffed toy seen in the background can change what you hear (Hay \u0026amp; Drager, 2010). All these phenomena together shape human speech perception. So if we want Automatic Speech Recognition (the Siri’s, Cortana’s, and Alexa’s of this world) to approach human-like behavior, we need to know each and every aspect that defines what words we (think we) hear. Oh, and apparently it also helps extraterrestrial communication.\nRelevant papers Hans Rutger Bosker (2017). How our own speech rate influences our perception of others. Journal of Experimental Psychology: Learning, Memory, and Cognition, 43(8), 1225-1238, doi:10.1037/xlm0000381. PDF Cite DOI Hans Rutger Bosker (2017). Accounting for rate-dependent category boundary shifts in speech perception. Attention, Perception \u0026amp; Psychophysics, 79, 333-343, doi:10.3758/s13414-016-1206-4. PDF Cite DOI ","date":1657497600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657497600,"objectID":"685c4d1fd8b797d0268cf144ce7ae724","permalink":"https://hrbosker.github.io/demos/conjuring-words/","publishdate":"2022-07-11T00:00:00Z","relpermalink":"/demos/conjuring-words/","section":"demos","summary":"\\\"*That's one small step for (a?) man...*\\\"","tags":null,"title":"Conjuring up words that were never spoken","type":"demos"},{"authors":null,"categories":null,"content":" We’ll cover making TextGrid annotations in Praat. We’ll read a wav file in Praat, create an empty TextGrid with several tiers, add boundaries delimiting individual words and phonemes in the recording, and save the annotations to a TextGrid file. Open Praat Download Praat: https://www.fon.hum.uva.nl/praat/ This will download a zipped folder containing a praat.exe file. Save it somewhere where you can find it later, unzip it, and open praat.exe Here’s a video tutorial from Matt Winn about downloading Praat: https://www.youtube.com/watch?v=QonCpMS5JPg\nWhen you open Praat, two windows pop up. One is called Praat Objects (or: object window), and the other is called Praat Picture. Praat Objects is the most important interface for working with sound and annotations, while Praat Picture is where the figures you draw appear (e.g., waveforms or spectrograms) allowing you to save them as .png or .eps files. In most cases, you can ignore Praat Picture and close it. In the Objects window, there are fixed menus at the top (Praat, New, Open, Save) and bottom (Rename, Copy, Inspect, Info, Remove). On the right is a dynamic menu, which changes depending on which objects are selected (Sounds, TextGrids, Spectra, etc.). Why is it called Praat anyway? Praat was created by Paul Boersma and David Weenink, two Dutch speech scientists from Amsterdam. The Dutch word ‘praat’ /pra:t/ is the imperative form of the verb ’to speak’ (so “speak!”).\nParts of this how-to are adapted from Aletheia Cui’s really clear and helpful Segmentation with Praat tutorial, including many of the screenshots. Praat also has a Praat manual itself: it is available online and offline as part of the Praat software. In Praat, go to Help \u0026gt; Praat Intro. However, the search function in Praat isn’t great so often typing a question into Google is more helpful. Open a sound file Open a sound file: Open \u0026gt; Read from file… The file will show up as a Sound object in the object window, and upon opening will already automatically be selected (highlighted in blue) Note that Praat, like me, has a serious dislike for spaces in filenames. If your file is called “file number one.wav”, Praat will present it in the object window as “file_number_one”, replacing spaces with underscores. This will for instance affect the default name when saving the object in Praat. So here’s a free piece of advice: better avoid spaces in filenames… You can view the sound by clicking View \u0026amp; Edit This will show the sound waveform (oscillogram) on top and a spectrogram below that. Zooming and playing You can zoom in to a different parts of the recording by clicking and dragging inside the waveform to select a part and click [sel] or [CTRL+N] in the bottom left to zoom in to this part. You can also use bak [CTRL+B] to go back to the previous view, [all] [CTRL+A] to view the entire sound file, and [in] [CTRL+I] and [out] [CTRL+O] to gradually zoom in and out. You can play parts of the sound by selecting an interval and hitting [TAB] to play and [ESCAPE] to stop the playback. This is identical to clicking on the interval appearing in the first gray bar at the very bottom of the TextGrid window. Clicking on the second gray bar will play the visible window, while clicking on the third and last gray bar will play the entire sound. Now close the Sound window because we do not only want to view the sound, we also want to annotate it in a TextGrid. Create a TextGrid Go back to the Praat object window, select the Sound by clicking on it (if it wasn’t selected already), and click Annotate \u0026gt; To TextGrid… Praat will then ask you for the names of tiers. Each TextGrid can have multiple tiers, combining for instance word-level annotations (longer intervals demarcating individual words), syllable-level annotations (shorter intervals demarcating the syllables inside the words), and phoneme-level annotations (short intervals demarcating the individual sounds). Tiers inside TextGrids come in two flavors: interval tiers and point tiers. Interval tiers allow you to add ‘boundaries’ that demarcate certain acoustic events (words, syllables, phonemes), indicating where they are and how long they are. For word-level annotations in an interval tier, you’d need two boundaries to demarcate a single word: one at its onset and another at its offset. Point tiers allow you to add ‘points’ that identify certain individual time points, but not the intervals between time points. Interval tiers are the most commonly used type of tier, so we’ll primarily focus on interval tiers. Enter the names of your interval tiers in the top field, separated by space (e.g., word phoneme), and click OK. If you want any of these to be point tiers instead of interval tiers, copy the name of the point tier in the second field (e.g., word phoneme F1 F2 in the first field, defining F1 F2 in the second field as point tiers). You’ll find a new object appear in the object window, namely a(n empty) TextGrid object with the same name as the Sound object. Also, …","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"19843a42353e9324f2ae348db919ea0c","permalink":"https://hrbosker.github.io/resources/how-to/annotate-in-praat/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resources/how-to/annotate-in-praat/","section":"resources","summary":"We’ll cover making TextGrid annotations in Praat. We’ll read a wav file in Praat, create an empty TextGrid with several tiers, add boundaries delimiting individual words and phonemes in the recording, and save the annotations to a TextGrid file.","tags":null,"title":"...annotate in Praat","type":"book"},{"authors":null,"categories":null,"content":"This script streamlines an annotation workflow: it presents a TextGrid for manual annotation to the user, you perform some changes, and when you click Next, it automatically saves the changes and presents the next TextGrid, and so on. This is particularly useful for when you have forced aligned TextGrids (e.g., from WebMAUS or EasyAlign) that you’d like to manually evaluate and edit.\nMoreover, the script keeps track of who annotated what, can continue where you left off yesterday, allows users to enter comments about their annotations, and blinds file names to avoid human annotation biases. The script can be updated to present new empty TextGrids (instead of any pre-existing ones, in case you only have .wav files) or to automatically perform changes to TextGrid tiers/intervals before presenting them for manual annotation.\nYou can also download the script as a .praat file.\n################################################################################ ### Hans Rutger Bosker, Radboud University ### HansRutger.Bosker@ru.nl ### Date: 30 June 2022, run in Praat 6.2.12 on Windows 10 ### License: CC BY-NC 4.0 ################################################################################ ###\u0026gt;\u0026gt; This script reads a directory containing sound files with pre-existing TextGrids, ###\u0026gt;\u0026gt;\tfor instance resulting from a forced aligner (e.g., WebMAUS or EasyAlign). ###\u0026gt;\u0026gt;\tIMPORTANT: Every Sound should have a pre-existing TextGrid file **with the same name**! ###\u0026gt;\u0026gt;\tIt opens every Sound + Textgrid combination, presents it to the user for editing, ###\u0026gt;\u0026gt;\tallows the user to enter comments about the annotations, and then saves the ###\u0026gt;\u0026gt;\tedited TextGrid with \u0026#34;_edited\u0026#34; suffix in the subfolder \u0026#39;edited_textgrids\u0026#39;. ###\u0026gt;\u0026gt;\tUser comments are tracked in the file \u0026#39;annotation_log.txt\u0026#39; in the same subfolder. ###\u0026gt;\u0026gt;\t###\u0026gt;\u0026gt;\t\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; If you **do not** yet have pre-existing TextGrids (i.e., only sound files),\t\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; ###\u0026gt;\u0026gt;\t\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; you can adjust the script to read all .wav files, create new empty TextGrids,\t\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; ###\u0026gt;\u0026gt;\t\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; and present those for editing and saving...\t\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; ###\u0026gt;\u0026gt;\t\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; See the line with \u0026#34;CREATE EMPTY TEXTGRIDS\u0026#34;\t\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; ###\u0026gt;\u0026gt; ###\u0026gt;\u0026gt; This script can be run by multiple users simultaneously, for instance when ###\u0026gt;\u0026gt; multiple annotators are working on the same shared folder. It keeps track ###\u0026gt;\u0026gt; of what files have already been edited: it only presents TextGrids for editing ###\u0026gt;\u0026gt; that do not yet have an \u0026#34;_edited\u0026#34; version pre-existing in the subfolder. ###\u0026gt;\u0026gt; This also means that users can close the script or Praat at anytime ###\u0026gt;\u0026gt; without losing data. Then, next time someone runs the script, it will ###\u0026gt;\u0026gt; start with the files that are \u0026#39;left over\u0026#39; from the previous run. ###\u0026gt;\u0026gt;\tNOTE: This checking of which files already exist can slow the script down ###\u0026gt;\u0026gt;\twhen working with folders with \u0026gt;5000 files... ###\u0026gt;\u0026gt; ###\u0026gt;\u0026gt; At present, the script **only** presents pre-existing tiers and intervals ###\u0026gt;\u0026gt; for editing (e.g., adding boundaries, dragging boundaries around, etc.). ###\u0026gt;\u0026gt; This script can be augmented by automatically adding tiers or intervals ###\u0026gt;\u0026gt; before the TextGrid is presented for editing, so users can annotate ###\u0026gt;\u0026gt; new tiers/intervals. See the line with \u0026#34;ADD/REMOVE TIERS HERE\u0026#34;. ################################################################################ ### Variables you will definitely need to customize: ################################################################################ ### Where can the Sound and TextGrid files be found? dir_in$ = \u0026#34;C:\\Users\\hanbos\\mysounds\u0026#34; ### Do you want to use \u0026#39;blinded\u0026#39; objects in Praat to avoid human biases in annotation? ### Default value: \u0026#34;yes\u0026#34; ### Change to \u0026#34;no\u0026#34; if you want to use original object names. blinded$ = \u0026#34;yes\u0026#34; ################################################################################ ### Before we start, let\u0026#39;s check whether you\u0026#39;ve entered sensible ### input for the variables above... ################################################################################ ### Let\u0026#39;s check if the input directory exists. ### This script will throw an error if the directory doesn\u0026#39;t exist ### (i.e., it won\u0026#39;t write to a mysterious temp directory). ### First check whether the input directory ends in a backslash (if so, removed) if right$(dir_in$,1)=\u0026#34;/\u0026#34; dir_in$ = left$(dir_in$,length(dir_in$)-1) elsif right$(dir_in$,1)=\u0026#34;\\\u0026#34; dir_in$ = left$(dir_in$,length(dir_in$)-1) endif ### Then create a temporary txt file in the folder ### and try to write it to the input folder. ### NOTE: The \u0026#34;nocheck\u0026#34; below asks Praat not to complain if the folder ### does *not* exist. We\u0026#39;ll manually check whether the saving of this ### temp txt file has succeeded or not further down below. temp_filename$ = dir_in$ + \u0026#34;/\u0026#34; + \u0026#34;my_temporary_Praat_file.txt\u0026#34; nocheck writeFileLine: temp_filename$, \u0026#34;This is just to check if the directory exists\u0026#34; ### Can the file be found? file_exists_yesno = fileReadable(temp_filename$) if file_exists_yesno = 1 # if you *could* read that temp txt file, # this confirms that the directory is valid. # Then you can …","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"87e2dde545e138e121782d0c7c595fac","permalink":"https://hrbosker.github.io/resources/scripts/annotate/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resources/scripts/annotate/","section":"resources","summary":"This script streamlines an annotation workflow: it presents a TextGrid for manual annotation to the user, you perform some changes, and when you click Next, it automatically saves the changes and presents the next TextGrid, and so on.","tags":null,"title":"Annotate","type":"book"},{"authors":null,"categories":null,"content":"How fast can your ears go? Listen to this clip of a talker saying the telephone number 496-0356…\n496-0356 – [original] You’ll probably have no problem understanding the same digits when it’s compressed by a factor of 2 (i.e., twice as fast)…\n496-0356 – compressed by 2 And when it’s compressed by a factor of 3?\n496-0356 – compressed by 3 And compressed by 4?\n496-0356 – compressed by 4 Or even by 5?\n496-0356 – compressed by 5 Research has demonstrated that compression rates up to 3 are still doable (kinda…) but intelligibility breaks down quite dramatically for higher compression rates (e.g., Ghitza, 2014; Bosker \u0026amp; Ghitza, 2018). So the last two clips are unintelligible to most listeners.\nThis has been suggested to be due to how our brain works. Our brain is known to ’track’ incoming speech by aligning its ‘brain waves’ (neural oscillations in the theta range, 3-9 Hz) to the syllable rhythm of the speech (amplitude modulations in the temporal envelope). But when the syllables come in too rapidly (\u0026gt;9 Hz), the brain waves can’t keep up, resulting in poor intelligibility.\nMaking unintelligible speech intelligible again But there’s a trick to help the brain keep up. Let’s take the unintelligible clip with the telephone number 496-0356 compressed by a factor of 5. Here it is again:\n496-0356 – compressed by 5 That’s tough, right? No wonder with a syllable rate of over 12 Hz!\nNow let’s chop this clip up into short snippets of 66 ms (“packages”, cf. top panel in the figure at the top of this page) and space them apart by 100 ms (i.e., inserting silent intervals). This brings the package rate down to around 6 Hz. Can your brain keep up with that?\n496-0356 – repackaged What wizardry! What was unintelligible before is made (more…) intelligible by adding some ‘breathing space’ for the brain. Note that the speech signal itself did not change: it is the same acoustic content as before, but just presented at a slower pace so your brain can keep up!\nAnd now the exam! Here is a new telephone number, also consisting of 7 digits. Can you tell me what the last four digits are?\nClick here to see the correct answer… 0592\nAnd what about this one?\nClick here to see the correct answer… 0137\nPresumably everybody has a hard time correctly hearing these digits, because these are again recordings that have been compressed by a factor of 5!\nBut now try these:\nClick here to see the correct answer… 0723\nClick here to see the correct answer… 0164\nThat probably sounded much more intelligible. These are two examples of ‘repackaged speech’: first compressed by a factor of 5, chopped up into 66 ms snippets, and then spaced apart by 100 ms. And your brain was presumably very grateful for that extra breathing space (“my pleasure, brain…”).\nWhy is this important? This phenomenon can tell us something about what acoustic aspects support speech intelligibility. If we know what aspects of speech are critical for proper intelligibility, then that knowledge would be helpful, for instance, (i) for speech synthesizers, such as Automatic Announcement Systems in public transport, to generate speech signals that human listeners can understand well, (ii) for hearing aids to ’enrich’ incoming speech signals and present those optimized signals to the listening brain, or (iii) for communication with the elderly who often experience difficulty with speech perception, especially in noise.\nRelevant papers Hans Rutger Bosker, Oded Ghitza (2018). Entrained theta oscillations guide perception of subsequent speech: Behavioral evidence from rate normalization. Language, Cognition and Neuroscience,33(8), 955-967, doi:10.1080/23273798.2018.1439179. PDF Cite DOI ","date":1657238400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657238400,"objectID":"bc474ee88d85da53d3377d1b86d37eb5","permalink":"https://hrbosker.github.io/demos/repackaging/","publishdate":"2022-07-08T00:00:00Z","relpermalink":"/demos/repackaging/","section":"demos","summary":"Making unintelligible speech intelligible again...","tags":null,"title":"Repackaging speech","type":"demos"},{"authors":null,"categories":null,"content":" Here’s a brief intro into the Praat scripting language. We’ll cover how to write and run a script, point you to some tutorials, highlight some of the strange quirks in the Praat scripting language, and provide some scripts we find useful ourselves. Run your first script Open Praat, and go to Praat \u0026gt; New Praat script. You’ll find a rather empty-looking window popping up on your screen, with the menus File, Edit, Search, Convert, Font, Run at the top. Now type this into that scripting window: Create Sound from formula: \u0026#34;demo\u0026#34;, 1, 0, 1, 44100, \u0026#34;1/2 * sin(2*pi*377*x)\u0026#34; Play Now turn on your speakers… …and click Run \u0026gt; Run or hit [CTRL+R] to execute the script. Congratulations, you’ve run your first script that creates and plays a short 377 Hz tone. Every line is a click The Praat scripting language is a Graphical User Interface (GUI) scripting language. This means that, put rather bluntly, every line in the script is like a click in the Praat object window. The two lines in the code above are identical to:\nselecting New \u0026gt; Sound \u0026gt; Create Sound from formula…, and entering some parameters clicking Play This also means that you can click around in Praat and then ask Praat to give you the code for those particular clicks. That is, Praat actually keeps track of every click you perform.\nGo to the scripting window click Edit \u0026gt; Paste history This asks Praat to paste every action you performed since opening Praat. This is particularly handy when you don’t know how to script a certain action. For instance, you wanna know how to open a sound file in Praat using a script?\nGo to the Praat object window click Open \u0026gt; Read from file… and open a sound file Now go to the scripting window click Edit \u0026gt; Paste history It should show you something along the lines of:\nRead from file: \u0026#34;C:\\Users\\hanbos\\mysounds\\demo.wav\u0026#34; but then with a different directory and filename.\nSo remember: perform the functions in the object window, paste history in the scripting window, and edit the code from there.\nTutorials Praat offers a scripting tutorial itself. Go to Help \u0026gt; Praat Intro and scroll down to find Scripting. Alternatively, go to https://www.fon.hum.uva.nl/praat/manual/Scripting.html. This tutorial is not too bad actually. Other third-party tutorials are:\nhttps://www.eleanorchodroff.com/tutorial/PraatScripting.pdf: some quick intro slides by Eleanor Chodroff https://praatscripting.lingphon.net: a comprehensive written tutorial by Jörg Mayer https://praatscriptingtutorial.com/: a comprehensive written tutorial by Daniel Riggs http://www.mauriciofigueroa.cl/04_scripts/04_scripts.html: a comprehensive written tutorial by Mauricio A. Figueroa Candia Strange quirks Praat has some peculiarities that make the Praat scripting language stand out compared to other languages, like python and R.\nOver the years, Praat has had three types of syntax. Current Praat versions are compatible with older and newer syntax types, and mixes thereof. # This line of code extracts the first 100 ms of a sound object... # ... in Praat versions 5.3.43 and older; before April 2013 Extract part... 0 0.1 rectangular 1 no # ... in Praat versions between 5.3.44 and 5.3.62; April 2013 - January 2014 do(\u0026#34;Extract part\u0026#34;, 0, 0.1, \u0026#34;rectangular\u0026#34;, 1, \u0026#34;no\u0026#34;) # ... in Praat versions 5.3.63 and later; after January 2014 Extract part: 0, 0.1, \u0026#34;rectangular, 1, \u0026#34;no\u0026#34; Praat variables always start in lowercase. Capitals are reserved for functions: play = 2 for i to play Play endfor Praat does not distinguish between single and double equal signs: intensityLevel = 75 if intensityLevel = 75 newIntensityLevel = 80 endif Praat uses single quotes to access the value of a variable. This is, for instance, important when concatenating the values of different string variables: myDirectory$ = \u0026#34;C:\\Users\\hanbos\\mysounds\u0026#34; myFilename$ = \u0026#34;demo.wav\u0026#34; Read from file: \u0026#34;\u0026#39;myDirectory$\u0026#39;\\\u0026#39;myFilename$\u0026#39;\u0026#34; Praat’s spelling is elsif, not elseif (don’t ask me why…) if intensityLevel = 80 Scale intensity: 75 elsif intensityLevel = 75 Scale intensity: 80 else Scale intensity: 65 endif Different objects in Praat have different functions. For Sound objects, you can run functions like Play, Resample..., Scale intensity..., etc., while for TextGrid objects, you can run functions like Duplicate tier..., Insert boundary..., etc. Therefore, it is important to keep track of which object is selected: myDirectory$ = \u0026#34;C:\\Users\\hanbos\\mysounds\u0026#34; myFilename$ = \u0026#34;demo.wav\u0026#34; Read from file: \u0026#34;\u0026#39;myDirectory$\u0026#39;\\\u0026#39;myFilename$\u0026#39;\u0026#34; To TextGrid: \u0026#34;words\u0026#34;, \u0026#34;\u0026#34; Play … will throw an error because the newly created TextGrid is automatically selected after To TextGrid: and Praat cannot play TextGrids. The solution is:\nmyDirectory$ = \u0026#34;C:\\Users\\hanbos\\mysounds\u0026#34; myFilename$ = \u0026#34;demo.wav\u0026#34; Read from file: \u0026#34;\u0026#39;myDirectory$\u0026#39;\\\u0026#39;myFilename$\u0026#39;\u0026#34; To TextGrid: \u0026#34;words\u0026#34;, \u0026#34;\u0026#34; select Sound demo Play Ready-made scripts See our Scripts archive for snippets of code we frequently use. Note, however, that they require customization for each individual new project. Use at your own risk! …","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ac1e7b923b87c4d51e7361cec3fa632f","permalink":"https://hrbosker.github.io/resources/how-to/script-in-praat/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resources/how-to/script-in-praat/","section":"resources","summary":"Here’s a brief intro into the Praat scripting language. We’ll cover how to write and run a script, point you to some tutorials, highlight some of the strange quirks in the Praat scripting language, and provide some scripts we find useful ourselves.","tags":null,"title":"...script in Praat","type":"book"},{"authors":null,"categories":null,"content":"This script is an in-house template / starting point for batch processing multiple files. Adapt it to your own needs to apply a particular function to multiple files or multiple time intervals within each file.\nIn its current form, the script reads each .wav file plus accompanying TextGrid in a given input directory, extracts all non-empty intervals individually, and then loops over those to find the ones labelled “vowel”. It then allows the user to apply a particular function to those intervals (such as Scale intensity: 65), after which it concatenates the individual intervals back together, and saves the output in an output directory.\nNOTE: In its current form, the script does not run any function on its input. It really only serves as a starting point, including snippets of code we regularly use and now do not need to look up every time we want to do batch processing in Praat.\nYou can also download the script as a .praat file.\n################################################################################ ### Hans Rutger Bosker, Radboud University ### HansRutger.Bosker@ru.nl ### Date: 6 July 2022, run in Praat 6.2.12 on Windows 10 ### License: CC BY-NC 4.0 ################################################################################ ###\u0026gt;\u0026gt; This script is a starting point for batch processing a set of files. ###\u0026gt;\u0026gt;\tThe script basically reads files in an input directory and runs a ###\u0026gt;\u0026gt;\ta to-be-defined function [see \u0026#39;Perform your function here\u0026#39; below] ###\u0026gt;\u0026gt;\tand writes the output to an output directory . This saves me having ###\u0026gt;\u0026gt;\tto look up how to create a file list, how to loop over files, etc. ###\u0026gt;\u0026gt;\t###\u0026gt;\u0026gt; Since this was basically intended for in-house use, I\u0026#39;ve added in bits ###\u0026gt;\u0026gt;\tand pieces that I find useful to have ready-to-go, such as: ###\u0026gt;\u0026gt;\t\u0026#39;beginPause\u0026#39; for manually specifying variables. ################################################################################ ### Variables you will definitely need to customize: ################################################################################ ### Where can the files be found? dir_in$ = \u0026#34;C:\\Users\\hanbos\\mysounds\u0026#34; ### Where should the output files be saved? dir_out$ = \u0026#34;C:\\Users\\hanbos\\mysounds\\output\u0026#34; ################################################################################ ### Let\u0026#39;s check whether the directories specified above exist... ################################################################################ ### Let\u0026#39;s check if the input directory exists. ### This script will throw an error if the directory doesn\u0026#39;t exist ### (i.e., it won\u0026#39;t write to a mysterious temp directory). ### First check whether the input directory ends in a backslash (if so, removed) if right$(dir_in$,1)=\u0026#34;/\u0026#34; dir_in$ = left$(dir_in$,length(dir_in$)-1) elsif right$(dir_in$,1)=\u0026#34;\\\u0026#34; dir_in$ = left$(dir_in$,length(dir_in$)-1) endif ### Then create a temporary txt file in the folder ### and try to write it to the input folder. ### NOTE: The \u0026#34;nocheck\u0026#34; below asks Praat not to complain if the folder ### does *not* exist. We\u0026#39;ll manually check whether the saving of this ### temp txt file has succeeded or not further down below. temp_filename$ = dir_in$ + \u0026#34;/\u0026#34; + \u0026#34;my_temporary_Praat_file.txt\u0026#34; nocheck writeFileLine: temp_filename$, \u0026#34;This is just to check if the directory exists\u0026#34; ### Can the file be found? file_exists_yesno = fileReadable(temp_filename$) if file_exists_yesno = 1 # if you *could* read that temp txt file, # this confirms that the directory is valid. # Then you can delete it. deleteFile: temp_filename$ else # if that file wasn\u0026#39;t readable, that means that the directory wasn\u0026#39;t valid. printline The folder \u0026#39;dir_in$\u0026#39; was not found exit Your input directory doesn\u0026#39;t exist. Check spelling. The directory must *already* exist. endif ## Now re-do this for the output directory: if right$(dir_out$,1)=\u0026#34;/\u0026#34; dir_out$ = left$(dir_out$,length(dir_out$)-1) elsif right$(dir_out$,1)=\u0026#34;\\\u0026#34; dir_out$ = left$(dir_out$,length(dir_out$)-1) endif ### Then create a temporary txt file in the folder ### and try to write it to the input folder. ### NOTE: The \u0026#34;nocheck\u0026#34; below asks Praat not to complain if the folder ### does *not* exist. We\u0026#39;ll manually check whether the saving of this ### temp txt file has succeeded or not further down below. temp_filename$ = dir_out$ + \u0026#34;/\u0026#34; + \u0026#34;my_temporary_Praat_file.txt\u0026#34; nocheck writeFileLine: temp_filename$, \u0026#34;This is just to check if the directory exists\u0026#34; ### Can the file be found? file_exists_yesno = fileReadable(temp_filename$) if file_exists_yesno = 1 # if you *could* read that temp txt file, # this confirms that the directory is valid. # Then you can delete it. deleteFile: temp_filename$ else # if that file wasn\u0026#39;t readable, that means that the directory wasn\u0026#39;t valid. printline The folder \u0026#39;dir_out$\u0026#39; was not found exit Your output directory doesn\u0026#39;t exist. Check spelling. The directory must *already* exist. endif ########################################################################### ##\tFORM TO MANUALLY SPECIFY VARIABLES …","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3b365b10c943f6e7d7f2a5d9dd414149","permalink":"https://hrbosker.github.io/resources/scripts/batch-processing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resources/scripts/batch-processing/","section":"resources","summary":"This script is an in-house template / starting point for batch processing multiple files. Adapt it to your own needs to apply a particular function to multiple files or multiple time intervals within each file.","tags":null,"title":"Batch processing","type":"book"},{"authors":null,"categories":null,"content":"A very dull cocktail party In everday life, we often encounter situations where there’s more than just one talker speaking, like having a conversation in a busy bar, listening to a presenter at a crowded poster session, or talking to someone on the phone while walking on the street. Somehow our brain has little trouble honing in on that one talker we want to attend to, while ignoring others. How do we that?\nSpeech researchers like ourselves tend to investigate this research question by creating terribly dull ‘cocktail parties’. These cocktail parties include one listener, two talkers (A and B), and a lamentable lack of any cocktails. Still, such a situation does allow the researcher to play with particular auditory and visual cues to see if that helps or hinders the listener to attend to Talker A and ignore Talker B.\nWhy call this a ‘cocktail party’ in the first place? Colin Cherry came up with ’the cocktail party problem’ in 1953: “how do we recognize what one person is saying when others are speaking at the same time”. His experiments involved playing speech from two different talkers over the same speaker, covering such topics as “the really complex nature of the causes and uses of birds’ colors” and about how “religious convictions, legal systems, and politics have been so successful in accomplishing their aims”. How prof. Cherry arrived at the term ‘cocktail party’ in light of these topics remains an outright mystery. Scientists and parties…\nThe time between your ears One such cue is the interaural time difference (ITD). This is the difference in arrival time of a sound between two ears. Imagine Talker A is talking on your left, while Talker B is talking on your right. Their speech needs to travel through the air before reaching your ears, which takes (a very short amount of) time. Their opposite locations in space mean that the speech of Talker A will hit your left ear a fraction of a second earlier than your right ear. Likewise, the speech of Talker B will hit your right ear earlier than your left ear. Remarkably, your brain can use this difference in arrival time (ITD) to locate speakers in space, helping you to separate the speech from Talker A from the speech from Talker B.\nWe can try to construct a situation where ITD is the only cue to speakers’ locations in space, creating a virtual auditory scene:\nPut on your headphones/ear buds Do not use speakers; this works better with headphones Let’s take two recordings: one from a female Google, another from a male Alexa FEMALE GOOGLE MALE ALEXA Now let’s simply mix these two recordings: GOOGLE/ALEXA MIX When listening to this mixture, we can already notice three things:\nThey’re lying. In defiance of their words, the positioning of the two talkers in (virtual) space is acutally identical. Most listeners would judge the two talkers as being positioned ‘right in front of them’, or even ’talking inside their heads’. That is because both voices reach both of your ears instantaneously. The audio clip is a mono file with only one channel, containing the mixed speech from both talkers. The browser sends this single channel to both sides of your headphones (if all went well…), so the female speech reaches your left ear at the same point in time as it reaches your right ear (ITD = 0 ms), and the same for the male speech. In reality, this hardly ever happens (if at all), unless a speaker is standing perfectly in front of you.\nOur brains are remarkable. Despite the unrealistic virtual positions of the two talkers, we can still freely direct our attention to the female talker (and ignore the male talker), or to the male talker (and ignore the female talker). Apparently, we can use other cues (i.e., other than ITDs) to attend one talker and ignore others, such as their (markedly different) pitch.\nThis truly is a dull party…\nVirtual reality Now let’s see if we can move these two talkers around in virtual space. Imagine the female talker is on your left and the male is on your right. This would mean that the female speech reaches your left ear before it reaches your right ear. And vice versa: the male speech would reach your right ear before reaching your left.\nWe can mimick this by applying ITDs to the individual speech signals. First, we take the mono clip above and copy it to another channel, resulting in a stereo clip with two identical channels (see illustration in the clip below). Second, we take the female speech (given in red below) and delay it in the right channel by 0.0006 seconds = 0.6 milliseconds = 600 μs. Finally, we take the male speech (in blue) and delay it in the left channel by the same minute amount of time. Now we have a stereo clip with opposite ITDs for the two talkers!\nIn the clip below, you’ll first hear the ‘old’ mixture we had initially, followed by the ’new’ clip with manipulated ITDs. Can you hear the difference?\nBut I unexpectedly hear the talkers in the wrong locations! If you do hear the difference between the two clips in the video, …","date":1657152000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657152000,"objectID":"e70ded1bbf3c419375fe80859deb4bec","permalink":"https://hrbosker.github.io/demos/cocktail-party/","publishdate":"2022-07-07T00:00:00Z","relpermalink":"/demos/cocktail-party/","section":"demos","summary":"Trying to attend one talker, while ignoring others...","tags":null,"title":"Cocktail party listening","type":"demos"},{"authors":null,"categories":null,"content":" This R script runs a simulation-based power analysis for a simple 2AFC experimental design. This is by no means a one-size-fits-all solution to all your power needs. Use at your own risk! The R script shared here is first and foremost intended as a lab template, providing code that we adjust each time we run a new 2AFC (two alternative forced choice) experiment. This means it requires customization for each individual new project.\nIt is based on Kumle et al. (2021, BRM), adapting code from its github repo (Scenario #3) to fit our needs. It runs a simulation-based GLMM power analysis with 1000 iterations for a 2AFC design with {10, 20, 30, 40, 50} participants, each presented with 100 trials sampling from a single 5-step phonetic continuum. The estimates for fixed effects and random effects are drawn from pilot data, but can also be adopted from previous literature.\nThe script can be adjusted to run LMM power analysis (instead of GLMM) and to include \u0026gt;1 random effects; see comments in script.\nDownload the R_power_analysis.R script here.\n##### Hans Rutger Bosker, Radboud University Nijmegen ##### SPEAC research group, hrbosker.github.io ##### HansRutger.Bosker@ru.nl ##### License: CC BY-NC 4.0 ##### Last updated: July 20, 2022 ##### Code adapted from Kumle et al. (2021, Behavior Research Methods) ##### doi: 10.3758/s13428-021-01546-0 ##### who introduced the R package \u0026#39;mixedpower\u0026#39; comparing it to \u0026#39;simr\u0026#39;. ##### Specifically: https://lkumle.github.io/power_notebooks/Scenario3_notebook.html ##### and https://github.com/lkumle/analyses_power_tutorial/blob/master/Scenario%203/Analysis_Scenario3.R ##### In its present form, the script runs a simulation-based GLMM power analysis ##### for a 2AFC experimental design with 20 participants and 100 trials per pp. ##### BinomResp is the binomial dependent variable of 0s and 1s. ##### Predictors are Group (between-participant) and ScaledStep (within-participant). #####\t\u0026gt; For LMMs, see: https://lkumle.github.io/power_notebooks/Scenario3_notebook.html ##### \u0026gt; For adding additional random effects, see suggestions in script below. ######################################################################################## # REQUIRED PACKAGES ######################################################################################## library(lme4) library(simr) # for comparison to mixedpower ##### Apparently, \u0026#39;mixedpower\u0026#39; does not live on the \u0026#39;default\u0026#39; package R server. ##### In order to install it, you need another package \u0026#39;remotes\u0026#39; to access it. ##### Note: you only need to install these packages once. Once they\u0026#39;re installed, ##### simply running the library() statements suffices. #install.packages(\u0026#34;remotes\u0026#34;) #library(remotes) #remotes::install_github(\u0026#34;DejanDraschkow/mixedpower\u0026#34;) library(mixedpower) ######################################################################################## # RETRIEVING ESTIMATES FROM PILOT DATA ######################################################################################## pilot \u0026lt;- read.table(file=\u0026#34;pilotdata.txt\u0026#34;, sep=\u0026#34;\\t\u0026#34;, header=T) head(pilot) # ppid = participant number (hence, a unique entry for each pp) # trialnr = test item order (1:100) # step = step on the test continuum: 1,2,3,4,5 # stepscaled = scaled version of step (z-scored), resulting from: scale(pilot$step) # group = group, with 2 levels: Group_1 or Group_2 # group_devcod = deviance coded \u0026#39;group\u0026#39;: -0.5 is Group_1, +0.5 is Group_2 # BinomResp = binomial coding of participants\u0026#39; responses (0s and 1s) pilot_m \u0026lt;- glmer(BinomResp ~ group_devcod * stepscaled + (1|ppid), data=pilot, family=\u0026#34;binomial\u0026#34;) summary(pilot_m) ######################################################################################## # CREATING ARTIFICIAL DATA ######################################################################################## ##### This creates a dataset in which 20 participants are presented 100 trials each. ##### Half of the participants are assigned to Group 1, the other half to Group 2. ##### The 100 trials per participant include steps 1, 2, 3, 4, and 5 from a phonetic continuum. ##### So each step is repeated 20 times per participant. # 1: RANDOM EFFECTS # including variables used as random effects in artificial data number_of_participants = 20 number_of_trials_per_participant = 100 artificial_data \u0026lt;- expand.grid(TrialID = (1:number_of_trials_per_participant), ParticipantID = (1:number_of_participants)) # At present, the script is designed for an analysis with only 1 random intercept. # If requiring \u0026gt;1 random effects # ...adjust the \u0026#39;artificial_data\u0026#39;; # ...add an estimate for \u0026#39;estim_ran_effs\u0026#39;; # ...adjust the \u0026#39;artificial_glmer\u0026#39;. # 2. FIXED EFFECTS # generate continuum steps continuum_steps \u0026lt;- c(1:5) # repeat for every ParticipantID in data (e.g., 20 times) artificial_data[\u0026#34;Continuum_step\u0026#34;] \u0026lt;- rep(continuum_steps, number_of_participants) artificial_data$ScaledStep \u0026lt;- scale(artificial_data$Continuum_step) # include group artificial_data[\u0026#34;Group\u0026#34;] \u0026lt;- 0.5 …","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c0bf5a4f96f08fd81aeb59e8012275fe","permalink":"https://hrbosker.github.io/resources/how-to/run-a-power-analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resources/how-to/run-a-power-analysis/","section":"resources","summary":"This R script runs a simulation-based power analysis for a simple 2AFC experimental design. This is by no means a one-size-fits-all solution to all your power needs. Use at your own risk!","tags":null,"title":"...run a power analysis","type":"book"},{"authors":null,"categories":null,"content":"This script automatically moves all boundaries in a given tier in a TextGrid file to zero-crossings, which is important for extracting sound intervals. Specifically, it adds a tier ’to0x’ at the top of the TextGrid that is identical to a given input tier, except that all boundaries are at zero-crossings.\nYou can also download the script as a .praat file.\n################################################################################ ### Hans Rutger Bosker, Radboud University ### HansRutger.Bosker@ru.nl ### Date: 22 July 2022, run in Praat 6.2.12 on Windows 10 ### License: CC BY-NC 4.0 ################################################################################ ###\u0026gt;\u0026gt; This script takes a single .wav and matching .TextGrid file as input ###\u0026gt;\u0026gt; and moves all boundaries in one particular tier to zero-crossings. ###\u0026gt;\u0026gt; Specifically, it *adds* a new \u0026#39;to0x\u0026#39; tier at the top of the TextGrid ###\u0026gt;\u0026gt; that is identical to a given tier except that all boundaries are ###\u0026gt;\u0026gt; at zero-crossings. The old TextGrid is then overwritten with the ###\u0026gt;\u0026gt; new extended TextGrid, but no information is lost (only added). ###\u0026gt;\u0026gt; This script works best if the script also subtracts the mean from ###\u0026gt;\u0026gt; the audio signal, and overwrites the original audio: ###\u0026gt;\u0026gt;\t\u0026#39;method\u0026#39; option 1 (default). ###\u0026gt;\u0026gt;\tIf you need to move the boundaries for more than one TextGrid file, ###\u0026gt;\u0026gt;\tyou can merge this script with \u0026#39;batch-processing.praat\u0026#39;, ###\u0026gt;\u0026gt;\tsee: https://hrbosker.github.io/resources/scripts/batch-processing/ ################################################################################ ### Form to enter variables ################################################################################ beginPause: \u0026#34;Enter settings\u0026#34; comment: \u0026#34;Provide the directory of the sound and TextGrid file (no slash at end of string)\u0026#34; text: \u0026#34;directory\u0026#34;, \u0026#34;C:/Users/hanbos/mysounds\u0026#34; comment: \u0026#34;Provide the name of the sound file\u0026#34; comment: \u0026#34;(should be identical to TextGrid name)\u0026#34; comment: \u0026#34;NOTE: do NOT include any extension (no .wav)\u0026#34; text: \u0026#34;filename\u0026#34;, \u0026#34;syll1\u0026#34; comment: \u0026#34;Provide the interval tier number (typically 1)\u0026#34; real: \u0026#34;tierNumber\u0026#34;, 1 choice: \u0026#34;method\u0026#34;, 1 option: \u0026#34;Subtract mean and overwrite audio file\u0026#34; option: \u0026#34;Do not subtract mean\u0026#34; clicked = endPause (\u0026#34;Cancel\u0026#34;, \u0026#34;OK\u0026#34;, 2) ################################################################################ ### Before we start, let\u0026#39;s check whether you\u0026#39;ve entered sensible ### input for the variables above... ################################################################################ ### Let\u0026#39;s check if the directory exists. ### This script will throw an error if the directory doesn\u0026#39;t exist ### (i.e., it won\u0026#39;t write to a mysterious temp directory). ### First check whether the input directory ends in a backslash (if so, removed) if right$(directory$,1)=\u0026#34;/\u0026#34; directory$ = left$(directory$,length(directory$)-1) elsif right$(directory$,1)=\u0026#34;\\\u0026#34; directory$ = left$(directory$,length(directory$)-1) endif ### Then create a temporary txt file in the folder ### and try to write it to the input folder. ### NOTE: The \u0026#34;nocheck\u0026#34; below asks Praat not to complain if the folder ### does *not* exist. We\u0026#39;ll manually check whether the saving of this ### temp txt file has succeeded or not further down below. temp_filename$ = directory$ + \u0026#34;/\u0026#34; + \u0026#34;my_temporary_Praat_file.txt\u0026#34; nocheck writeFileLine: temp_filename$, \u0026#34;This is just to check if the directory exists\u0026#34; ### Can the file be found? file_exists_yesno = fileReadable(temp_filename$) if file_exists_yesno = 1 # if you *could* read that temp txt file, # this confirms that the directory is valid. # Then you can delete it. deleteFile: temp_filename$ ## Let\u0026#39;s also check whether the specified wav and TextGrid filenames exist ## inside this particular directory. filepath$ = directory$ + \u0026#34;/\u0026#34; + filename$ + \u0026#34;.wav\u0026#34; wavFileExists = fileReadable(filepath$) if wavFileExists = 0 # if the wav file does not exist printline Could not find \u0026#39;filename$\u0026#39;.wav in the folder \u0026#39;directory$\u0026#39; exit Could not find \u0026#39;filename$\u0026#39;.wav in the folder \u0026#39;directory$\u0026#39;. Check spelling. endif filepath$ = directory$ + \u0026#34;/\u0026#34; + filename$ + \u0026#34;.TextGrid\u0026#34; tgFileExists = fileReadable(filepath$) if tgFileExists = 0 # if the TextGrid file does not exist printline Could not find \u0026#39;filename$\u0026#39;.TextGrid in the folder \u0026#39;directory$\u0026#39; exit Could not find \u0026#39;filename$\u0026#39;.TextGrid in the folder \u0026#39;directory$\u0026#39;. Check spelling. endif else # if the temporary file wasn\u0026#39;t readable, that means that the directory wasn\u0026#39;t valid. printline The folder \u0026#39;directory$\u0026#39; was not found exit Your directory doesn\u0026#39;t exist. Check spelling. The directory must *already* exist. endif ################################################################################ ################################################################################ ################################# SCRIPT ################################# ################################################################################ ################################################################################ Read from file... …","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"dbd7452a041ee79842acb4e0eaf90519","permalink":"https://hrbosker.github.io/resources/scripts/move-to-zero-crossings/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resources/scripts/move-to-zero-crossings/","section":"resources","summary":"This script automatically moves all boundaries in a given tier in a TextGrid file to zero-crossings, which is important for extracting sound intervals. Specifically, it adds a tier ’to0x’ at the top of the TextGrid that is identical to a given input tier, except that all boundaries are at zero-crossings.","tags":null,"title":"Move to zero-crossings","type":"book"},{"authors":null,"categories":null,"content":"Listening test In the video below, you’ll see a simple display with four objects. First, see if you know each of the four objects. Then play the video. You’ll hear a female voice asking you to press a button for one of the objects (i.e., click on it). While watching and listening, try to keep track of where your eyes go in the display…\nDo you have any idea what the next word of the speaker will be? Probably not, right? Did you notice anything particular about where in the display your gaze was at? Since you probably didn’t know what object the speaker was going to name, chances are your eyes were all over the place.\nOK, next video. It’s the same display, but with a new audio recording. Have a look and see if you can tell which of the four objects the speaker selects…\nWell, in this case, you may already have a slight hunch, right? The speaker was hesitating at the end of her utterance, wasn’t she? Well, chances are this native speaker of British English won’t have much trouble naming common objects, like lion, ear, or bike, would she? So could it be that she’ll refer to the Italian moka pot in the top left?\nDisfluencies help you predict what’s coming up Natural speech is messy. We stumble over words, lose our line of thought, and produce tons of uhm’s and uh’s. Still, these kinds of disfluencies don’t occur randomly throughout an utterance. We are much more likely to stumble before rarely occurring (low-frequency), novel (not mentioned before), and complex (long) words than we are before common and simple words.\nInterestingly, human listeners seem to be aware of this. In our experiments, we presented listeners with displays like the ones above together with spoken instructions to click on one of the objects. While people were watching/listening, we recorded where they were looking on the screen using eye-tracking (see lab photo below). This allowed us to track their gaze on a millisecond time scale as the utterance unfolds. Results showed that when people heard the speaker hesitate, they were much more likely to look at a low-frequency object, like moka pot, compared to high-frequency objects (Bosker et al., 2014).\nEye-tracking lab at the MPI. (C) Max-Planck-Gesellschaft, https://www.mpi.nl/page/mpi-labs OK, let’s try again… Here’s another video, again with the same display, but another audio recording. Once again, have a listen and see if you can tell which object the speaker will name:\nIn the last few milliseconds of the clip, you may have discovered a glimpse of the object. Did she say “Now press the button for the li-…”? Does that mean we’ve finally figured out that it’ll be the lion after all?\nLet’s find out:\nAargh!\nFiller words can be misleading As mentioned before, speech is messy. We don’t only produce hesitations and disfluencies, but also litter our speech with seemingly meaningless filler words, such as ‘you know’, ‘well’, and (worst of all) ’like’. Our audience, in turn, is tasked with distilling from this chaos what we actually want to communicate.\nAnd that can be hard. Filler words share their sounds (phonology) with many other words. The filler ’like’ shares its initial sounds with words such as ’lion’, ’lime’, ’lice’, lightbulb’, etc. Our experiments have shown that listeners are actually considering these similar-sounding words (cohort competitor) when encountering ’like’. When presented with displays with one ‘cohort competitor’ (e.g., lion) and three distractors, participants were biased towards looking at the lion upon hearing “…for the like…”. This suggests that filler words, like “like” (see what I did there?), have an impact on the efficiency of word recognition (Bosker et al., 2021).\nWhy is this important? Eye-tracking can reveal the time-course of speech processing. It allows tracking people’s gaze with millisecond precision, often without participants themselves being aware of their own looking behavior. As such, it can show when in time certain acoustic and/or visual cues influence speech perception. That kind of temporal information has for instance been used to discriminate between different models of word recognition.\nRelevant papers Merel Maslowski, Antje S. Meyer, Hans Rutger Bosker (2020). Eye-tracking the time course of distal and global speech rate effects. Journal of Experimental Psychology: Human Perception and Performance, 46(10), 1148-1163, doi:10.1037/xhp0000838. PDF Cite Dataset DOI Hans Rutger Bosker, Esperanza Badaya, Martin Corley (2021). Discourse markers activate their, like, cohort competitors. Discourse Processes, 58(9), 837-851, doi:10.1080/0163853X.2021.1924000. PDF Cite Dataset DOI Hans Rutger Bosker, Marjolein van Os,, Rik Does, Geertje van Bergen (2019). Counting ‘uhm’s: how tracking the distribution of native and non-native disfluencies influences online language comprehension. Journal of Memory and Language, 106, 189-202, doi:10.1016/j.jml.2019.02.006. PDF Cite Dataset DOI Hans Rutger Bosker, Geertje van Bergen (2018). Linguistic expectation management …","date":1657065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657065600,"objectID":"e4e9399826b7013e174c41cdead91ce2","permalink":"https://hrbosker.github.io/demos/visual-world-paradigm/","publishdate":"2022-07-06T00:00:00Z","relpermalink":"/demos/visual-world-paradigm/","section":"demos","summary":"How your eyes betray what you think you hear...","tags":null,"title":"Look and listen","type":"demos"},{"authors":null,"categories":null,"content":" Here’s a Word template that includes all basic sections of a paper, template statements (e.g., ethics, participants specs, etc.), ‘fields’ to automatically update figure/table numbers, heading styles, and Zotero for reference management. Word template Download the Word template here.\nTips n tricks Start now. In the olden days, I wrote the paper after all experiments had been run, all data had been collected and analyzed, and I had made my mind up about its theoretical implications. Don’t go there! Start writing the moment you think of your research question. Take the template above and just start filling in some of the gaps with text. Add a Methods section when you’ve designed the experiment, start adding some sentences to the Introduction when waiting for data to come in, add a reference immediately the moment you encounter a paper you will definitely want to cite, you can even start writing the Results and General Discussion based on your expectations, and then once the data are in, all you need to do is just fill in the numbers. It takes some commitment and diligence, but it will pay off in the end!\nCut an onion. A paper is like an onion: start BIG AND GRAND at the beginning, describing a major problem in human behavior, and then gradually get smaller and smaller and more detailed as you reach the end of the Introduction, describing the actual experiment, then go really nitty-gritty when you reach the Methods and Results. When you reach the General Discussion, you start zooming out again, first summarizing the experimental outcomes, then discussing their theoretical implications, and finally drawing BIG AND GRAND conclusions about life, the universe, and everything.\nBuild a skeleton. First try to create an outline of your Introduction, using one sentence to summarize each paragraph. Start with “Speech perception is amazing” for the first paragraph, and “This experiment tested…” for the final paragraph of the Introduction, and try to find your way from the first claim to the last one. For instance:\n(1) Speech perception is amazing (2) People use both visual and auditory cues in speech perception (3) These visual cues include both facial articulatory cues as well as hand gestures (4) However, little is known about how visual cues to prosody influence speech perception (5) Therefore, this experiment tested… Once you have this skeleton, all you then need to do is just ‘fill the paragraphs with words’. The skeleton also helps to find out which citation goes where (as in: when should I cite this seminal study I found?). Finally, according to the Onion Theory, the General Discussion is then simply the skeleton for the Introduction turned upside down:\n(5) This experiment tested ABC and found XYZ (4) This suggests that people use visual gestural cues to prosody in speech perception (3) How these visual cues interact with other visual cues, such as articulation on the face, remains an issue for future research (2) Our outcomes emphasize the importance of both visual and auditory cues in face-to-face spoken communication. (1) Isn’t speech perception amazing? Cheat. Use tools such as Thesaurus for synonym searching, Zotero for reference management, fields in Word for figure and table numbers, etc.\nNow just start typing. It’s easier to revise and edit something that you wrote yesterday than to fill an empty page. So why not jot down a few sketchy sentences and rework them later into something nice?\nDid you know that… in the olden days, APA guidelines asked you to place figures only at the end of the manuscript, presumably for copyediting reasons. Their positions in the main text were identified by short “Insert Figure 1 about here” boxes. However, this is a real pain, for reviewers in particular (and everyone else too, really…), because this means you need to flip back and forth between main text and figures to understand the results. It’s much easier and more effective if you place figures in text. Even if journal guidelines still ask you to place figures at the end of the manuscript, I usually just put them in text and wait for an editorial assistant to call me out… Happy writing!\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"570dc2511c14e91bb48d506b8cdd76bc","permalink":"https://hrbosker.github.io/resources/how-to/write-a-manuscript/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resources/how-to/write-a-manuscript/","section":"resources","summary":"Here’s a Word template that includes all basic sections of a paper, template statements (e.g., ethics, participants specs, etc.), ‘fields’ to automatically update figure/table numbers, heading styles, and Zotero for reference management.","tags":null,"title":"...write a paper","type":"book"},{"authors":null,"categories":null,"content":"This script creates an F0 continuum for two segmentally matching words (e.g., for a lexical stress pair, like SUBject vs. subJECT). First, it matches the two words in duration and then interpolates the F0 contour linearly in 11 steps (no. 1 and 11 being the original contours), controlling for intensity.\nYou can also download the script as a .praat file.\n#____________________________________ HEADER __________________________________#### # date: 31.01.2023, run in Praat 6.2.12 on Windows 11 # author: Ronny Bujok (adapted from Hans Rutger Bosker) # email: Ronny.Bujok@mpi.nl # filename: F0_stress continuum_interpolation.praat # project: Audiovisual Perception of Lexical Stress # license:\tCC BY-NC 4.0 #################################################################################################################### # This script takes recordings of segmentally identical, disyllabic minimal stress pairs with lexical stress on the first # (strong-weak; SW) or second (weak-strong; WS) syllable (e.g., VOORnaam vs. voorNAAM) and interpolates their F0-contours linearly. # The manipulated F0-contours are applied to the SW recording to create an F0-based lexical stress continuum. # Duration and Intensity are set to mean, ambiguous values. # # This script requires minimal pairs to match in their name. Stress pattern in input files is denoted with \u0026#34;_sw\u0026#34; or \u0026#34;_ws\u0026#34; # (e.g., voornaam_sw.wav \u0026amp; voornaam_ws.wav). No other underscore characters are allowed. # Textgrids for each recording are required, with boundaries at word onset, second syllable onset and word offset. # # Note that the parameters for pitch estimation are set to match a male voice. When working with a female voice, # please adjust the arguments of the functions \u0026#34;Lengthen (overlap-add)\u0026#34; and \u0026#34;To Manipulation\u0026#34;. #################################################################################################################### # Define input and output directory input_directory$ = \u0026#34;C:\\input folder\u0026#34; output_directory$= \u0026#34;C:\\output folder\u0026#34; # create file list of all files with the ending \u0026#34;_sw.wav\u0026#34; (just sound files, one per minimal pair (sw)) Create Strings as file list... list \u0026#39;input_directory$\u0026#39;\\*_sw.wav n = Get number of strings # _________________________________ CREATE FILELIST ____________________________________________________ # create list again because it removes itself after every iteration for x from 1 to n Create Strings as file list... list \u0026#39;input_directory$\u0026#39;\\*_sw.wav # get only the word name without extension or stress identifier (sw or ws) select Strings list current_file$ = Get string... x idx = rindex (current_file$, \u0026#34;_\u0026#34;) word$ = left$(current_file$,idx-1) #_____________________________________ DURATION ___________________________________________________________ # To interpolate F0 contours, recordings must be of equal length. So duration must be manipulated first. #---------------------------------- get values from SW word --------------------------------------------- # open SW word and convert to mono wavfile_sw$ = \u0026#34;\u0026#39;word$\u0026#39;_sw\u0026#34; Read from file... \u0026#39;input_directory$\u0026#39;\\\u0026#39;wavfile_sw$\u0026#39;.Textgrid Read from file... \u0026#39;input_directory$\u0026#39;\\\u0026#39;wavfile_sw$\u0026#39;.wav Convert to mono Rename: \u0026#34;mono_sw\u0026#34; wav_sw_id = selected() # select the mono file and the corresponding textgrid and extract all intervals selectObject: \u0026#34;TextGrid \u0026#39;wavfile_sw$\u0026#39;\u0026#34; plusObject: \u0026#34;Sound mono_sw\u0026#34; Extract all intervals: 1, \u0026#34;no\u0026#34; # select the second interval (first syllable) and get the total duration and the intensity\tselect (wav_sw_id+2) sw_dur1= Get total duration sw_int1= Get intensity (dB) Rename: \u0026#34;sw_1\u0026#34; # select the third interval (second syllable) and get the total duration and the intensity\tselect (wav_sw_id+3) sw_dur2= Get total duration sw_int2= Get intensity (dB) Rename: \u0026#34;sw_2\u0026#34; # select and rename the first and last interval (silences) for future reference, to concatenate the final audio select (wav_sw_id+1) Rename: \u0026#34;pre_silence_sw\u0026#34; select (wav_sw_id+4) Rename: \u0026#34;post_silence_sw\u0026#34; #---------------------------------- get values from WS word ----------------------------------------- # open WS word and convert to mono\twavfile_ws$ = \u0026#34;\u0026#39;word$\u0026#39;_ws\u0026#34; Read from file... \u0026#39;input_directory$\u0026#39;\\\u0026#39;wavfile_ws$\u0026#39;.Textgrid Read from file... \u0026#39;input_directory$\u0026#39;\\\u0026#39;wavfile_ws$\u0026#39;.wav Convert to mono Rename: \u0026#34;mono_ws\u0026#34; wav_ws_id = selected() # select the mono file and the corresponding textgrid and extract all intervals\tselectObject: \u0026#34;TextGrid \u0026#39;wavfile_ws$\u0026#39;\u0026#34; plusObject: \u0026#34;Sound mono_ws\u0026#34; Extract all intervals: 1, \u0026#34;no\u0026#34; # select the second interval (first syllable) and get the total duration and the intensity\tselect (wav_ws_id+2) ws_dur1= Get total duration ws_int1= Get intensity (dB) Rename: \u0026#34;ws_1\u0026#34;\t# select the third interval (second syllable) and get the total duration and the intensity\tselect (wav_ws_id+3) ws_dur2= Get total duration ws_int2= Get intensity (dB) Rename: \u0026#34;ws_2\u0026#34;\t#---------------------------------- Manipulate duration ------------------------------------------ # Set the duration of syllable 1 of the SW word to the …","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"de89edadd0773becf17145c9a48b8264","permalink":"https://hrbosker.github.io/resources/scripts/interpolate-f0/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resources/scripts/interpolate-f0/","section":"resources","summary":"This script creates an F0 continuum for two segmentally matching words (e.g., for a lexical stress pair, like SUBject vs. subJECT). First, it matches the two words in duration and then interpolates the F0 contour linearly in 11 steps (no.","tags":null,"title":"Interpolate F0 continuum","type":"book"},{"authors":null,"categories":null,"content":"Same audio, different perception In May 2018, social media exploded after the surfacing of an audio clip that some perceived as Laurel, but others as Yanny. Listen and decide for yourself:\nLaurel/Yanny – [original] #Laurelgate was quickly seen as the auditory version of #TheDress, a photo going viral in 2015 of a white and gold dress, or was it black and blue? But how fixed is this divide between individuals? Can we turn #Yannists into #Laurelites, and vice versa?\nHigher vs. lower frequencies Acoustic analysis of the original clip suggests that the higher frequencies (\u0026gt;1000 Hz) resembled the word Yanny, but the lower frequencies (\u0026lt;1000 Hz) are more like Laurel. This can be seen in the figure at the top of this page, where the upper part of the middle panel (Original) is more like the right panel (Yanny), but the lower part is more like the left panel (Laurel). This is best demonstrated by artificially emphasizing/attenuating the higher vs. lower frequencies in the audio clip.\nIn these sounds below, we gradually attenuate (~turn down) the higher frequencies while we simultaneously emphasize (~turn up) the lower frequencies. Play the sounds below, can you hear Laurel turning into Yanny?\nClick here for audio specs Middle clip: original Laurel/Yanny clip Manipulation: filtered by 10 bandpass filters (with center frequencies: 31.5, 63, 125, 250, 500, 1000, 2000, 4000, 8000, 16000 Hz; using a Hann window with a roll-off width of 20, 20, 40, 80, 100, 100, 100, 100, 100, 100 Hz, respectively). Inverse intensity manipulation for high (\u0026gt;1000 Hz) vs. low (\u0026lt;1000 Hz) frequency bands in steps of 6 dB. Top clip: -18 dB attenuation for higher frequency bands, +18 dB emphasis for lower frequency bands. Bottom clip: +18 dB emphasis for higher frequency bands, -18 dB attenuation for lower frequency bands. OK, so we can guide what people hear by artificially editing the higher vs. lower frequencies in the clip. But can we also make someone hear one and the same clip differently?\nLet’s add Laurel’s telephone number The perception of speech sounds is influenced by the surrounding acoustic context. The same sound can be perceived differently when, for instance, the acoustics of a preceding sentence are changed. Below, you will hear the original Laurel/Yanny clip, but this time preceded by a telephone number: 496-0356. In the first clip, we filtered out (~removed) the lower frequencies in the telephone number leaving only the high frequency content. In the second clip, we filtered out the higher frequencies leaving only the low frequency content. Note: the Laurel/Yanny clip itself is identical in the two audios. Do you hear a different name after each telephone number?\nHigh-pass filtered telephone number + Laurel/Yanny Low-pass filtered telephone number + Laurel/Yanny Numbing your ears In a crowd-sourced experiment with \u0026gt;500 online participants, we found that the same people were more likely to report hearing Laurel for the first clip, but Yanny for the second clip. This is because the high-frequency content in the telephone number in the first clip ’numbs your ears’ for any following high-frequency content, thus making the lower frequencies stand out more, biasing perception towards Laurel. And vice versa, the low-frequency content in the telephone number in the second clip ’numbs your ears’ for any following low-frequency content, thus making the higher frequencies stand out more, biasing perception towards Yanny.\nReally? Convince me… This is Figure 1 from Bosker (2018, JASA) showing people’s responses in panel C. The blue line shows the proportion of Yanny responses after a high-pass filtered telephone number (~first clip above), which is higher than the red line illustrating people’s responses for the same Laurel/Yanny clips after a low-pass filtered telephone number (~second clip above).\nWhy is this important? These social media phenomena are great examples of how our perception of the world is strongly context-dependent. What we perceive is not wholly determined by the input signal alone, but also by the context in which the signal is perceived, including the sounds heard previously, our prior expectations, who is talking, etc. etc. As such, they highlight the subtle intricacies of human perception.\nRelevant papers Hans Rutger Bosker (2018). Putting Laurel and Yanny in context. The Journal of the Acoustic Society of America, 144(6), EL503-EL508, doi:10.1121/1.5070144. PDF Cite Dataset DOI ","date":1656979200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656979200,"objectID":"e501aee23aa3d94c5466896220f32ba6","permalink":"https://hrbosker.github.io/demos/laurel-yanny/","publishdate":"2022-07-05T00:00:00Z","relpermalink":"/demos/laurel-yanny/","section":"demos","summary":"Making you hear *Yanny* when previously you heard *Laurel*...","tags":null,"title":"Laurel or Yanny?","type":"demos"},{"authors":null,"categories":null,"content":" This describes which journal to choose, which suggested (or dispreferred?) reviewers to mention, an example cover letter, how to read the submission system, when to contact the editorial office about your submission, and how to respond to reviewers. I currently serve as Associate Editor of the journal Language \u0026amp; Speech. However, the statements below do not reflect the official opinion of Language \u0026amp; Speech nor any other journal I ever edited or reviewed for. They merely describe my personal experiences with submitting, reviewing, and editing papers, so take them with a grain of salt… What journal do I pick? Nowadays, with Twitter, Psyarxiv, and Google Scholar, what journal a paper appears in has become a lot less important. Your peers will see it online no matter where it is published. Moreover, science as a whole is moving towards valuing the impact of an article on its own merits rather than weighing its value by the impact factor of the journal it appeared in. Still, in general, one can say that the broader the interest and audience of the journal, the more prestigious it is considered, and the harder it is to get a paper accepted (think Nature vs. Language and Speech, for instance). That said, the story is a little different for open access journals of broad interest. Even though these journals have papers from a wide range of disciplines, it is - at least in my experience - easier to get into these than their non-open broad-interest counterparts, hurting their prestige a bit.\nAs speech researchers, we are livin’ on the edge… of the fields of experimental psychology, neurobiology, and the speech sciences. Consequently, we need to consider journals from differents fields. Is your experimental study more appealing and relevant to a psychology crowd, or to hardcore phoneticians, or nutty neuroscientists? Also remember there is no single perfect journal for your study; in fact, in all likelihood you may have to go through several before seeing it in print. Here are some things to consider…\nConsider a journal’s prestige because that will influence your chances of getting in.\nConsider the journal’s values, for instance in terms of open access and open data.\nConsider the implications of your study: some papers simply fit better in a specialist journal than in one of broad interest.\nConsider your CV: it’s probably better if your CV lists publications in different journals from different fields compared to only publishing in a single specialist journal.\nConsider journal-specific criteria and guidelines, such as what article types do they have (Regular Article vs. Brief Reports), what word limits do they have, etc.\nFinally, before submitting check whether your institution has an agreement with the target journal about open access fees. Many academic institutions will have signed contracts with publishers, waiving open access fees for their employees. However, often certain conditions apply, such as applying only to first-authors, or to authors with an institute email address. Make sure you know about these conditions before submitting as whatever details you submit will decide whether or not your paper falls under those institutional agreements.\nSubmitting your paper Most journals work with an online submission system, such as Manuscript Central. Go to the website of your journal of interest, search for ‘Submit your paper here’ or something similar, and create an account with the system.\nIt helps if you create your account carefully because many of your account’s details will carry over to your submissions. For instance, add your ORCID number to your account so your ORCID iD will automatically appear on your publications. Use your institute email address so your institute will pay the open access fees. Then create a new submission, which typically involves…\n…selecting an article type\nIs this a Regular Article (most experimental studies are) or a Review Paper (without new empirical data)? Or is it a Brief Report (check out the journal guidelines) or even a Registered Report (pre-registered study in an open repository, like OSF)?\n…copying some manuscript details\nCopy the title, abstract, keywords, etc. into the relevant boxes. Note that most submission systems are simple text only, so formatting in the title or abstract may get lost. Make sure you enter these details only when the manuscript is in its definitive form. You don’t want to have a different title in the system vs. in the manuscript itself. This is particularly relevant for keywords. Some journals allow you to select keywords yourself (see journal guidelines for how many and what character to use to separate different keywords) but sometimes they ask you to select from a dropdown menu. If the latter, you want to make sure you don’t have different keywords in the manuscript.\n…ticking a few boxes\nNo, we did not submit this paper anywhere else; yes, we adhered to all relevant ethical requirements and guidelines; no, we do not have any conflicts of interest. …","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c98460c72ae357807084468893a58e23","permalink":"https://hrbosker.github.io/resources/how-to/get-published/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resources/how-to/get-published/","section":"resources","summary":"This describes which journal to choose, which suggested (or dispreferred?) reviewers to mention, an example cover letter, how to read the submission system, when to contact the editorial office about your submission, and how to respond to reviewers.","tags":null,"title":"...get published","type":"book"},{"authors":null,"categories":null,"content":"Let’s do a little test… First, let’s take care of your audio settings:\nput on your headphones/ear buds/speakers turn your volume way down play this sound… SOME WHITE NOISE… …and adjust your volume until it’s at a loud but still comfortable level. OK, now we’ll do a short reading test:\nplay the video below you’ll see a counter counting down from 3… …and then it will present a simple sentence on screen your task is simply to read out the sentence aloud. Ready? Go!\nWhy thank you! OK, now let’s do this again. Make sure to keep wearing your headphones, play the next video, and read out the sentence aloud.\nSurprise!\nWhat’s going on? Perhaps you noticed your voice sounding somewhat different the second time, when there was loud babble from 16 other talkers playing in your ears, compared to the first time (in quiet). This phenomenon is called Lombard speech (or: Lombard effect; Lombard reflex). It’s the type of speech people produce when speaking in noise.\nBut perhaps you didn’t quite hear yourself all too well because it’s hard to listen to your own voice when there’s other sounds around. So here’s two clips from a male speaker of British English (and a rather posh one, if I may say so…) giving you some really useful dietary advice. The first is from when he was speaking in quiet: this is called ‘plain speech’. The second clip is a recording of the same sentence but this time the talker heard loud noise over headphones, speaking in noise: ‘Lombard speech’.\nPLAIN SPEECH LOMBARD SPEECH retrieved from the Acted clear speech corpus\nWhat is Lombard speech? In the clips above, you can clearly hear the difference between ‘plain speech’ and ‘Lombard speech’. And this speaker is not the odd-one-out: many vocal learning species, like dolphins, seals, and birds adjust their vocalizations when encountering noise.\nIn humans, Lombard speech sounds louder, higher pitched, is a little slower, with more pronounced higher frequencies, and clearer vowels. In our own research, we demonstrated that Lombard speech is also more rhythmic, having a stronger ‘beat’ to it compared to plain speech (see refs below).\nLombard speech rulez Speech perception studies have demonstrated that these ‘vocal adjustments’ people make when speaking in noise actually have a purpose: they make you more intelligible! When you take the plain and Lombard clips above, scale their intensities to be exactly the same, and then mix them with loud babble, this is what you get:\nPLAIN SPEECH in BABBLE (SNR = -6 dB) LOMBARD SPEECH in BABBLE (SNR = -6 dB) You may experience that it’s easier to pick out the male target talker from the babble in the second (Lombard) clip than in the first (plain) clip. Apparently, ‘speaking up’ actually helps!\nWhy is this important? Lombard speech is more intelligible in noise than plain speech. This means that speech researchers can ‘borrow’ acoustic aspects of Lombard speech to boost speech intelligibility, for instance in hearing aids. So next time you wanna make sure your message comes across in that busy bar, you’d better boost your F0, raise your spectral tilt, and increase your vowel dispersion; got it?!\nRelevant papers Hans Rutger Bosker, Martin Cooke (2018). Talkers produce more pronounced amplitude modulations when speaking in noise. The Journal of the Acoustical Society of America, 143(2), EL121-EL126, doi:10.1121/1.5024404. PDF Cite DOI Hans Rutger Bosker, Martin Cooke (2020). Enhanced amplitude modulations contribute to the Lombard intelligibility benefit: Evidence from the Nijmegen Corpus of Lombard Speech. The Journal of the Acoustical Society of America, 147(2), 721-730, doi:10.1121/10.0000646. PDF Cite Dataset DOI ","date":1656892800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656892800,"objectID":"7d509f542847ddd8835f927d980b7ab2","permalink":"https://hrbosker.github.io/demos/lombard-speech/","publishdate":"2022-07-04T00:00:00Z","relpermalink":"/demos/lombard-speech/","section":"demos","summary":"How speaking up changes your voice...","tags":null,"title":"Lombard speech","type":"demos"},{"authors":["Giulio Severijnen","Giuseppe Di Dona","Hans Rutger Bosker","James M. McQueen"],"categories":null,"content":" ","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"9c9e33165225a4c8911265ddb45e3780","permalink":"https://hrbosker.github.io/publication/severijnen-etal-2023-jephpp/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/severijnen-etal-2023-jephpp/","section":"publication","summary":"When recognizing spoken words, listeners are confronted by variability in the speech signal caused by talker differences. Previous research has focused on segmental talker variability; less is known about how suprasegmental variability is handled. Here we investigated the use of perceptual learning to deal with between-talker differences in lexical stress. Two groups of participants heard Dutch minimal stress pairs (e.g., VOORnaam vs. voorNAAM, ‘first name’ vs. ‘respectable’) spoken by two male talkers. Group 1 heard Talker 1 use only F0 to signal stress (intensity and duration values were ambiguous), while Talker 2 used only intensity (F0 and duration were ambiguous). Group 2 heard the reverse talker-cue mappings. After training, participants were tested on words from both talkers containing conflicting stress cues (‘mixed items’; e.g., one spoken by Talker 1 with F0 signaling initial stress and intensity signaling final stress). We found that listeners used previously learned information about which talker used which cue to interpret the mixed items. For example, the mixed item described above tended to be interpreted as having initial stress by Group 1 but as having final stress by Group 2. This demonstrates that listeners learn how individual talkers signal stress and use that knowledge in spoken-word recognition.","tags":["lexical stress","perceptual learning","talker variability","suprasegmental cues","cue weighting"],"title":" Tracking talker-specific cues to lexical stress: Evidence from perceptual learning","type":"publication"},{"authors":null,"categories":null,"content":"This ‘Phonetics Day’ is the annual meeting of the ‘Dutch Society for Phonetic Sciences’ [NVFW], taking place in Utrecht on December 16, 2022.\nOpen to all! The ‘Dag van de Fonetiek’ 2022 will take place on Friday Dec 16, 2022, in the Sweelinckzaal at Drift 21, Utrecht, The Netherlands. It is an event celebrating everything ‘speechy’ and is free and open to all: members, non-members, scientists, students, anyone! This year, you can hear Ronny Bujok talk about whether beat gestures recalibrate lexical stress perception, and Orhun Uluşahin has some intriguing results about how listeners track a talker’s pitch!\nSee https://www.nvfw.org/ for the full program. We hope to see you there!\n","date":1671062400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671062400,"objectID":"ac2de7a6b09aa00cd0913eb3b278d25a","permalink":"https://hrbosker.github.io/news/22-12-15-dagvdfonetiek2022/","publishdate":"2022-12-15T00:00:00Z","relpermalink":"/news/22-12-15-dagvdfonetiek2022/","section":"news","summary":"This ‘Phonetics Day’ is the annual meeting of the ‘Dutch Society for Phonetic Sciences’ [NVFW], taking place in Utrecht on December 16, 2022.\n","tags":null,"title":"Come join us at the 'Dag van de Fonetiek'","type":"news"},{"authors":null,"categories":null,"content":"Congratulazioni a Giulio e Giuseppe for successfully publishing their collaborative project “Tracking talker-specific cues to lexical stress: Evidence from perceptual learning”!\nAccepted! Today we heard that the paper “Tracking talker-specific cues to lexical stress: Evidence from perceptual learning” has been accepted for publication in the Journal of Experimental Psychology: Human Perception and Performance (JEP:HPP), authored by Giulio Severijnen, Giuseppe Di Dona, Hans Rutger Bosker, and James McQueen. The fulltext and all data are publicly available at the links provided at the bottom of this post.\nWhat’s it about? The joint first-authors Giulio and Giuseppe set out to test whether listeners track how different talkers cue lexical stress. They first exposed two groups of listeners to two talkers: Talker A and B. These two talkers consistently used different cues to signal lexical stress in Dutch (e.g., differentiating PLAto from plaTEAU). Group 1 always heard Talker A use F0 to cue stressed syllables in Dutch, while Talker B always used intensity. Conversely, Group 2 heard the reverse talker-cue mappings: Talker A always used intensity, and Talker B always F0.\nAfter this (admittedly strange) exposure phase, participants were given an (admittedly even stranger) test phase. They were presented with audio recordings from the two talkers but this time the F0 and intensity cues had been artificially manipulated to ‘point in different directions’. For instance, while F0 would clearly cue stress on the first syllable of the word, intensity cues would signal stress on the second syllable. Critically, these ‘mixed items’ were perceived by listeners according to the talker-cue mappings they had learnt during exposure. That is, Group 1 had learnt that Talker A always used F0 in the exposure phase and therefore, at test, when they heard Talker A produce a mixed item, they were more likely to perceive stress on the syllable marked by F0. However, Group 2 was more likely to perceive the exact same mixed item as having stress on the syllable marked by intensity.\nWhy is this important? These findings support Bayesian models of spoken word recognition. These predict that listeners can adjust their prior beliefs about the perceptual weight of different phonetic cues on the basis of short-term regularities in a talker-specific fashion. This had already been observed for segmental contrasts (e.g., the perception of different consonants and vowels). Now we demonstrate that people also track suprasegmental variability in prosody, such as lexical stress.\nFull reference Giulio Severijnen, Giuseppe Di Dona, Hans Rutger Bosker, James M. McQueen (2023). Tracking talker-specific cues to lexical stress: Evidence from perceptual learning. Journal of Experimental Psychology: Human Perception and Performance. PDF Cite Dataset ","date":1670803200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670803200,"objectID":"cd8d99771cae9d5ba8364dd6a5115e78","permalink":"https://hrbosker.github.io/news/22-12-12-paper-jephpp/","publishdate":"2022-12-12T00:00:00Z","relpermalink":"/news/22-12-12-paper-jephpp/","section":"news","summary":"Congratulazioni a Giulio e Giuseppe for successfully publishing their collaborative project “Tracking talker-specific cues to lexical stress: Evidence from perceptual learning”!\n","tags":null,"title":"Paper accepted in JEP:HPP!","type":"news"},{"authors":null,"categories":null,"content":"The Gesture and Speech in Interaction [GESPIN] conference is coming to Nijmegen on September 13-15, 2023.\nBroadening perspectives, integrating views …that is the theme of the 2023 edition. This promises a highly interdisciplinary event, approaching the interaction of gesture and speech from the perspectives of language development, neurobiology, biomechanics, animal models, and many other fields. Keynotes are: Nuria Esteve Gibert, Yifei He, Susanne Fuchs, and Franz Goller. It is co-organized by CLS, the Donders Institute, and MPI. Paper submission opens January 10th, 2023 and the deadline is March 15th, 2023.\nSee https://gespin2023.nl for all the details. We hope to see you there!\n","date":1667260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667260800,"objectID":"9c10121ac5443dfbb3a908a4a9b44949","permalink":"https://hrbosker.github.io/news/22-11-01-gespin2023/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/news/22-11-01-gespin2023/","section":"news","summary":"The Gesture and Speech in Interaction [GESPIN] conference is coming to Nijmegen on September 13-15, 2023.\n","tags":null,"title":"GESPIN 2023 in Nijmegen","type":"news"},{"authors":["Giulio Severijnen","Hans Rutger Bosker","James M. McQueen"],"categories":null,"content":" ","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"c9b859ec7e1a4a133fec2bb356a2cba0","permalink":"https://hrbosker.github.io/publication/severijnen-etal-2022-speechprosody/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/severijnen-etal-2022-speechprosody/","section":"publication","summary":"The present study examined two acoustic cues in the production of lexical stress in Dutch: spectral tilt and overall intensity. Sluijter and Van Heuven (1996) reported that spectral tilt is a more reliable cue to stress than intensity. However, that study included only a small number of talkers (10) and only syllables with the vowels /aː/ and /ɔ/. The present study re-examined this issue in a larger and more variable dataset. We recorded 38 native speakers of Dutch (20 females) producing 744 tokens of Dutch segmentally overlapping words (e.g., *VOORnaam* vs. *voorNAAM*, “first name” vs. “respectable”), targeting 10 different vowels, in variable sentence contexts. For each syllable, we measured overall intensity and spectral tilt following Sluijter and Van Heuven (1996). Results from Linear Discriminant Analyses showed that, for the vowel /aː/ alone, spectral tilt showed an advantage over intensity, as evidenced by higher stressed/unstressed syllable classification accuracy scores for spectral tilt. However, when all vowels were included in the analysis, the advantage disappeared. These findings confirm that spectral tilt plays a larger role in signaling stress in Dutch /aː/ but show that, for a larger sample of Dutch vowels, overall intensity and spectral tilt are equally important.","tags":["lexical stress","spectral tilt","intensity","speech production","cue weighting","prosody"],"title":"Acoustic correlates of Dutch lexical stress re-examined: Spectral tilt is not always more reliable than intensity","type":"publication"},{"authors":["Eva Reinisch","Hans Rutger Bosker"],"categories":null,"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"34ced0d71e5fa8242a1551b9da50205a","permalink":"https://hrbosker.github.io/publication/reinisch-bosker-app/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/reinisch-bosker-app/","section":"publication","summary":"Temporal contrasts in speech are perceived relative to the speech rate of the surrounding context. That is, following a fast context sentence, listeners interpret a given target sound as longer than following a slow context, and vice versa. This rate effect, often referred to as \"rate-dependent speech perception\", has been suggested to be the result of a robust, low-level perceptual process, typically examined in quiet laboratory settings. However, speech perception often occurs in more challenging listening conditions. Therefore, we asked whether rate-dependent perception would be (partially) compromised by signal degradation relative to a clear listening condition. Specifically, we tested effects of white noise and reverberation, with the latter specifically distorting temporal information. We hypothesized that signal degradation would reduce the precision of encoding the speech rate in the context and thereby reduce the rate effect relative to a clear context. This prediction was borne out for both types of degradation in Experiment 1, where the context sentences but not the subsequent target words were degraded. However, in Experiment 2, which compared rate effects when contexts and targets were coherent in terms of signal quality, no reduction of the rate effect was found. This suggests that, when confronted with coherently degraded signals, listeners adapt to challenging listening situations, eliminating the difference between rate-dependent perception in clear and degraded conditions. Overall, the present study contributes towards understanding the consequences of different types of listening environments on the functioning of low-level perceptual processes that listeners use during speech perception.","tags":["speech rate","degraded speech perception","psycholinguistics","spoken word recognition"],"title":"Encoding speechrate in challenging listening conditions: white noise and reverberation","type":"publication"},{"authors":["Hans Rutger Bosker"],"categories":null,"content":" ","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"21892849beeab5b3993a13cb9c4bd643","permalink":"https://hrbosker.github.io/publication/bosker-2022-langspeech/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-2022-langspeech/","section":"publication","summary":"Individuals vary in how they produce speech. This variability affects both the segments (vowels  and consonants) and the suprasegmental properties of their speech (prosody). Previous literature  has demonstrated that listeners can adapt to variability in how different talkers pronounce the  segments of speech. This study shows that listeners can also adapt to variability in how talkers  produce lexical stress. Experiment 1 demonstrates a selective adaptation effect in lexical stress  perception- repeatedly hearing Dutch trochaic words biased perception of a subsequent lexical stress continuum towards more iamb responses. Experiment 2 demonstrates a recalibrationeffect in lexical stress perception- when ambiguous suprasegmental cues to lexical stress were disambiguated by lexical orthographic context as signaling a trochaic word in an exposure phase, Dutch participants categorized a subsequent test continuum as more trochee-like. Moreover, the selective adaptation and recalibration effects generalized to novel words, not encountered during exposure. Together, the experiments demonstrate that listeners also flexibly adapt to variability in the suprasegmental properties of speech, thus expanding our understanding of the utility of listener adaptation in speech perception. Moreover, the combined outcomes speak for an architecture of spoken word recognition involving abstract prosodic representations at a prelexical level of analysis.","tags":["lexical stress","recalibration","selective adaptation","suprasegmental cues","prosody"],"title":"Evidence for selective adaptation and recalibration in the perception of lexical stress","type":"publication"},{"authors":["Ronny Bujok","Antje S. Meyer","Hans Rutger Bosker"],"categories":null,"content":" ","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"a1c6d07ddf19db624006b31cb28d4b6f","permalink":"https://hrbosker.github.io/publication/bujok-etal-2022-sp/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bujok-etal-2022-sp/","section":"publication","summary":"Producing lexical stress leads to visible changes on the face, such as longer duration and greater size of the opening of the mouth. Research suggests that these visual cues alone can inform participants about which syllable carries stress (i.e., lip-reading silent videos). This study aims to determine the influence of visual articulatory cues on lexical stress perception in more naturalistic audiovisual settings. Participants were presented with seven disyllabic, Dutch minimal stress pairs (e.g., VOORnaam [first name] \u0026 voorNAAM [respectable]) in audio-only (phonetic lexical stress continua without video), video-only (lip-reading silent videos), and audiovisual trials (e.g., phonetic lexical stress continua with video of talker saying VOORnaam or voorNAAM). Categorization data from video-only trials revealed that participants could distinguish the minimal pairs above chance from seeing the silent videos alone. However, responses in the audiovisual condition did not differ from the audio-only condition. We thus conclude that visual lexical stress information on the face, while clearly perceivable, does not play a major role in audiovisual speech perception. This study demonstrates that clear unimodal effects do not always generalize to more naturalistic multimodal communication, advocating that speech prosody is best considered in multimodal settings.","tags":["lexical stress","audiovisual speech","articulatory cues","spoken-word recognition","prosody"],"title":"Visible lexical stress cues on the face do not influence audiovisual speech perception","type":"publication"},{"authors":["Joe Rodd","Caitlin Decuyper","Hans Rutger Bosker","Louis ten Bosch"],"categories":null,"content":" ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"a946f68c4cabe846a081b5edb6eef52a","permalink":"https://hrbosker.github.io/publication/rodd-etal-2021-brm/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/rodd-etal-2021-brm/","section":"publication","summary":"Despite advances in automatic speech recognition (ASR), human input is still essential for producing research-grade segmentations of speech data. Conventional approaches to manual segmentation are very labor-intensive. We introduce POnSS, a browser-based system that is specialized for the task of segmenting the onsets and offsets of words, which combines aspects of ASR with limited human input. In developing POnSS, we identified several sub-tasks of segmentation, and implemented each of these as separate interfaces for the annotators to interact with to streamline their task as much as possible. We evaluated segmentations made with POnSS against a baseline of segmentations of the same data made conventionally in Praat. We observed that POnSS achieved comparable reliability to segmentation using Praat, but required 23% less annotator time investment. Because of its greater efficiency without sacrificing reliability, POnSS represents a distinct methodological advance for the segmentation of speech data.","tags":["speech data","segmentation","research tool"],"title":"A tool for efficient and accurate segmentation of speech data: Announcing POnSS","type":"publication"},{"authors":["Hans Rutger Bosker","David Peeters"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"394cafb43835bfa97485f69172aac2cd","permalink":"https://hrbosker.github.io/publication/bosker-etal-2021-procroysocb/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2021-procroysocb/","section":"publication","summary":"Beat gestures—spontaneously produced biphasic movements of the hand are among the most frequently encountered co-speech gestures in human communication. They are closely temporally aligned to the prosodic characteristics of the speech signal, typically occurring on lexically stressed syllables. Despite their prevalence across speakers of the world’s languages, how beat gestures impact spoken word recognition is unclear. Can these simple ‘flicks of the hand’ influence speech perception? Across a range of experiments, we demonstrate that beat gestures influence the explicit and implicit perception of lexical stress (e.g. distinguishing ‘OBject’ from ‘obJECT’), and in turn can influence what vowels listeners hear. Thus, we provide converging evidence for a manual McGurk effect-relatively simple and widely occurring hand movements influence which speech sounds we hear.","tags":["audiovisual speech perception","manual McGurk effect","multimodal communication","beat gestures","lexical stress"],"title":"Beat gestures influence which speech sounds you hear","type":"publication"},{"authors":["Hans Rutger Bosker","Esperanza Badaya","Martin Corley"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"5be7cb5e80af5432428c7848acf38c78","permalink":"https://hrbosker.github.io/publication/bosker-etal-2021-discproc/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2021-discproc/","section":"publication","summary":"Speech in everyday conversations is riddled with discourse markers (DMs), such as well, you know, and like. However, in many lab-based studies of speech comprehension, such DMs are typically absent from the carefully articulated and highly controlled speech stimuli. As such, little is known about how these DMs influence online word recognition. The present study specifically investigated the online processing of DM like and how it influences the activation of words in the mental lexicon. We specifically targeted the cohort competitor (CC) effect in the Visual World Paradigm- Upon hearing spoken instructions to “pick up the beaker,” human listeners also typically fixate—next to the target object—referents that overlap phonologically with the target word (cohort competitors such as beetle; CCs). However, several studies have argued that CC effects are constrained by syntactic, semantic, pragmatic, and discourse constraints. Therefore, the present study investigated whether DM like influences online word recognition by activating its cohort competitors (e.g., lightbulb). In an eye-tracking  experiment using the Visual World Paradigm, we demonstrate that when participants heard spoken instructions such as “Now press the button for the, like... unicycle,” they showed anticipatory looks to the CC referent (lightbulb) well before hearing the target. This CC effect was sustained for a relatively long period of time, even despite hearing disambiguating information (i.e., the /k/ in like). Analysis of the reaction times also showed that participants were significantly faster to select CC targets (lightbulb) when preceded by DM like. These findings suggest that seemingly trivial DMs, such as like, activate their CCs, impacting online word recognition. Thus, we advocate a more holistic perspective on spoken language comprehension in naturalistic communication, including the processing of DMs.","tags":["cohort competitor effect","fillers","disfluency","word recognition","eye-tracking"],"title":"Discourse markers activate their, like, cohort competitors","type":"publication"},{"authors":["Giulio Severijnen","Hans Rutger Bosker","Piai Vitoria","James M. McQueen"],"categories":null,"content":" ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"cb4f47f2920130ed3a45ed6cf95e641f","permalink":"https://hrbosker.github.io/publication/severijnen-etal-2021-brainres/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/severijnen-etal-2021-brainres/","section":"publication","summary":"One of the challenges in speech perception is that listeners must deal with considerable segmental and suprasegmental variability in the acoustic signal due to differences between talkers. Most previous studies have focused on how listeners deal with segmental variability. In this EEG experiment, we investigated whether listeners track talker-specific usage of suprasegmental cues to lexical stress to recognize spoken words correctly. In a three-day training phase, Dutch participants learned to map non-word minimal stress pairs onto different object referents (e.g., USklot meant “lamp”; usKLOT meant “train”). These non-words were produced by two male talkers. Critically, each talker used only one suprasegmental cue to signal stress (e.g., Talker A used only F0 and Talker B only intensity). We expected participants to learn which talker used which cue to signal stress. In the test phase, participants indicated whether spoken sentences including these non-words were correct (“The word for lamp is…”). We found that participants were slower to indicate that a stimulus was correct if the non-word was produced with the unexpected cue (e.g., Talker A using intensity). That is, if in training Talker A used F0 to signal stress, participants experienced a mismatch between predicted and perceived phonological word-forms if, at test, Talker A unexpectedly used intensity to cue stress. In contrast, the N200 amplitude, an event-related potential related to phonological prediction, was not modulated by the cue mismatch. Theoretical implications of these contrasting results are discussed. The behavioral findings illustrate talker-specific prediction of prosodic cues, picked up through perceptual learning during training.","tags":["prosody","perceptual learning","lexical stress","phonological prediction","N200"],"title":"Listeners track talker-specific prosody to deal with talker-variability","type":"publication"},{"authors":["Hans Rutger Bosker"],"categories":null,"content":" ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"afe100f101d4e4afb230d571951c139b","permalink":"https://hrbosker.github.io/publication/bosker-2021-bookchapter/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-2021-bookchapter/","section":"publication","summary":"Speech contains pronounced amplitude modulations in the 1–9 Hz range, correlating with the syllabic rate of speech. Recent models of speech perception propose that this rhythmic nature of speech is central to speech recognition and has beneficial effects on language processing. Here, we investigated the contribution of amplitude modulations to the subjective impression listeners have of public speakers. The speech from US presidential candidates Hillary Clinton and Donald Trump in the three TV debates of 2016 was acoustically analyzed by means of modulation spectra. These indicated that Clinton’s speech had more pronounced amplitude modulations than Trump’s speech, particularly in the 1–9 Hz range. A subsequent perception experiment, with listeners rating the perceived charisma of (low-pass filtered versions of) Clinton’s and Trump’s speech, showed that more pronounced amplitude modulations (i.e., more ‘rhythmic’ speech) increased perceived charisma ratings. These outcomes highlight the important contribution of speech rhythm to charisma perception.","tags":["amplitude modulations","speech rhythm","modulation spectrum","charisma perception","temporal envelope","amplitude envelope"],"title":"The contribution of amplitude modulations in speech to perceived charisma","type":"publication"},{"authors":["Hans Rutger Bosker"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"cb9629ab6e5f9f3fab618c91c8136a05","permalink":"https://hrbosker.github.io/publication/bosker-2021-brm/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-2021-brm/","section":"publication","summary":"Many studies of speech perception assess the intelligibility of spoken sentence stimuli by means of transcription tasks (‘type out what you hear’). The intelligibility of a given stimulus is then often expressed in terms of percentage of words correctly reported from the target sentence. Yet scoring the participants’ raw responses for words correctly identified from the target sentence is a time-consuming task, and hence resource-intensive. Moreover, there is no consensus among speech scientists about what specific protocol to use for the human scoring, limiting the reliability of human scores. The present paper evaluates various forms of fuzzy string matching between participants’ responses and target sentences, as automated metrics of listener transcript accuracy. We demonstrate that one particular metric, the token sort ratio, is a consistent, highly efficient, and accurate metric for automated assessment of listener transcripts, as evidenced by high correlations with human-generated scores (best correlation r = 0.940) and a strong relationship to acoustic markers of speech intelligibility. Thus, fuzzy string matching provides a practical tool for assessment of listener transcript accuracy in large-scale speech intelligibility studies. See https//tokensortratio.netlify.app for an online implementation.","tags":["speech intelligibility","fuzzy string matching","transcription accuracy","automated assessment","token sort ratio"],"title":"Using fuzzy string matching for automated assessment of listener transcripts in speech intelligibility studies","type":"publication"},{"authors":["Anne Kosem","Hans Rutger Bosker","Ole Jensen","Peter Hagoort","Lars Riecke"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"7e338625612b44260276d9898b481815","permalink":"https://hrbosker.github.io/publication/kosem-etal-2020-jcn/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/kosem-etal-2020-jcn/","section":"publication","summary":"Recent neuroimaging evidence suggests that the frequency of entrained oscillations in auditory cortices influences the perceived duration of speech segments, impacting word perception [Kösem, A., Bosker, H. R., Takashima, A., Meyer, A., Jensen, O., \u0026 Hagoort, P. Neural entrainment determines the words we hear. Current Biology, 28, 2867–2875, 2018]. We further tested the causal influence of neural entrainment frequency during speech processing, by manipulating entrainment with continuous transcranial alternating current stimulation (tACS) at distinct oscillatory frequencies (3 and 5.5 Hz) above the auditory cortices. Dutch participants listened to speech and were asked to report their percept of a target Dutch word, which contained a vowel with an ambiguous duration. Target words were presented either in isolation (first experiment) or at the end of spoken sentences (second experiment). We predicted that the tACS frequency would influence neural entrainment and therewith how speech is perceptually sampled, leading to a perceptual overestimation or underestimation of the vowel’s duration. Whereas results from Experiment 1 did not confirm this prediction, results from Experiment 2 suggested a small effect of tACS frequency on target word perception- Faster tACS leads to more long-vowel word percepts, in line with the previous neuroimaging findings. Importantly, the difference in word perception induced by the different tACS frequencies was significantly larger in Experiment 1 versus Experiment 2, suggesting that the impact of tACS is dependent on the sensory context. tACS may have a stronger effect on spoken word perception when the words are presented in continuous speech as compared to when they are isolated, potentially because prior (stimulus-induced) entrainment of brain oscillations might be a prerequisite for tACS to be effective.","tags":null,"title":"Biasing the perception of spoken words with transcranial alternating current stimulation","type":"publication"},{"authors":["Greta Kaufeld","Wibke Naumann","Antje S. Meyer","Hans Rutger Bosker","Andrea E. Martin}"],"categories":null,"content":" ","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"5f8bd0f6ae39380aa7439c7cbd900666","permalink":"https://hrbosker.github.io/publication/kaufeld-etal-2020-lcn/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/kaufeld-etal-2020-lcn/","section":"publication","summary":"Understanding spoken language requires the integration and weighting of multiple cues, and may call on cue integration mechanisms that have been studied in other areas of perception. In the current study, we used eye-tracking (visual-world paradigm) to examine how contextual speech rate (a lower-level, perceptual cue) and morphosyntactic knowledge (a higher-level, linguistic cue) are iteratively combined and integrated. Results indicate that participants used contextual rate information immediately, which we interpret as evidence of perceptual inference and the generation of predictions about upcoming morphosyntactic information. Additionally, we observed that early rate effects remained active in the presence of later conflicting lexical information. This result demonstrates that (1) contextual speech rate functions as a cue to morphosyntactic inferences, even in the presence of subsequent disambiguating information; and (2) listeners iteratively use multiple sources of information to draw inferences and generate predictions during speech comprehension. We discuss the implication of these demonstrations for theories of language processing.","tags":["language comprehension","speech perception","cue integration","morphology","rate normalization"],"title":"Contextual speech rate influences morphosyntactic prediction and integration","type":"publication"},{"authors":["Joe Rodd","Hans Rutger Bosker","Mirjam Ernestus","Phillip M. Alday","Antje S. Meyer","Louis ten Bosch"],"categories":null,"content":" ","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"af62ce577b2d57d5d13dcbc0db40028f","permalink":"https://hrbosker.github.io/publication/rodd-etal-2020-psychreview/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/rodd-etal-2020-psychreview/","section":"publication","summary":"That speakers can vary their speaking rate is evident, but how they accomplish this has hardly been studied. Consider this analogy: When walking, speed can be continuously increased, within limits, but to speed up further, humans must run. Are there multiple qualitatively distinct speech “gaits” that resemble walking and running? Or is control achieved by continuous modulation of a single gait? This study investigates these possibilities through simulations of a new connectionist computational model of the cognitive process of speech production, EPONA, that borrows from Dell, Burger, and Svec’s (1997) model. The model has parameters that can be adjusted to fit the temporal characteristics of speech at different speaking rates. We trained the model on a corpus of disyllabic Dutchwords produced at different speaking rates. During training, different clusters of parameter values (regimes) were identified for different speaking rates. In a 1-gait system, the regimes used to achieve fast and slow speech are qualitatively similar, but quantitatively different. In a multiple gait system, there is no linear relationship between the parameter settings associated with each gait, resulting in an abrupt shift in parameter values to move from speaking slowly to speaking fast. After training, the model achieved good fits in all three speaking rates. The parameter settings associated with each speaking rate were not linearly related, suggesting the presence of cognitive gaits. Thus, we provide the first computationally explicit account of the ability to modulate the speech production system to achieve different speaking styles.","tags":["speech production","speech rate","connectionist models","executive control","lexical access"],"title":"Control of speaking rate is achieved by switching between qualitatively distinct cognitive ‘gaits’: Evidence from simulation","type":"publication"},{"authors":["Hans Rutger Bosker","Martin Cooke"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"a41a7cc0e2d07199ea693b60e5b3d01e","permalink":"https://hrbosker.github.io/publication/bosker-etal-2020-jasa/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2020-jasa/","section":"publication","summary":"Speakers adjust their voice when talking in noise, which is known as Lombard speech. These acoustic adjustments facilitate speech comprehension in noise relative to plain speech (i.e., speech produced in quiet). However, exactly which characteristics of Lombard speech drive this intelligibility benefit in noise remains unclear. This study assessed the contribution of enhanced amplitude modulations to the Lombard speech intelligibility benefit by demonstrating that (1) native speakers of Dutch in the Nijmegen Corpus of Lombard Speech produce more pronounced amplitude modulations in noise vs in quiet; (2) more enhanced amplitude modulations correlate positively with intelligibility in a speech-in-noise perception experiment; (3) transplanting the amplitude modulations from Lombard speech onto plain speech leads to an intelligibility improvement, suggesting that enhanced amplitude modulations in Lombard speech contribute towards intelligibility in noise. Results are discussed in light of recent neurobiological models of speech perception with reference to neural oscillators phase-locking to the amplitude modulations in speech, guiding the processing of speech.","tags":["Lombard speech","speech in noise","amplitude modulations","prosody transplantation","neural entrainment"],"title":"Enhanced amplitude modulations contribute to the Lombard intelligibility benefit: Evidence from the Nijmegen Corpus of Lombard Speech","type":"publication"},{"authors":["Merel Maslowski","Antje S. Meyer","Hans Rutger Bosker"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"076057a4bbefc492977ad8053af494e8","permalink":"https://hrbosker.github.io/publication/maslowski-etal-2020-jephpp/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/maslowski-etal-2020-jephpp/","section":"publication","summary":"To comprehend speech sounds, listeners tune in to speech rate information in the proximal (immediately adjacent), distal (nonadjacent), and global context (further removed preceding and following sentences). Effects of global contextual speech rate cues on speech perception have been shown to follow constraints not found for proximal and distal speech rate. Therefore, listeners may process such global cues at distict time points during word recognition. We conducted a printed-word eye-tracking experiment to compare the time courses of distal and global rate effects. Results indicated that the distal rate effect emerged immediately after target sound presentation, in line with a general-auditory account. The global rate effect, however, arose more than 200 ms later than the distal rate effect, indicating that distal and global context effects involve distinct processing mechanisms. Results are interpreted in a 2-stage model of acoustic context effects. This model posits that distal context effects involve very early perceptual processes, while global context effects arise at a later stage, involving cognitive adjustments conditioned by higher-level information.","tags":["rate normalization","distal context","global context","two-stage model","eye-tracking"],"title":"Eye-tracking the time course of distal and global speech rate effects","type":"publication"},{"authors":["Marjolein van Os","Nivja H. de Jong","Hans Rutger Bosker"],"categories":null,"content":" ","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"f2da57a002b37c1192f3369a211f909d","permalink":"https://hrbosker.github.io/publication/vanos-etal-2020-ll/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/vanos-etal-2020-ll/","section":"publication","summary":"Fluency is an important part of research on second language learning, but most research on language proficiency typically has not included oral fluency as part of interactions even though natural communication usually occurs in conversations. The present study considered aspects of turn-taking behavior as part of the construct of fluency and investigated whether these aspects differentially influence perceived fluency ratings of native and nonnative speech. Results from two experiments using acoustically manipulated speech showed that, in native speech, too “eager” answers (interrupting a question with a fast answer) and too “reluctant” answers (answering slowly after a long turn gap) negatively affected fluency ratings. However, in nonnative speech, only too “reluctant” answers led to lower fluency ratings. Thus, we demonstrated that acoustic properties of dialogue are perceived as part of fluency. By adding to the current understanding of dialogue fluency, these lab-based findings carry implications for language teaching and assessment.","tags":["fluency","turn-taking","turn gaps","dialogue","ratings"],"title":"Fluency in dialogue: Turn‐taking behavior shapes perceived fluency in native and nonnative speech","type":"publication"},{"authors":["Hans Rutger Bosker","David Peeters","Judith Holler"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"160b1f6e0e16f8ef5cb2034fad1960c7","permalink":"https://hrbosker.github.io/publication/bosker-etal-2020-qjep/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2020-qjep/","section":"publication","summary":"Spoken words are highly variable and therefore listeners interpret speech sounds relative to the surrounding acoustic context, such as the speech rate of a preceding sentence. For instance, a vowel midway between short /ɑ/ and long /a/ in Dutch is perceived as short /ɑ/ in the context of preceding slow speech, but as long /a/ if preceded by a fast context. Despite the well-established influence of visual articulatory cues on speech comprehension, it remains unclear whether visual cues to speech rate also influence subsequent spoken word recognition. In two “Go Fish”–like experiments, participants were presented with audio-only (auditory speech + fixation cross), visual-only (mute videos of talking head), and audiovisual (speech + videos) context sentences, followed by ambiguous target words containing vowels midway between short /ɑ/ and long /a/. In Experiment 1, target words were always presented auditorily, without visual articulatory cues. Although the audio-only and audiovisual contexts induced a rate effect (i.e., more long /a/ responses after fast contexts), the visual-only condition did not. When, in Experiment 2, target words were presented audiovisually, rate effects were observed in all three conditions, including visual-only. This suggests that visual cues to speech rate in a context sentence influence the perception of following visual target cues (e.g., duration of lip aperture), which at an audiovisual integration stage bias participants’ target categorisation responses. These findings contribute to a better understanding of how what we see influences what we hear.","tags":["speech rate","neural entrainment","audiovisual speech perception","rate-dependent perception","rate normalisation","supramodal perception"],"title":"How visual cues to speech rate influence speech perception","type":"publication"},{"authors":["Greta Kaufeld","Anna Ravenschlag","Antje S. Meyer","Andrea E. Martin","Hans Rutger Bosker"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"8cd3aa663a7d5a88707fa58237e3d081","permalink":"https://hrbosker.github.io/publication/kaufeld-etal-2020-jeplmc/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/kaufeld-etal-2020-jeplmc/","section":"publication","summary":"During spoken language comprehension, listeners make use of both knowledge-based and signal-based sources of information, but little is known about how cues from these distinct levels of representational hierarchy are weighted and integrated online. In an eye-tracking experiment using the visual world paradigm, we investigated the flexible weighting and integration of morphosyntactic gender marking (a knowledge-based cue) and contextual speech rate (a signal-based cue). We observed that participants used the morphosyntactic cue immediately to make predictions about upcoming referents, even in the presence of uncertainty about the cue’s reliability. Moreover, we found speech rate normalization effects in participants’ gaze patterns even in the presence of preceding morphosyntactic information. These results demonstrate that cues are weighted and integrated flexibly online, rather than adhering to a strict hierarchy. We further found rate normalization effects in the looking behavior of participants who showed a strong behavioral preference for the morphosyntactic gender cue. This indicates that rate normalization effects are robust and potentially automatic. We discuss these results in light of theories of cue integration and the two-stage model of acoustic context effects.","tags":["language comprehension","speech perception","cue integration","rate normalization"],"title":"Knowledge-based and signal-based cues are weighted flexibly during spoken language comprehension","type":"publication"},{"authors":["Greta Kaufeld","Hans Rutger Bosker","Sanne ten Oever","Philip M. Alday","Antje S. Meyer","Andrea E. Martin"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"9e40ccf211a40da78fd81c04a173df3b","permalink":"https://hrbosker.github.io/publication/kaufeld-etal-2020-jneuro/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/kaufeld-etal-2020-jneuro/","section":"publication","summary":"Neural oscillations track linguistic information during speech comprehension (Ding et al., 2016; Keitel et al., 2018), and are known to be modulated by acoustic landmarks and speech intelligibility (Doelling et al., 2014; Zoefel and VanRullen, 2015). However, studies investigating linguistic tracking have either relied on non-naturalistic isochronous stimuli or failed to fully control for prosody. Therefore, it is still unclear whether low-frequency activity tracks linguistic structure during natural speech, where linguistic structure does not follow such a palpable temporal pattern. Here, we measured electroencephalography (EEG) and manipulated the presence of semantic and syntactic information apart from the timescale of their occurrence, while carefully controlling for the acoustic-prosodic and lexical-semantic information in the signal. EEG was recorded while 29 adult native speakers (22 women, 7 men) listened to naturally spoken Dutch sentences, jabberwocky controls with morphemes and sentential prosody, word lists with lexical content but no phrase structure, and backward acoustically matched controls. Mutual information (MI) analysis revealed sensitivity to linguistic content: MI was highest for sentences at the phrasal (0.8–1.1 Hz) and lexical (1.9–2.8 Hz) timescales, suggesting that the delta-band is modulated by lexically driven combinatorial processing beyond prosody, and that linguistic content (i.e., structure and meaning) organizes neural oscillations beyond the timescale and rhythmicity of the stimulus. This pattern is consistent with neurophysiologically inspired models of language comprehension (Martin, 2016, 2020; Martin and Doumas, 2017) where oscillations encode endogenously generated linguistic content over and above exogenous or stimulus-driven timing and rhythm information.","tags":["combinatorial processing","lexical semantics","mutual information","neural oscillations","prosody","sentence comprehension"],"title":"Linguistic structure and meaning organize neural oscillations into a content-specific hierarchy","type":"publication"},{"authors":["Hans Rutger Bosker","Matthias J. Sjerps","Eva Reinisch"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"b61bf53d11c1ab28b52639de46e622f0","permalink":"https://hrbosker.github.io/publication/bosker-etal-2020-app/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2020-app/","section":"publication","summary":"Speech sounds are perceived relative to spectral properties of surrounding speech. For instance, target words ambiguous between /bɪt/ (with low F1) and /bɛt/ (with high F1) are more likely to be perceived as “bet” after a ‘low F1’ sentence, but as “bit” after a ‘high F1’ sentence. However, it is unclear how these spectral contrast effects (SCEs) operate in multi-talker listening conditions. Recently, Feng and Oxenham [(2018b). J.Exp.Psychol.-Hum.Percept.Perform. 44(9), 1447–1457] reported that selective attention affected SCEs to a small degree, using two simultaneously presented sentences produced by a single talker. The present study assessed the role of selective attention in more naturalistic ‘cocktail party’ settings, with 200 lexically unique sentences, 20 target words, and different talkers. Results indicate that selective attention to one talker in one ear (while ignoring another talker in the other ear) modulates SCEs in such a way that only the spectral properties of the attended talker influences target perception. However, SCEs were much smaller in multi-talker settings (Experiment 2) than those in single-talker settings (Experiment 1). Therefore, the influence of SCEs on speech comprehension in more naturalistic settings (i.e., with competing talkers) may be smaller than estimated based on studies without competing talkers.","tags":["spectral contrast effects","spectral normalization","selective attention","stream segregation","cocktail party listening"],"title":"Spectral contrast effects are modulated by selective attention in ‘cocktail party’ settings","type":"publication"},{"authors":["Hans Rutger Bosker","Matthias J. Sjerps","Eva Reinisch"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"9441a5b9cbe776f21a2688c67fdd3031","permalink":"https://hrbosker.github.io/publication/bosker-etal-2020-scirep/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2020-scirep/","section":"publication","summary":"Two fundamental properties of perception are selective attention and perceptual contrast, but how these two processes interact remains unknown. Does an attended stimulus history exert a larger contrastive influence on the perception of a following target than unattended stimuli? Dutch listeners categorized target sounds with a reduced prefix “ge-” marking tense (e.g., ambiguous between gegaan-gaan “gone-go”). In ‘single talker’ Experiments 1–2, participants perceived the reduced syllable (reporting gegaan) when the target was heard after a fast sentence, but not after a slow sentence (reporting gaan). In ‘selective attention’ Experiments 3–5, participants listened to two simultaneous sentences from two different talkers, followed by the same target sounds, with instructions to attend only one of the two talkers. Critically, the speech rates of attended and unattended talkers were found to equally influence target perception – even when participants could watch the attended talker speak. In fact, participants’ target perception in ‘selective attention’ Experiments 3–5 did not differ from participants who were explicitly instructed to divide their attention equally across the two talkers (Experiment 6). This suggests that contrast effects of speech rate are immune to selective attention, largely operating prior to attentional stream segregation in the auditory processing hierarchy.","tags":["temporal contrast effects","rate normalization","selective attention","stream segregation","cocktail party listening"],"title":"Temporal contrast effects in human speech perception are immune to selective attention","type":"publication"},{"authors":["Hans Rutger Bosker","Marjolein van Os,","Rik Does","Geertje van Bergen"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"d9f3a515881dbb1b8aa2785fbac0fb35","permalink":"https://hrbosker.github.io/publication/bosker-etal-2019-jml/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2019-jml/","section":"publication","summary":"Disfluencies, like uh, have been shown to help listeners anticipate reference to low-frequency words. The associative account of this ‘disfluency bias’ proposes that listeners learn to associate disfluency with low-frequency referents based on prior exposure to non-arbitrary disfluency distributions (i.e., greater probability of low-frequency words after disfluencies). However, there is limited evidence for listeners actually tracking disfluency distributions online. The present experiments are the first to show that adult listeners, exposed to a typical or more atypical disfluency distribution (i.e., hearing a talker unexpectedly say uh before high-frequency words), flexibly adjust their predictive strategies to the disfluency distribution at hand (e.g., learn to predict high-frequency referents after disfluency). However, when listeners were presented with the same atypical disfluency distribution but produced by a non-native speaker, no adjustment was observed. This suggests pragmatic inferences can modulate distributional learning, revealing the flexibility of, and constraints on, distributional learning in incremental language comprehension.","tags":["distributional learning","pragmatic inferences","disfluencies","non-native speech","prediction","eye-tracking"],"title":"Counting ‘uhm’s: how tracking the distribution of native and non-native disfluencies influences online language comprehension","type":"publication"},{"authors":["Joe Rodd","Hans Rutger Bosker","Louis ten Bosch","Mirjam Ernestus"],"categories":null,"content":" ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"cbf0fcb25c0b05fa2e75c569cb77e8af","permalink":"https://hrbosker.github.io/publication/rodd-etal-2019-jasa/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/rodd-etal-2019-jasa/","section":"publication","summary":"Many psycholinguistic models of speech sequence planning make claims about the onset and offset times of planning units, such as words, syllables, and phonemes. These predictions typically go untested, however, since psycholinguists have assumed that the temporal dynamics of the speech signal is a poor index of the temporal dynamics of the underlying speech planning process. This article argues that this problem is tractable, and presents and validates two simple metrics that derive planning unit onset and offset times from the acoustic signal and articulatographic data.","tags":null,"title":"Deriving the onset and offset times of planning units from acoustic and articulatory measurements","type":"publication"},{"authors":["Merel Maslowski","Antje S. Meyer","Hans Rutger Bosker"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"58667e15717a4744f554472fbc50754e","permalink":"https://hrbosker.github.io/publication/maslowski-etal-2019-jeplmc/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/maslowski-etal-2019-jeplmc/","section":"publication","summary":"Listeners are known to track statistical regularities in speech. Yet, which temporal cues are encoded is unclear. This study tested effects of talker-specific habitual speech rate and talker-independent average speech rate (heard over a longer period of time) on the perception of the temporal Dutch vowel contrast /ɑ/–/a:/. First, Experiment 1 replicated that slow local (surrounding) speech contexts induce fewer long /a:/ responses than faster contexts. Experiment 2 tested effects of long-term habitual speech rate. A high-rate group listened to ambiguous vowels embedded in “neutral” speech from Talker A, intermixed with fast speech from Talker B. A low-rate group listened to the same neutral speech from Talker A, and/but to Talker B speaking at a slow rate. Between-groups comparison of the neutral trials showed that the high-rate group demonstrated a lower proportion of /a:/ responses, indicating that Talker A’s habitual speech rate sounded slower when B was faster. In Experiment 3, both talkers produced speech at both rates, removing the different habitual speech rates of Talkers A and B, while maintaining the average rates differing between groups. In Experiment 3, no global rate effect was observed. Taken together, the present experiments show that a talker’s habitual rate is encoded relative to the habitual rate of another talker, carrying implications for episodic and constraint-based models of speech perception.","tags":["speech rate","rate-dependent perception","rate normalization","habitual speech rate"],"title":"How the tracking of habitual rate influences speech perception","type":"publication"},{"authors":["Merel Maslowski","Antje S. Meyer","Hans Rutger Bosker"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"abe220dc07750360b49269cfd402f1da","permalink":"https://hrbosker.github.io/publication/maslowski-etal-2019-jasa/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/maslowski-etal-2019-jasa/","section":"publication","summary":"Speech can be produced at different rates. Listeners take this rate variation into account by normalizing vowel duration for contextual speech rate: An ambiguous Dutch word /m?t/ is perceived as short /mAt/ when embedded in a slow context, but long /ma:t/ in a fast context. While some have argued that this rate normalization involves low-level automatic perceptual processing, there is also evidence that it arises at higher-level cognitive processing stages, such as decision making. Prior research on rate-dependent speech perception has only used explicit recognition tasks to investigate the phenomenon, involving both perceptual processing and decision making. This study tested whether speech rate normalization can be observed without explicit decision making, using a crossmodal repetition priming paradigm. Results show that a fast precursor sentence makes an embedded ambiguous prime (/m?t/) sound (implicitly) more /a:/-like, facilitating lexical access to the long target word “maat” in a (explicit) lexical decision task. This result suggests that rate normalization is automatic, taking place even in the absence of an explicit recognition task. Thus, rate normalization is placed within the realm of everyday spoken conversation, where explicit categorization of ambiguous sounds is rare.","tags":["speech rate","rate-dependent perception","rate normalization","habitual speech rate"],"title":"Listeners normalize speech for contextual speech rate even without an explicit recognition task","type":"publication"},{"authors":["Hans Rutger Bosker","Oded Ghitza"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"b50c266bfb995a1cad9fecefa2f893f9","permalink":"https://hrbosker.github.io/publication/bosker-etal-2018-lcn/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2018-lcn/","section":"publication","summary":"This psychoacoustic study provides behavioural evidence that neural entrainment in the theta range (3–9 Hz) causally shapes speech perception. Adopting the “rate normalization” paradigm (presenting compressed carrier sentences followed by uncompressed target words), we show that uniform compression of a speech carrier to syllable rates inside the theta range influences perception of subsequent uncompressed targets, but compression outside theta range does not. However, the influence of carriers – compressed outside theta range – on target perception is salvaged when carriers are “repackaged” to have a packet rate inside theta. This suggests that the brain can only successfully entrain to syllable/packet rates within theta range, with a causal influence on the perception of subsequent speech, in line with recent neuroimaging data. Thus, this study points to a central role for sustained theta entrainment in rate normalisation and contributes to our understanding of the functional role of brain oscillations in speech perception.","tags":["neural entrainment","theta oscillations","speech rate","rate normalization"],"title":"Entrained theta oscillations guide perception of subsequent speech: Behavioral evidence from rate normalization","type":"publication"},{"authors":["Hans Rutger Bosker","Geertje van Bergen"],"categories":null,"content":" ","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"c2f7ff39a6e36571db77d276fabb2f04","permalink":"https://hrbosker.github.io/publication/bosker-etal-2018-jml/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2018-jml/","section":"publication","summary":"Interpersonal discourse particles (DPs), such as Dutch inderdaad (≈‘indeed’) and eigenlijk (≈‘actually’) are highly frequent in everyday conversational interaction. Despite extensive theoretical descriptions of their polyfunctionality, little is known about how they are used by language comprehenders. In two visual world eyetracking experiments involving an online dialogue completion task, we asked to what extent inderdaad, confirming an inferred expectation, and eigenlijk, contrasting with an inferred expectation, influence real-time understanding of dialogues. Answers in the dialogues contained a DP or a control adverb, and a critical discourse referent was replaced by a beep; participants chose the most likely dialogue completion by clicking on one of four referents in a display. Results show that listeners make rapid and fine-grained situation-specific inferences about the use of DPs, modulating their expectations about how the dialogue will unfold. Findings further specify and constrain theories about the conversation-managing function and polyfunctionality of DPs.","tags":["discourse particles","discourse processing","dialogue","polyfunctionality","visual world eye-tracking"],"title":"Linguistic expectation management in online discourse processing: An investigation of Dutch inderdaad ‘indeed’ and eigenlijk ‘actually’","type":"publication"},{"authors":["Merel Maslowski","Antje S. Meyer","Hans Rutger Bosker"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"1c884357b0088b688c1c441ce55910a2","permalink":"https://hrbosker.github.io/publication/maslowski-etal-2018/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/maslowski-etal-2018/","section":"publication","summary":"Listeners are known to use adjacent contextual speech rate in processing temporally ambiguous speech sounds. For instance, an ambiguous vowel between short /α/ and long /a:/ in Dutch sounds relatively long (i.e., as /a:/) embedded in a fast precursor sentence, but short in a slow sentence. Besides the local speech rate, listeners also track talker-specific global speech rates. However, it is yet unclear whether other talkers’ global rates are encoded with reference to a listener’s self-produced rate. Three experiments addressed this question. In Experiment 1, one group of participants was instructed to speak fast, whereas another group had to speak slowly. The groups were compared on their perception of ambiguous /α/-/a:/ vowels embedded in neutral rate speech from another talker. In Experiment 2, the same participants listened to playback of their own speech and again evaluated target vowels in neutral rate speech. Neither of these experiments provided support for the involvement of self-produced speech in perception of another talker’s speech rate. Experiment 3 repeated Experiment 2 but with a new participant sample that was unfamiliar with the participants from Experiment 2. This experiment revealed fewer /a:/ responses in neutral speech in the group also listening to a fast rate, suggesting that neutral speech sounds slow in the presence of a fast talker and vice versa. Taken together, the findings show that selfproduced speech is processed differently from speech produced by others. They carry implications for our understanding of rate-dependent speech perception in dialogue settings, suggesting that both perceptual and cognitive mechanisms are involved.","tags":["rate normalization","self perception","speech rate"],"title":"Listening to yourself is special: Evidence from global speech rate tracking","type":"publication"},{"authors":["Anne Kösem","Hans Rutger Bosker","Atsuko  Takashima","Antje S. Meyer","Ole Jensen","Peter Hagoort"],"categories":null,"content":" ","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"00158eb073cf11d7662c3887ab8a0cac","permalink":"https://hrbosker.github.io/publication/kosem-etal-2018-currbiol/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/kosem-etal-2018-currbiol/","section":"publication","summary":"Low-frequency neural entrainment to rhythmic input has been hypothesized as a canonical mechanismthat shapes sensory perception in time. Neural entrainment is deemed particularly relevant for speech analysis, as it would contribute to the extraction of discrete linguistic elements from continuous acoustic signals. However, its causal influence in speech perception has been difficult to establish. Here, we provide evidence that oscillations build temporal predictions about the duration of speech tokens that affect perception. Using magnetoencephalography (MEG), we studied neural dynamics during listening to sentences that changed in speech rate. We observed neural entrainment to preceding speech rhythms persisting for several cycles after the change in rate. The sustained entrainment was associated with changes in the perceived duration of the last word’s vowel, resulting in the perception of words with different meanings. These findings support oscillatory models of speech processing, suggesting that neural oscillations actively shape speech perception.","tags":["speech","rhythm","temporal prediction","neural oscillations","MEG","rate normalization"],"title":"Neural entrainment determines the words we hear","type":"publication"},{"authors":["Hans Rutger Bosker"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"5277515244fb2c112e399c24f290a106","permalink":"https://hrbosker.github.io/publication/bosker-2018-jasa/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-2018-jasa/","section":"publication","summary":"Recently, the world’s attention was caught by an audio clip that was perceived as “Laurel” or “Yanny.” Opinions were sharply split. Many could not believe others heard something different from their perception. However, a crowd-source experiment with \u003e500 participants shows that it is possible to make people hear Laurel, where they previously heard Yanny, by manipulating preceding acoustic context. This study is not only the first to reveal within-listener variation in Laurel/ Yanny percepts, but also to demonstrate contrast effects for global spectral information in larger frequency regions. Thus, it highlights the intricacies of human perception underlying these social media phenomena.","tags":["speech perception","crowd-source experiment","acoustic context effects","contrast effects","social media"],"title":"Putting Laurel and Yanny in context","type":"publication"},{"authors":["Hans Rutger Bosker","Martin Cooke"],"categories":null,"content":" ","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"d7f61337abb55475eab648c185c44c33","permalink":"https://hrbosker.github.io/publication/bosker-etal-2018-jasa/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2018-jasa/","section":"publication","summary":"Speakers adjust their voice when talking in noise (known as Lombard speech), facilitating speech comprehension. Recent neurobiological models of speech perception emphasize the role of amplitude modulations in speech-in-noise comprehension, helping neural oscillators to “track” the attended speech. This study tested whether talkers produce more pronounced amplitude modulations in noise. Across four different corpora, modulation spectra showed greater power in amplitude modulations below 4 Hz in Lombard speech compared to matching plain speech. This suggests that noise-induced speech contains more pronounced amplitude modulations, potentially helping the listening brain to entrain to the attended talker, aiding comprehension.","tags":["Lombard speech","speech in noise","amplitude modulations","rhythmicity","rhythm"],"title":"Talkers produce more pronounced amplitude modulations when speaking in noise","type":"publication"},{"authors":["Hans Rutger Bosker"],"categories":null,"content":" ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"9cafbdbc17484ba87e2d37f66fca2de9","permalink":"https://hrbosker.github.io/publication/bosker-2017-app/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-2017-app/","section":"publication","summary":"The perception of temporal contrasts in speech is known to be influenced by the speech rate in the surrounding context. This rate-dependent perception is suggested to involve general auditory processes because it is also elicited by non-speech contexts, such as pure tone sequences. Two general auditory mechanisms have been proposed to underlie rate-dependent perception, durational contrast and neural entrainment. This study compares the predictions of these two accounts of rate-dependent speech perception by means of four experiments, in which participants heard tone sequences followed by Dutch target words ambiguous between /ɑs/ 'ash' and /a:s/ 'bait'. Tone sequences varied in the duration of tones (short vs. long) and in the presentation rate of the tones (fast vs. slow). Results show that the duration of preceding tones did not influence target perception in any of the experiments, thus challenging durational contrast as explanatory mechanism behind rate-dependent perception. Instead, the presentation rate consistently elicited a category boundary shift, with faster presentation rates inducing more /a:s/ responses, but only if the tone sequence was isochronous. Therefore, this study proposes an alternative, neurobiologically plausible account of rate-dependent perception involving neural entrainment of endogenous oscillations to the rate of a rhythmic stimulus","tags":["Speech rate","Rate-dependent perception","Rate normalization","Durational contrast","Neural entrainment"],"title":"Accounting for rate-dependent category boundary shifts in speech perception","type":"publication"},{"authors":["Hans Rutger Bosker","Anne Kösem"],"categories":null,"content":" ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"8c924dd435401c21b24ccd086ce3b67c","permalink":"https://hrbosker.github.io/publication/bosker-kosem-2017-interspeech/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-kosem-2017-interspeech/","section":"publication","summary":"Brain oscillations have been shown to track the slow amplitude fluctuations in speech during comprehension. Moreover, there is evidence that these stimulus-induced cortical rhythms may persist even after the driving stimulus has ceased. However, how exactly this neural entrainment shapes speech perception remains debated. This behavioral study investigated whether and how the frequency and phase of an entrained rhythm would influence the temporal sampling of subsequent speech. In two behavioral experiments, participants were presented with slow and fast isochronous tone sequences, followed by Dutch target words ambiguous between as /ɑs/ “ash” (with a short vowel) and aas /a:s/ “bait” (with a long vowel). Target words were presented at various phases of the entrained rhythm. Both experiments revealed effects of the frequency of the tone sequence on target word perception: fast sequences biased listeners to more long /a:s/ responses. However, no evidence for phase effects could be discerned. These findings show that an entrained rhythm’s frequency, but not phase, influences the temporal sampling of subsequent speech. These outcomes are compatible with theories suggesting that sensory timing is evaluated relative to entrained frequency. Furthermore, they suggest that phase tracking of (syllabic) rhythms by theta oscillations plays a limited role in speech parsing.","tags":["neural entrainment","phase-locking","temporal sampling","speech parsing","rate normalization","speech rate"],"title":"An entrained rhythm’s frequency, not phase, influences temporal sampling of speech","type":"publication"},{"authors":["Hans Rutger Bosker","Eva Reinisch","Matthias J. Sjerps"],"categories":null,"content":" ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"99e0771d11bbe42e8f80a12590f2fd84","permalink":"https://hrbosker.github.io/publication/bosker-etal-2017-jml/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2017-jml/","section":"publication","summary":"In natural situations, speech perception often takes place during the concurrent execution of other cognitive tasks, such as listening while viewing a visual scene. The execution of a dual task typically has detrimental effects on concurrent speech perception, but how exactly cognitive load disrupts speech encoding is still unclear. The detrimental effect on speech representations may consist of either a general reduction in the robustness of processing of the speech signal (‘noisy encoding’), or, alternatively it may specifically influence the temporal sampling of the sensory input, with listeners missing temporal pulses, thus underestimating segmental durations (‘shrinking of time’). The present study investigated whether and how spectral and temporal cues in a precursor sentence that has been processed under high vs. low cognitive load influence the perception of a subsequent target word. If cognitive load effects are implemented through ‘noisy encoding’, increasing cognitive load during the  recursor should attenuate the encoding of both its temporal and spectral cues, and hence reduce the contextual effect that these cues can have on subsequent target sound perception. However, if cognitive load effects are expressed as ‘shrinking of time’, context effects should not be modulated by load, but a main effect would be expected on the perceived duration of the speech signal. Results from two experiments indicate that increasing cognitive load (manipulated through a secondary visual search task) did not modulate temporal (Experiment 1) or spectral context effects (Experiment 2). However, a consistent main effect of cognitive load was found: increasing cognitive load during the precursor induced a perceptual increase in its perceived speech rate, biasing the perception of a following target word towards longer durations. This finding suggests that cognitive load effects in speech perception are implemented via ‘shrinking of time’, in line with a temporal sampling framework. In addition, we argue that our results align with a model in which early (spectral and temporal) normalization is unaffected by attention but later adjustments may be attention-dependent.","tags":["cognitive load","acoustic context","rate normalization","spectral normalization"],"title":"Cognitive load makes speech sound fast, but does not modulate acoustic context effects","type":"publication"},{"authors":["Hans Rutger Bosker","Eva Reinisch"],"categories":null,"content":" ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"00f81cb6138ee650e2c0f2326d0d4b2e","permalink":"https://hrbosker.github.io/publication/bosker-etal-2017-frontiers/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2017-frontiers/","section":"publication","summary":"Anecdotal evidence suggests that unfamiliar languages sound faster than one’s native language. Empirical evidence for this impression has, so far, come from explicit rate judgments. The aim of the present study was to test whether such perceived rate differences between native and foreign languages (FLs) have effects on implicit speech processing. Our measure of implicit rate perception was “normalization for speech rate”: an ambiguous vowel between short /a/ and long /a:/ is interpreted as /a:/ following a fast but as /a/ following a slow carrier sentence. That is, listeners did not judge speech rate itself; instead, they categorized ambiguous vowels whose perception was implicitly affected by the rate of the context. We asked whether a bias towards long /a:/ might be observed when the context is not actually faster but simply spoken in a FL. A fully symmetrical experimental design was used: Dutch and German participants listened to rate matched (fast and slow) sentences in both languages spoken by the same bilingual speaker. Sentences were followed by non-words that contained vowels from an /a-a:/ duration continuum. Results from Experiments 1 and 2 showed a consistent effect of rate normalization for both listener groups. Moreover, for German listeners, across the two experiments, foreign sentences triggered more /a:/ responses than (rate matched) native sentences, suggesting that foreign sentences were indeed perceived as faster. Moreover, this FL effect was modulated by participants’ ability to understand the FL. Those participants that scored higher on a FL translation task showed less of a FL effect. However, opposite effects were found for the Dutch listeners. For them, their native rather than the FL induced more /a:/ responses. Nevertheless, this reversed effect could be reduced when additional spectral properties of the context were controlled for. Experiment 3, using explicit rate judgments, replicated the effect for German but not Dutch listeners. We therefore conclude that the subjective impression that FLs sound fast may have an effect on implicit speech processing, with implications for how language learners perceive spoken segments in a FL.","tags":["speech rate","speech segmentation","rate normalization","second language acquisition","L2 speech","perception","‘Gabbling Foreigner Illusion’"],"title":"Foreign languages sound fast: evidence from implicit rate normalization","type":"publication"},{"authors":["Hans Rutger Bosker"],"categories":null,"content":" ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"8749e7e636e8805f6986e895daf6bc51","permalink":"https://hrbosker.github.io/publication/bosker-2017-jeplmc/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-2017-jeplmc/","section":"publication","summary":"In conversation, our own speech and that of others follow each other in rapid succession. Effects of the surrounding context on speech perception are well documented but, despite the ubiquity of the sound of our own voice, it is unknown whether our own speech also influences our perception of other talkers. This study investigated context effects induced by our own speech through 6 experiments, specifically targeting rate normalization (i.e., perceiving phonetic segments relative to surrounding speech rate). Experiment 1 revealed that hearing prerecorded fast or slow context sentences altered the perception of ambiguous vowels, replicating earlier work. Experiment 2 demonstrated that talking at a fast or slow rateprior to target presentation also altered target perception, though the effect of preceding speech rate was reduced. Experiment 3 showed that silent talking (i.e., inner speech) at fast or slow rates did not modulatethe perception of others, suggesting that the effect of self-produced speech rate in Experiment 2 arose through monitoring of the external speech signal. Experiment 4 demonstrated that, when participants were played back their own (fast/slow) speech, no reduction of the effect of preceding speech rate was observed, suggesting that the additional task of speech production may be responsible for the reduced effect in Experiment 2. Finally, Experiments 5 and 6 replicate Experiments 2 and 3 with new participant samples. Taken together, these results suggest that variation in speech production may induce variation in speech perception, thus carrying implications for our understanding of spoken communication in dialogue settings.","tags":["speech rate normalization","self-monitoring","covert speech","phonetic convergence","speaking induced suppression"],"title":"How our own speech rate influences our perception of others","type":"publication"},{"authors":["Hans Rutger Bosker"],"categories":null,"content":" ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"06ac0e50637b077c7b550ac8c8485cd9","permalink":"https://hrbosker.github.io/publication/bosker-2017-interspeech/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-2017-interspeech/","section":"publication","summary":"Speech is an acoustic signal with inherent amplitude modulations in the 1-9 Hz range. Recent models of speech perception propose that this rhythmic nature of speech is central to speech recognition. Moreover, rhythmic amplitude modulations have been shown to have beneficial effects on language processing and the subjective impression listeners have of the speaker. This study investigated the role of amplitude modulations in the political arena by comparing the speech produced by Hillary Clinton and Donald Trump in the three presidential debates of 2016. Inspection of the modulation spectra, revealing the spectral content of the two speakers’ amplitude envelopes after matching for overall intensity, showed considerably greater power in Clinton’s modulation spectra (compared to Trump’s) across the three debates, particularly in the 1-9 Hz range. The findings suggest that Clinton’s speech had a more pronounced temporal envelope with rhythmic amplitude modulations below 9 Hz, with a preference for modulations around 3 Hz. This may be taken as evidence for a more structured temporal organization of syllables in Clinton’s speech, potentially due to more frequent use of preplanned utterances. Outcomes are interpreted in light of the potential beneficial effects of a rhythmic temporal envelope on intelligibility and speaker perception.","tags":["amplitude envelope","amplitude modulations","speech rhythm","modulation spectrum"],"title":"The role of temporal amplitude modulations in the political arena: Hillary Clinton vs. Donald Trump","type":"publication"},{"authors":["Merel Maslowski","Antje S. Meyer","Hans Rutger Bosker"],"categories":null,"content":" ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"22af9b2ebe89dfcf6d7ef0bde3ee4f4c","permalink":"https://hrbosker.github.io/publication/maslowski-etal-2017-interspeech/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/maslowski-etal-2017-interspeech/","section":"publication","summary":"Speech rate is known to modulate perception of temporally ambiguous speech sounds. For instance, a vowel may be perceived as short when the immediate speech context is slow, but as long when the context is fast. Yet, effects of long-term tracking of speech rate are largely unexplored. Two experiments tested whether long-term tracking of rate influences perception of the temporal Dutch vowel contrast /A/-/a:/. In Experiment 1, one low-rate group listened to ‘neutral’ rate speech from talker A and to slow speech from talker B. Another high-rate group was exposed to the same neutral speech from A, but to fast speech from B. Between-group comparison of the ‘neutral’ trials revealed that the low-rate group reported a higher proportion of /a:/ in A’s ‘neutral’ speech, indicating that A sounded faster when B was slow. Experiment 2 tested whether one’s own speech rate also contributes to effects of long-term tracking of rate. Here, talker B’s speech was replaced by playback of participants’ own fast or slow speech. No evidence was found that one’s own voice affected perception of talker A in larger speech contexts. These results carry implications for our understanding of the mechanisms involved in rate-dependent speech perception and of dialogue.","tags":["speech rate","rate-dependent speech perception","rate normalization","global context effects","self-produced speech"],"title":"Whether long-term tracking of speech rate affects perception depends on who is talking","type":"publication"},{"authors":["Hans Rutger Bosker","Eva Reinisch","Matthias J. Sjerps"],"categories":null,"content":" ","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"424c11fa7b6d0b6f593af263db793167","permalink":"https://hrbosker.github.io/publication/bosker-etal-2016-spire/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2016-spire/","section":"publication","summary":"Listeners interpret local temporal cues (e.g., vowel durations) relative to the surrounding speech rate. For instance, an ambiguous Dutch vowel midway between short /ɑ/ and long /a:/ may be perceived as long /a:/ when presented in a fast context, but as short /ɑ/ in a slow context [1]. It is widely assumed that this process known as rate normalization is an early general auditory process [1, 2], and as such would operate independent from other higher level influences such as attention. However, when the perceptual system is taxed by the concurrent execution of another task, the encoding of the incoming speech signal is known to be negatively affected [3]. Therefore, listening to, for example, fast speech under cognitive load may result in impoverished encoding of the fast speech rate, reducing the effect that a fast context may have on the perception of subsequent speech (i.e., a reduction of the rate effect; cf. [4]). Alternatively, an increase in cognitive load has been shown to speed up time perception (the “shrinking of time”, [5]), potentially increasing the perceived rate of concurrent speech. This argument has, for instance, been used to explain why foreign-accented speech sounds faster than native speech [6]. Here we attempt to distinguish between these alternatives by testing Dutch /ɑ/-/a:/ categorization as a function of (1) the rate of the preceding carrier sentence and (2) the difficulty of a dual task (visual search) performed during carrier presentation.","tags":["rate normalization","spectral normalization","cognitive load","dual-tasking"],"title":"Listening under cognitive load makes speech sound fast","type":"publication"},{"authors":["Hans Rutger Bosker"],"categories":null,"content":" ","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"15e37b3ce74d6b207e79e6cb22e65d4c","permalink":"https://hrbosker.github.io/publication/bosker-2016-sp/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-2016-sp/","section":"publication","summary":"During conversation, spoken utterances occur in rich acoustic contexts, including speech produced by our interlocutor(s) and speech we produced ourselves. Prosodic characteristics of the acoustic context have been known to influence speech perception in a contrastive fashion: for instance, a vowel presented in a fast context is perceived to have a longer duration than the same vowel in a slow context. Given the ubiquity of the sound of our own voice, it may be that our own speech rate - a common source of acoustic context - also influences our perception of the speech of others. Two experiments were designed to test this hypothesis. Experiment 1 replicated earlier contextual rate effects by showing that hearing pre-recorded fast or slow context sentences alters the perception of ambiguous Dutch target words. Experiment 2 then extended this finding by showing that talking at a fast or slow rate prior to the presentation of the target words also altered the perception of those words. These results suggest that between-talker variation in speech rate production may induce between-talker variation in speech perception, thus potentially explaining why interlocutors tend to converge on speech rate in dialogue settings.","tags":["speech rate","rate normalization","self-monitoring","phonetic convergence"],"title":"Our own speech rate influences speech perception","type":"publication"},{"authors":["Hans Rutger Bosker","V. J. Tjiong","Hugo Quené","Ted Sanders","Nivja H. de Jong"],"categories":null,"content":" ","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"868b6fca440042fac1a08c511e23bcf1","permalink":"https://hrbosker.github.io/publication/bosker-etal-2015/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2015/","section":"publication","summary":"Disfluencies, such as uh and uhm, are known to help the listener in speech comprehension. For instance, disfluencies may elicit prediction of less accessible referents and may trigger listeners’ attention to the following word. However, recent work suggests differential processing of disfluencies in native and non-native speech. The current study investigated whether the beneficial effects of disfluencies on listeners’ attention are modulated by the (non-)native identity of the speaker. Using the Change Detection Paradigm, we investigated listeners’ recall accuracy for words presented in disfluent and fluent contexts, in native and non-native speech. We observed beneficial effects of both native and non-native disfluencies on listeners’ recall accuracy, suggesting that native and non-native disfluencies trigger listeners’ attention in a similar fashion.","tags":["disfluencies","attention","non-native speech","Change Detection paradigm"],"title":"Both native and non-native disfluencies trigger listeners’ attention","type":"publication"},{"authors":["Hans Rutger Bosker","Eva Reinisch"],"categories":null,"content":" ","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"ca90a1a2186a487023f22764791ca12a","permalink":"https://hrbosker.github.io/publication/bosker-reinisch-2015/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-reinisch-2015/","section":"publication","summary":"Speech perception involves a number of processes that deal with variation in the speech signal. One such process is normalization for speechrate: local temporal cues are perceived relative to the rate in the surrounding context. It is as yet unclear whether and how this perceptual effect interacts with higher level impressions of rate, such as a speaker’s nonnative identity. Nonnative speakers typically speak more slowly than natives, an experience that listeners take into account when explicitly judging the rate of nonnative speech. The present study investigatedwhether this is also reflected in implicit rate normalization. Results indicate that nonnative speech is implicitly perceived as faster than temporally-matched native speech, suggesting that the additional cognitive load of listening to an accent speeds up rate perception. Therefore, rate perception in speech is not dependent on syllable durations alone but also on the ease of processing of the temporal signal.","tags":["speech perception","speechrate","implicit processing","nonnative speech","cognitive load"],"title":"Normalization for speechrate in native and nonnative speech","type":"publication"},{"authors":["Hans Rutger Bosker","Hugo Quené","Ted Sanders","Nivja H. de Jong"],"categories":null,"content":" ","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"5745382d86a7db69495aae3569f1c2f6","permalink":"https://hrbosker.github.io/publication/bosker-etal-2014-jml/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2014-jml/","section":"publication","summary":"Speech comprehension involves extensive use of prediction. Linguistic prediction may be guided by the semantics or syntax, but also by the performance characteristics of the speech signal, such as disfluency. Previous studies have shown that listeners, when presented with the filler uh, exhibit a disfluency bias for discourse-new or unknown referents, drawing inferences about the source of the disfluency. The goal of the present study is to study the contrast between native and non-native disfluencies in speech comprehension. Experiment 1 presented listeners with pictures of high-frequency (e.g., a hand) and low frequency objects (e.g., a sewing machine) and with fluent and disfluent instructions. Listeners were found to anticipate reference to low-frequency objects when encountering disfluency, thus attributing disfluency to speaker trouble in lexical retrieval. Experiment 2  showed that, when participants listened to disfluent non-native speech, no anticipation of low-frequency referents was observed. We conclude that listeners can adapt their predictive strategies to the (non-native) speaker at hand, extending our understanding of the role of speaker identity in speech comprehension.","tags":["prediction","disfluency bias","disfluency","hesitation","non-native speech","speech comprehension"],"title":"Native ‘um’s elicit prediction of low-frequency referents, but non-native ‘um’s do not","type":"publication"},{"authors":["Anne-France Pinget","Hans Rutger Bosker","Hugo Quené","Nivja H. de Jong"],"categories":null,"content":" ","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"860b5116ff3a96ab1d5f234a309dab0e","permalink":"https://hrbosker.github.io/publication/pinget-etal-2014-lt/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/pinget-etal-2014-lt/","section":"publication","summary":"Oral fluency and foreign accent distinguish L2 from L1 speech production. In language testing practices, both fluency and accent are usually assessed by raters. This study investigates what exactly native raters of fluency and accent take into account when judging L2. Our aim is to explore the relationship between objectively measured temporal, segmental and suprasegmental properties of speech on the one hand, and fluency and accent as rated by native raters on the other hand. For 90 speech fragments from Turkish and English L2 learners of Dutch, several acoustic measures of fluency and accent were calculated. In Experiment 1, 20 native speakers of Dutch rated the L2 Dutch samples on fluency. In Experiment 2, 20 different untrained native speakers of Dutch judged the L2 Dutch samples on accentedness. Regression analyses revealed, first, that acousticmeasures of fluency were good predictors of fluency ratings. Second, segmental and suprasegmental measures of accent could predict some variance of accent ratings. Third, perceived fluency and perceived accent were only weakly related. In conclusion, this study shows that fluency and perceived foreign accent can be judged as separate constructs.","tags":["foreign accent","L2 specific fluency","native raters","perception of L2 speech","second language learners"],"title":"Native speakers’ perceptions of fluency and accent in L2 speech","type":"publication"},{"authors":["Katja Poellmann","Hans Rutger Bosker","James M. McQueen","Holger Mitterer"],"categories":null,"content":" ","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"6f2c18a699dffc4f95c19fc8a058167e","permalink":"https://hrbosker.github.io/publication/poellmann-etal-2014-jphon/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/poellmann-etal-2014-jphon/","section":"publication","summary":"This study investigates if and how listeners adapt to reductions in casual continuous speech. In a perceptual learning variant of the visual-world paradigm, two groups of Dutch participants were exposed to either segmental (/b/ → [ʋ]) or syllabic (ver- → [fː]) reductions in spoken Dutch sentences. In the test phase, both groups heard both kinds of reductions, but now applied to different words. In one of two experiments, the segmental reduction exposure group was better than the syllabic reduction exposure group in recognizing new reduced /b/-words.  In both experiments, the syllabic reduction group showed a greater target preference for new reduced ver-words. Learning about reductions was thus applied to previously unheard words. This lexical generalization suggests that mechanisms compensating for semental and syllabic reductions take place at a prelexical level, and hence that lexical access involves an abstractionist mode of processing. Existing abstractionist models need to be revised, however, as they do not include representations of sequences of segments (corresponding e.g. to ver-) at the prelexical level.","tags":["segmental reduction","syllabic reduction","adaptation","perceptual learning","reductions"],"title":"Perceptual adaptation to segmental and syllabic reductions in continuous spoken Dutch","type":"publication"},{"authors":["Hans Rutger Bosker","Hugo Quené","Ted Sanders","Nivja H. de Jong"],"categories":null,"content":" ","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"9f1e87355123fe8a62a02069a7185a56","permalink":"https://hrbosker.github.io/publication/bosker-etal-2014-ll/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2014-ll/","section":"publication","summary":"Where native speakers supposedly are fluent by default, nonnative speakers often have to strive hard to achieve a nativelike fluency level. However, disfluencies (such as pauses, fillers, repairs, etc.) occur in both native and nonnative speech and it is as yet unclear how fluency raters weigh the fluency characteristics of native and nonnative speech. Two rating experiments compared the way raters assess the fluency of native and nonnative speech. The fluency characteristics were controlled by using phonetic manipulations in pause (Experiment 1) and speed characteristics (Experiment 2). The results show that the ratings of manipulated native and nonnative speech were affected in a similar fashion. This suggests that there is no difference in the way listeners weigh the fluency characteristics of native and nonnative speakers.","tags":["fluency","disfluencies","hesitations","phonetic manipulations","nonnative speech"],"title":"The perception of fluency in native and nonnative speech","type":"publication"},{"authors":["Hans Rutger Bosker"],"categories":null,"content":" ","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"1b7308fe0760729e15bf8e89d926ad3e","permalink":"https://hrbosker.github.io/publication/bosker-2014-thesis/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-2014-thesis/","section":"publication","summary":" ","tags":null,"title":"The processing and evaluation of fluency in native and non-native speech","type":"publication"},{"authors":["Nivja H. de Jong","Hans Rutger Bosker"],"categories":null,"content":" ","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"6ad8b93a6b3ee427320da920d6033fba","permalink":"https://hrbosker.github.io/publication/dejong-bosker-2013/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/dejong-bosker-2013/","section":"publication","summary":"Second language (L2) research often involves analyses of acoustic measures of fluency. The studies investigating fluency, however, have been difficult to compare because the measures of fluency that were used differed widely. One of the differences between studies concerns the lower cut-off point for silent pauses, which has been set anywhere between 100 ms and 1000 ms. The goal of this paper is to find an optimal cut-off point. We calculate acoustic measures of fluency using different pause thresholds and then relate these measures to a measure of L2 proficiency and to ratings on fluency.","tags":["silent pauses","number of pauses","duration of pauses","silent pause threshold","second language speech"],"title":"Choosing a threshold for silent pauses to measure second language fluency","type":"publication"},{"authors":["Hans Rutger Bosker"],"categories":null,"content":" ","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"470925b5e95697e8be0a6841dfe55cc4","permalink":"https://hrbosker.github.io/publication/bosker-2013-ehll-juncture/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-2013-ehll-juncture/","section":"publication","summary":" ","tags":null,"title":"Juncture (prosodic)","type":"publication"},{"authors":["Hans Rutger Bosker"],"categories":null,"content":" ","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"0ee034120bdf164a6dde53c53d7c784a","permalink":"https://hrbosker.github.io/publication/bosker-2013-ehll-sibilants/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-2013-ehll-sibilants/","section":"publication","summary":" ","tags":null,"title":"Sibilant consonants","type":"publication"},{"authors":["Hans Rutger Bosker","Anne-France Pinget","Hugo Quené","Ted Sanders","Nivja H. de Jong"],"categories":null,"content":" ","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"a1709dfd10dc52b4fe5e5d6475ae7e43","permalink":"https://hrbosker.github.io/publication/bosker-etal-2013-lt/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2013-lt/","section":"publication","summary":"The oral fluency level of an L2 speaker is often used as a measure in assessing language proficiency. The present study reports on four experiments investigating the contributions of three fluency aspects (pauses, speed and repairs) to perceived fluency. In Experiment 1 untrained raters evaluated the oral fluency of L2 Dutch speakers. Using specific acoustic measures of pause, speed and repair phenomena, linear regression analyses revealed that pause and speed measures best predicted the subjective fluency ratings, and that repair measures contributed only very little. A second research question sought to account for these results by investigating perceptual sensitivity to acoustic pause, speed and repair phenomena, possibly accounting for the results from Experiment 1. In Experiments 2–4 three new groups of untrained raters rated the same L2 speech materials from Experiment 1 on the use of pauses, speed and repairs. A comparison of the results from perceptual sensitivity (Experiments 2–4) with fluency perception (Experiment 1) showed that perceptual sensitivity alone could not account for the contributions of the three aspects to perceived fluency. We conclude that listeners weigh the importance of the perceived aspects of fluency to come to an overall judgment.","tags":["perceived fluency","utterance fluency","fluency ratings","language proficiency"],"title":"What makes speech sound fluent? The contributions of pauses, speed and repairs","type":"publication"},{"authors":["Hans Rutger Bosker","Jeroen Briaire","Willemijn Heeren","Vincent J. van Heuven","Suzanne R. Jongman"],"categories":null,"content":" ","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"e169c4bcaf6d42512f725a50fca289a1","permalink":"https://hrbosker.github.io/publication/bosker-etal-2010/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bosker-etal-2010/","section":"publication","summary":" ","tags":null,"title":"Whispered speech as input for cochlear implants","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://hrbosker.github.io/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9f5fb74c757270807d9c8ecf5259914e","permalink":"https://hrbosker.github.io/empty/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/empty/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"google-site-verification: googled886c90778b4364a.html","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4a515080cbe86164dd2dc1518bddacac","permalink":"https://hrbosker.github.io/googled886c90778b4364a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/googled886c90778b4364a/","section":"","summary":"google-site-verification: googled886c90778b4364a.","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://hrbosker.github.io/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f1d044c0738ab9f19347f15c290a71a1","permalink":"https://hrbosker.github.io/research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/research/","section":"","summary":"","tags":null,"title":"Research","type":"widget_page"}]