<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SPEAC | Hans Rutger Bosker</title>
    <link>https://hrbosker.github.io/</link>
      <atom:link href="https://hrbosker.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>SPEAC | Hans Rutger Bosker</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sun, 01 Jan 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://hrbosker.github.io/media/logo_hu4ee931c5040e3d0fbcac8149a377b250_283935_300x300_fit_lanczos_3.png</url>
      <title>SPEAC | Hans Rutger Bosker</title>
      <link>https://hrbosker.github.io/</link>
    </image>
    
    <item>
      <title>Manual McGurk effect</title>
      <link>https://hrbosker.github.io/demos/manual-mcgurk/</link>
      <pubDate>Tue, 12 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/demos/manual-mcgurk/</guid>
      <description>&lt;h2 id=&#34;do-you-hear-voornaam-or-voornaam&#34;&gt;Do you hear &lt;em&gt;VOORnaam&lt;/em&gt; or &lt;em&gt;voorNAAM&lt;/em&gt;?&lt;/h2&gt;
&lt;p&gt;In the video below, you&amp;rsquo;ll see a (randomly selected&amp;hellip;) talker say a Dutch word. Is he saying &lt;em&gt;VOORnaam&lt;/em&gt; (Eng. &amp;ldquo;first name&amp;rdquo;, with stress on the first syllable &lt;em&gt;VOOR-&lt;/em&gt;) or &lt;em&gt;voorNAAM&lt;/em&gt; (Eng. &amp;ldquo;respectable&amp;rdquo;, with stress on the second syllable &lt;em&gt;-NAAM&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;In other words: &lt;strong&gt;where do you hear the stress?&lt;/strong&gt;&lt;/p&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/manual-mcgurk/voornaam_AV_swbeat_sw_4.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;blockquote&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;&lt;em&gt;Spoiler: click here to reveal the video specs&lt;/em&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Audio:&lt;/strong&gt; &lt;em&gt;ambiguous&lt;/em&gt;; midway between &lt;em&gt;VOORnaam&lt;/em&gt; &amp;ndash; &lt;em&gt;voorNAAM&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lips:&lt;/strong&gt; head taken from a recording of &lt;em&gt;VOORnaam&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hands:&lt;/strong&gt; beat gesture aligned to the &lt;strong&gt;first&lt;/strong&gt; syllable&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now play the video below. &lt;strong&gt;Where do you hear the stress now?&lt;/strong&gt;&lt;/p&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/manual-mcgurk/voornaam_AV_wsbeat_sw_4.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;blockquote&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;&lt;em&gt;Spoiler: click here to reveal the video specs&lt;/em&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Audio:&lt;/strong&gt; &lt;em&gt;ambiguous&lt;/em&gt;; midway between &lt;em&gt;VOORnaam&lt;/em&gt; &amp;ndash; &lt;em&gt;voorNAAM&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lips:&lt;/strong&gt; head taken from a recording of &lt;em&gt;VOORnaam&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hands:&lt;/strong&gt; beat gesture aligned to the &lt;strong&gt;second&lt;/strong&gt; syllable&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;explanation&#34;&gt;Explanation&lt;/h2&gt;
&lt;p&gt;The audio in these videos is perfectly identical: it has been manipulated to be ambiguous, falling roughly midway between &lt;em&gt;VOORnaam&lt;/em&gt; and &lt;em&gt;voorNAAM&lt;/em&gt;. The head of the talker is also the same: it has been copy-pasted from a video recording of the talker saying &lt;em&gt;VOORnaam&lt;/em&gt;. &lt;strong&gt;The only difference between these two videos is the timing of the hand gesture.&lt;/strong&gt; In the first clip, the talker produces a beat gesture on the &lt;em&gt;first&lt;/em&gt; syllable, while in the second video the talker gestures on the &lt;em&gt;second&lt;/em&gt; syllable. Our experiments show that this slight change in timing has major consequences for perception. When we ask a group of Dutch participants to indicate what word they hear the talker say, the majority reports hearing &lt;em&gt;VOORnaam&lt;/em&gt; in the first clip, but &lt;em&gt;voorNAAM&lt;/em&gt; in the second clip.&lt;/p&gt;
&lt;blockquote&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-4&#34;&gt;
  &lt;summary&gt;&lt;em&gt;Really? Convince me&amp;hellip;&lt;/em&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;p&gt;This is Figure 1 from &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2021-procroysocb&#34;&gt;Bosker &amp;amp; Peeters (2021)&lt;/a&gt;. In the bottom left panel, you see the proportion of &amp;lsquo;I hear stress on the first syllable&amp;rsquo; responses for when the beat gesture falls on the first syllable (blue line) or on the second syllable (red line). The blue line lies above the red line, indicating an overall bias to report more &amp;lsquo;stress on first syllable&amp;rsquo; responses when the gesture falls on the first vs. second syllable. The difference between the lines is sizable, averaging around 20%.&lt;/p&gt;
&lt;figure  id=&#34;figure-figure-1-in-bosker--peeters-2021&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://royalsocietypublishing.org/cms/asset/53082c59-4ac5-43d8-a411-9f5f5edda544/rspb20202419f01.jpg&#34; alt=&#34;Figure 1 in Bosker &amp;amp; Peeters (2021)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;800&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Figure 1 in Bosker &amp;amp; Peeters (2021)
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;how-hands-help-us-hear&#34;&gt;How hands help us hear&lt;/h2&gt;
&lt;p&gt;When we have a face-to-face conversation, we don&amp;rsquo;t only exchange sounds. We also move our head, hands, and body to the rhythm of the speech. &lt;em&gt;Beat gestures&lt;/em&gt; are relatively &amp;lsquo;simple&amp;rsquo; up-and-down hand gestures that are closely aligned to the rhythm of speech. They tend to fall on the stressed syllable in free-stress languages, such as English and Dutch. These videos demonstrate that people are sensitive to the timing of beat gestures, influencing lexical stress perception. In &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2021-procroysocb&#34;&gt;Bosker &amp;amp; Peeters (2021)&lt;/a&gt;, this effect was termed the &lt;strong&gt;manual McGurk effect&lt;/strong&gt;. That is, just like seeing a talker close their lips can make you hear the sound /b/ in the classic McGurk effect (&lt;a href=&#34;https://www.nature.com/articles/264746a0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;McGurk &amp;amp; McDonald, 1976&lt;/a&gt;), so can the timing of hand gestures influence speech perception in the manual McGurk effect.&lt;/p&gt;
&lt;h2 id=&#34;why-is-this-important&#34;&gt;Why is this important?&lt;/h2&gt;
&lt;p&gt;The manual McGurk effect is the first demonstration of how the timing of hand gestures influences low-level speech perception. Even the simplest flicks-of-the-hands that do not convey any particular meaning of themselves can shape what words you hear. This promises that these seemingly unimportant hand gestures contribute meaningfully to audiovisual speech comprehension. Perhaps &amp;rsquo;enriching&amp;rsquo; our speech with carefully timed gestures can help our audience understand our spoken message, particularly in challenging listening conditions, such as in noise.&lt;/p&gt;
&lt;h2 id=&#34;relevant-papers&#34;&gt;Relevant papers&lt;/h2&gt;
&lt;blockquote&gt;










  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/david-peeters/&#34;&gt;David Peeters&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2021).
  &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2021-procroysocb/&#34;&gt;Beat gestures influence which speech sounds you hear&lt;/a&gt;.
  &lt;em&gt;Proceedings of the Royal Society B: Biological Sciences, 288&lt;/em&gt;, 20202419, doi:10.1098/rspb.2020.2419.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3243428_3/component/file_3280864/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/bosker-etal-2021-procroysocb/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/b7kue/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1098/rspb.2020.2419&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;
&lt;blockquote&gt;










  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/ronny-bujok/&#34;&gt;Ronny Bujok&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/antje-s.-meyer/&#34;&gt;Antje S. Meyer&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2022).
  &lt;a href=&#34;https://hrbosker.github.io/publication/bujok-etal-2022-sp/&#34;&gt;Visible lexical stress cues on the face do not influence audiovisual speech perception&lt;/a&gt;.
  In &lt;em&gt;Proceedings of Speech Prosody 2022&lt;/em&gt; (ed. S. Frota, M. Cruz, and M. Vig√°rio), 259-263, doi:10.21437/SpeechProsody.2022-53.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3387404_1/component/file_3387405/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/bujok-etal-2022-sp/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/um7ph&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.21437/SpeechProsody.2022-53&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Ronny Bujok, Antje S. Meyer, and Hans Rutger Bosker (2022). Audiovisual perception of lexical stress: Beat gestures are stronger visual cues for lexical stress than visible articulatory cues on the face. &lt;em&gt;PsyArXiv Preprints&lt;/em&gt;, doi:&lt;a href=&#34;https://doi.org/10.31234/osf.io/y9jck&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.31234/osf.io/y9jck&lt;/a&gt;, data:&lt;a href=&#34;https://osf.io/4d9w5/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://osf.io/4d9w5/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>...record audio</title>
      <link>https://hrbosker.github.io/resources/how-to/record-audio/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/resources/how-to/record-audio/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    We&amp;rsquo;ll cover making clean audio recordings in &lt;strong&gt;Audacity&lt;/strong&gt; and in &lt;strong&gt;SpeechRecorder&lt;/strong&gt;. Audacity is easy-to-use and perfect for making a single (long) recording of a speaker. SpeechRecorder presents individual word/sentence prompts to a speaker, saving each utterance separately.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;audacity&#34;&gt;Audacity&lt;/h2&gt;
&lt;p&gt;Audacity is a free, open-source, and cross-platform audio software package. It is easy to use, very intuitive, and fast. It is particularly suited for making a single (long) recording of a speaker, after which you manually extract the relevant words/sentences from this raw recording.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download, install, and open Audacity:&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.audacityteam.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.audacityteam.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;First check the sampling frequency. Typically, 48 kHz or 44.1 kHz will do just fine.&lt;/li&gt;
&lt;/ul&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /resources/how-to/record-audio/audacity%20%285%29_hu2376441eede321f8a9da7e7d53920a1d_91532_fcfff5ccd21f291198de3c5bb7eb2094.webp 400w,
               /resources/how-to/record-audio/audacity%20%285%29_hu2376441eede321f8a9da7e7d53920a1d_91532_3ad6b5f589837d118005f71e66a94edd.webp 760w,
               /resources/how-to/record-audio/audacity%20%285%29_hu2376441eede321f8a9da7e7d53920a1d_91532_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://hrbosker.github.io/resources/how-to/record-audio/audacity%20%285%29_hu2376441eede321f8a9da7e7d53920a1d_91532_fcfff5ccd21f291198de3c5bb7eb2094.webp&#34;
               width=&#34;600&#34;
               height=&#34;404&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;Audacity&amp;rsquo;s default is to make &lt;em&gt;stereo&lt;/em&gt; recordings, with each recording containing two channels:&lt;/li&gt;
&lt;/ul&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /resources/how-to/record-audio/audacity%20%282%29_hu7be07d5036e9b7da3a74763866b1da0f_108602_c15791337c4530288e9b26dcc221855b.webp 400w,
               /resources/how-to/record-audio/audacity%20%282%29_hu7be07d5036e9b7da3a74763866b1da0f_108602_101079cc1c9588fccf33605617292340.webp 760w,
               /resources/how-to/record-audio/audacity%20%282%29_hu7be07d5036e9b7da3a74763866b1da0f_108602_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://hrbosker.github.io/resources/how-to/record-audio/audacity%20%282%29_hu7be07d5036e9b7da3a74763866b1da0f_108602_c15791337c4530288e9b26dcc221855b.webp&#34;
               width=&#34;600&#34;
               height=&#34;404&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;In most cases, it&amp;rsquo;s better to make &lt;em&gt;mono&lt;/em&gt; recordings (with only one channel) to make annotating and manipulating the speech a lot easier. Change the settings at the top to MONO:&lt;/li&gt;
&lt;/ul&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /resources/how-to/record-audio/audacity%20%283%29_hu2376441eede321f8a9da7e7d53920a1d_86535_45cf3f591e7a0596c0971f6ee5819e49.webp 400w,
               /resources/how-to/record-audio/audacity%20%283%29_hu2376441eede321f8a9da7e7d53920a1d_86535_2e6d7ff407a4e6ada78768b2c74fe77f.webp 760w,
               /resources/how-to/record-audio/audacity%20%283%29_hu2376441eede321f8a9da7e7d53920a1d_86535_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://hrbosker.github.io/resources/how-to/record-audio/audacity%20%283%29_hu2376441eede321f8a9da7e7d53920a1d_86535_45cf3f591e7a0596c0971f6ee5819e49.webp&#34;
               width=&#34;600&#34;
               height=&#34;404&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;Then click the red button to start a recording&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /resources/how-to/record-audio/audacity-%281%29_hu4b45ac301e71be0d2144ce8adc0b578d_82319_cc3d39f0447e3a7c95a9ba21a3bab9e9.webp 400w,
               /resources/how-to/record-audio/audacity-%281%29_hu4b45ac301e71be0d2144ce8adc0b578d_82319_623908e7d9c6dc88c31e511ed137b576.webp 760w,
               /resources/how-to/record-audio/audacity-%281%29_hu4b45ac301e71be0d2144ce8adc0b578d_82319_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://hrbosker.github.io/resources/how-to/record-audio/audacity-%281%29_hu4b45ac301e71be0d2144ce8adc0b578d_82319_cc3d39f0447e3a7c95a9ba21a3bab9e9.webp&#34;
               width=&#34;600&#34;
               height=&#34;404&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;&amp;hellip;which should look something like this:&lt;/li&gt;
&lt;/ul&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /resources/how-to/record-audio/audacity%20%288%29_hu4b4dab99231e2d53a28bf9b9c2ccdad6_170078_91355917154f899b9e63f27b458a3e6d.webp 400w,
               /resources/how-to/record-audio/audacity%20%288%29_hu4b4dab99231e2d53a28bf9b9c2ccdad6_170078_687d0e6e3ddb5e255870cfd935a13ec7.webp 760w,
               /resources/how-to/record-audio/audacity%20%288%29_hu4b4dab99231e2d53a28bf9b9c2ccdad6_170078_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://hrbosker.github.io/resources/how-to/record-audio/audacity%20%288%29_hu4b4dab99231e2d53a28bf9b9c2ccdad6_170078_91355917154f899b9e63f27b458a3e6d.webp&#34;
               width=&#34;600&#34;
               height=&#34;404&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;When you&amp;rsquo;re done recording, click the yellow square to stop.&lt;/li&gt;
&lt;/ul&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /resources/how-to/record-audio/audacity%20%286%29_hu5a8d2d1acc1a5d49f87cae738a910dc5_107877_41b57c8c8d948f1104ab9c5ae4b18710.webp 400w,
               /resources/how-to/record-audio/audacity%20%286%29_hu5a8d2d1acc1a5d49f87cae738a910dc5_107877_fddef4143c77084bfe82d4e714971019.webp 760w,
               /resources/how-to/record-audio/audacity%20%286%29_hu5a8d2d1acc1a5d49f87cae738a910dc5_107877_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://hrbosker.github.io/resources/how-to/record-audio/audacity%20%286%29_hu5a8d2d1acc1a5d49f87cae738a910dc5_107877_41b57c8c8d948f1104ab9c5ae4b18710.webp&#34;
               width=&#34;600&#34;
               height=&#34;405&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Do not SAVE projects&lt;/strong&gt; in Audacity, but instead &lt;strong&gt;EXPORT audio files&lt;/strong&gt;. Go to: &lt;em&gt;File &amp;gt; Export Audio&amp;hellip;&lt;/em&gt;, and then click the &amp;lsquo;Save as type&amp;rsquo; drop-down menu to select the file format. Select .wav for uncompressed (raw) audio which is best suited for speech manipulations, or .mp3 for compressed (processed) audio.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Audacity is also great for quickly converting a batch of .wav files to .mp3. Open Audacity, drag a selection of files from a folder into Audacity, and go to &lt;em&gt;File &amp;gt; Export Multiple&lt;/em&gt;. In the pop-up window, select &lt;em&gt;MP3 Files&lt;/em&gt; as &lt;code&gt;Export format&lt;/code&gt;, choose an &lt;code&gt;Export location&lt;/code&gt;, and keep the other defaults (in particular, &lt;code&gt;Split files based on:&lt;/code&gt; &lt;em&gt;Tracks&lt;/em&gt;; &lt;code&gt;Name files:&lt;/code&gt; &lt;em&gt;Using Label/Track Name&lt;/em&gt;). Click &lt;code&gt;Export&lt;/code&gt; and voila, you now have an .mp3 file for each .wav file.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;tips-n-tricks&#34;&gt;Tips n tricks&lt;/h3&gt;
&lt;p&gt;But how to make recordings that are &amp;lsquo;clean&amp;rsquo;: with little noise and of good quality?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;For longer recordings, ask your speaker to sit in a relaxed position that they can keep up for a longer period of time. People typically tend to slouch during a recording, changing the distance between them and the table mic, thus changing the intensity of speech appearing early vs. later in your recording. If you ask your speaker to try to &amp;lsquo;already slouch a little&amp;rsquo;, this can help avoid large variability in intensity.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Before recording your items, start a dummy recording and ask the speaker to count to 100. Then adjust the volume/sensitivity/gain of the microphone. You want the average intensity of the speech to fall roughly in between 0.5 and -0.5 in Audacity. So &lt;strong&gt;this is good&lt;/strong&gt;:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /resources/how-to/record-audio/audacity%20%288%29_hu4b4dab99231e2d53a28bf9b9c2ccdad6_170078_91355917154f899b9e63f27b458a3e6d.webp 400w,
               /resources/how-to/record-audio/audacity%20%288%29_hu4b4dab99231e2d53a28bf9b9c2ccdad6_170078_687d0e6e3ddb5e255870cfd935a13ec7.webp 760w,
               /resources/how-to/record-audio/audacity%20%288%29_hu4b4dab99231e2d53a28bf9b9c2ccdad6_170078_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://hrbosker.github.io/resources/how-to/record-audio/audacity%20%288%29_hu4b4dab99231e2d53a28bf9b9c2ccdad6_170078_91355917154f899b9e63f27b458a3e6d.webp&#34;
               width=&#34;600&#34;
               height=&#34;404&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;This is &lt;strong&gt;risky&lt;/strong&gt; because it&amp;rsquo;s approaching 1 and -1:&lt;/li&gt;
&lt;/ul&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /resources/how-to/record-audio/audacity%20%284%29_hu2376441eede321f8a9da7e7d53920a1d_100282_98959f412f17659ca2270b433e362987.webp 400w,
               /resources/how-to/record-audio/audacity%20%284%29_hu2376441eede321f8a9da7e7d53920a1d_100282_43e0b7a1bda07f0f2168f38b12caab4d.webp 760w,
               /resources/how-to/record-audio/audacity%20%284%29_hu2376441eede321f8a9da7e7d53920a1d_100282_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://hrbosker.github.io/resources/how-to/record-audio/audacity%20%284%29_hu2376441eede321f8a9da7e7d53920a1d_100282_98959f412f17659ca2270b433e362987.webp&#34;
               width=&#34;600&#34;
               height=&#34;404&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;This is &lt;strong&gt;bad&lt;/strong&gt; because it&amp;rsquo;s outside 1 and -1.&lt;/li&gt;
&lt;/ul&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /resources/how-to/record-audio/audacity%20%287%29_hu46cec7b9763626ae177ec4efbcbad4ab_167475_fa51829f8d628713ef0f015d3061c84b.webp 400w,
               /resources/how-to/record-audio/audacity%20%287%29_hu46cec7b9763626ae177ec4efbcbad4ab_167475_6ce8cd8fb0a845cfdc5141be27f94f8c.webp 760w,
               /resources/how-to/record-audio/audacity%20%287%29_hu46cec7b9763626ae177ec4efbcbad4ab_167475_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://hrbosker.github.io/resources/how-to/record-audio/audacity%20%287%29_hu46cec7b9763626ae177ec4efbcbad4ab_167475_fa51829f8d628713ef0f015d3061c84b.webp&#34;
               width=&#34;600&#34;
               height=&#34;404&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;This is called &amp;lsquo;clippings&amp;rsquo; and it&amp;rsquo;s very well audible in the signal. These two audio clips contain the same telephone number, but the first is clean and the second has clippings:&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;CLEAN






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/resources/how-to/record-audio/clean.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;WITH CLIPPINGS






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/resources/how-to/record-audio/clippings.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;When boosting the volume/sensitivity/gain of the microphone, be aware that &amp;ndash; by doing so &amp;ndash; you may actually be boosting the background noise too. It&amp;rsquo;s often preferable to move the location of the mic closer to the speaker compared to amplifying the input signal.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When you need the mic to be particularly close to the speaker (e.g., when using a head-mounted mic), make sure &lt;strong&gt;the mic is not too close to the speakers lips&lt;/strong&gt; as that will introduce puffs of loud noise for many stop consonants. Better to aim for the chin!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Why not ask the speaker to go through your list of items from top to bottom first, and then repeat all items from bottom to top? This will give you two recordings of each item, allowing you to select the best one. Moreover, items at the top and bottom of lists typically sound different from the rest. First items typically have loud intensity and raised pitch, while last items have low intensity and pitch. By switching the order of items, you may be able to circumvent some of these order effects.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;chopping-up-the-recording&#34;&gt;Chopping up the recording&lt;/h3&gt;
&lt;p&gt;When you&amp;rsquo;ve finished recording with Audacity, and have extracted the audio as a .wav file, you will then need to isolate the individual items (e.g., words or sentences) from that one long recording. There&amp;rsquo;s different solutions for this task:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Do it by hand.&lt;/strong&gt; Read the .wav file into Praat, create an empty TextGrid, and insert boundaries at the onset and offset of every item. Once you&amp;rsquo;re done, you select the Sound and TextGrid objects together in Praat, and select an option from the Extract menu, such as &lt;code&gt;Extract all intervals...&lt;/code&gt; or &lt;code&gt;Extract non-empty intervals...&lt;/code&gt; and save the individual items.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Go full automatic.&lt;/strong&gt; You can use a forced-aligner to find the words inside your recording. We tend to use &lt;a href=&#34;https://hrbosker.github.io/resources/other-resources/#videoaudio-editing&#34;&gt;WebMAUS&lt;/a&gt; (for word- and phoneme-level annotations) or &lt;a href=&#34;https://hrbosker.github.io/resources/other-resources/#videoaudio-editing&#34;&gt;EasyAlign&lt;/a&gt; (for syllable-level annotations). In both cases, you provide the forced-aligner with the .wav file and a .txt file that contains the orthographic content of the recording (i.e., the prompts that your speaker was asked to read out). The forced-aligner will then try to find the words from the .txt file in the .wav file, providing a .TextGrid file as output.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;However&lt;/em&gt;, often your speaker wasn&amp;rsquo;t perfect, fluffing their lines, repeating items several times before getting it right, or chucking in a few coughs and &amp;lsquo;uhm&amp;rsquo;s here and there. In that case, automatic forced-alignment is suboptimal. In such (commonly occurring) cases, you could consider:&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;Go semi-automatic&lt;/strong&gt;. Read your .wav file in Praat and click &lt;em&gt;Annotate &amp;gt; To TextGrid (silences)&amp;hellip;&lt;/em&gt;. Praat will then create a TextGrid for you, labeling silent intervals as &amp;lsquo;silence&amp;rsquo; and intervals with speech as &amp;lsquo;sounding&amp;rsquo;. You can then go through this TextGrid manually and&amp;hellip;
&lt;ol&gt;
&lt;li&gt;&amp;hellip;remove incorrectly detected &amp;lsquo;sounding&amp;rsquo; intervals&lt;/li&gt;
&lt;li&gt;&amp;hellip;adjust the boundaries of individual intervals&lt;/li&gt;
&lt;li&gt;&amp;hellip;label the intervals correctly&lt;/li&gt;
&lt;li&gt;&amp;hellip;extract the intervals for saving and further analysis/manipulation.&lt;br&gt;
(&lt;em&gt;actions (3) and (4) can be performed automatically using scripts in Praat&lt;/em&gt;).&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /resources/how-to/record-audio/praat%20textgrid%20to%20silences_huaf5bafd9495b95bf7399b1b8ff70f2e4_241763_8d787d42ba7df2968908582761cc1228.webp 400w,
               /resources/how-to/record-audio/praat%20textgrid%20to%20silences_huaf5bafd9495b95bf7399b1b8ff70f2e4_241763_a7f3916459fecd13582f4b7d6dfe0ef0.webp 760w,
               /resources/how-to/record-audio/praat%20textgrid%20to%20silences_huaf5bafd9495b95bf7399b1b8ff70f2e4_241763_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://hrbosker.github.io/resources/how-to/record-audio/praat%20textgrid%20to%20silences_huaf5bafd9495b95bf7399b1b8ff70f2e4_241763_8d787d42ba7df2968908582761cc1228.webp&#34;
               width=&#34;600&#34;
               height=&#34;404&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt;I&amp;rsquo;ve found that the default settings for the &lt;code&gt;To TextGrid (silences)&lt;/code&gt; function are best adjusted to: &lt;code&gt;Minimum pitch (Hz) = 50&lt;/code&gt; (instead of the default 100) and &lt;code&gt;Silence threshold (dB) = -40&lt;/code&gt; (instead of the default -32). In my experience, this works considerably better especially for single-word recordings, but often also for full-sentence recordings.&lt;/p&gt;
&lt;p&gt;Oh, and as I&amp;rsquo;m writing this, I see the latest versions of Praat have a new &lt;code&gt;To TextGrid (voice activity)&lt;/code&gt; function. My initial impression is that the default settings for this function perform comparable to the &amp;lsquo;adjusted&amp;rsquo; settings for &lt;code&gt;To TextGrid (silences)&lt;/code&gt; suggested above.&lt;/p&gt;
&lt;h2 id=&#34;speechrecorder&#34;&gt;SpeechRecorder&lt;/h2&gt;
&lt;p&gt;SpeechRecorder is also a free, open-source, and cross-platform audio software package. It takes a little more preparation to use it appropriately compared to Audacity, but an important advantage is that it allows users to &lt;strong&gt;efficiently record multiple individual items&lt;/strong&gt; (instead of one massively long recording). This means that the &amp;lsquo;chopping up recordings into items&amp;rsquo; part of the workflow can be (largely) skipped&amp;hellip; &lt;em&gt;but see &lt;a href=&#34;#trim-silence&#34;&gt;this advice&lt;/a&gt; below&lt;/em&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download, install, and open SpeechRecorder:&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.bas.uni-muenchen.de/Bas/software/speechrecorder/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.bas.uni-muenchen.de/Bas/software/speechrecorder/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /resources/how-to/record-audio/speechrecorder_1_hub1cc186d544025fb70cb3e12bd4d70a4_137608_1bca9ca60a3d625d8dc0d878c25a9a57.webp 400w,
               /resources/how-to/record-audio/speechrecorder_1_hub1cc186d544025fb70cb3e12bd4d70a4_137608_b4c17a9c98e2a20104ced09e325f85cb.webp 760w,
               /resources/how-to/record-audio/speechrecorder_1_hub1cc186d544025fb70cb3e12bd4d70a4_137608_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://hrbosker.github.io/resources/how-to/record-audio/speechrecorder_1_hub1cc186d544025fb70cb3e12bd4d70a4_137608_1bca9ca60a3d625d8dc0d878c25a9a57.webp&#34;
               width=&#34;600&#34;
               height=&#34;404&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;Go to &lt;em&gt;Project &amp;gt; New&lt;/em&gt;, and enter a name for your project&lt;/li&gt;
&lt;li&gt;The directory for the audio output is &lt;code&gt;C:/Users/hanbos/speechrecorder/[myProjectName]/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;In the &amp;lsquo;Speaker data&amp;rsquo; window that pops up, add a speaker by clicking &lt;em&gt;Add&lt;/em&gt; and then &lt;em&gt;Select&lt;/em&gt;.
&lt;ul&gt;
&lt;li&gt;If you have multiple talkers, you can add several speakers, and by selecting one of them, you add a speaker identifier to the filename of each audio output file.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Go to &lt;em&gt;Project &amp;gt; Preferences&lt;/em&gt; and find the &lt;em&gt;Recording&lt;/em&gt; tab to change the &lt;em&gt;Format&lt;/em&gt; settings, such as the sampling frequency (e.g., 44.1 or 48 kHz) and the number of channels (usually &lt;em&gt;mono&lt;/em&gt; so 1 channel).&lt;/li&gt;
&lt;li&gt;Then go to &lt;em&gt;Script &amp;gt; Import text table&amp;hellip;&lt;/em&gt; and select a headerless tab-separated file with 2 columns:
&lt;ol&gt;
&lt;li&gt;filenames without extension (e.g., &lt;code&gt;item42_cacao_isolated&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;prompts (e.g., &lt;code&gt;cacao&lt;/code&gt;)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    You can easily create a .txt file like this in Excel. In Excel, enter filenames in one column and prompts in another column, select all cells from the two columns, copy them, and paste them into Notepad. If you save this as a .txt file, you will have a headerless tab-separated 2-column file.
  &lt;/div&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;SpeechRecorder will then look something like this:&lt;/li&gt;
&lt;/ul&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /resources/how-to/record-audio/speechrecorder_2_hu0d357699356910442a098a9c0c2bf723_325046_187da9bf15b0ba202cd908ddf8e7e76a.webp 400w,
               /resources/how-to/record-audio/speechrecorder_2_hu0d357699356910442a098a9c0c2bf723_325046_f7788fcf60be0acbb6309b03e7971671.webp 760w,
               /resources/how-to/record-audio/speechrecorder_2_hu0d357699356910442a098a9c0c2bf723_325046_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://hrbosker.github.io/resources/how-to/record-audio/speechrecorder_2_hu0d357699356910442a098a9c0c2bf723_325046_187da9bf15b0ba202cd908ddf8e7e76a.webp&#34;
               width=&#34;600&#34;
               height=&#34;404&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;Click &lt;em&gt;Record&lt;/em&gt; at the bottom to start a recording. The traffic light will go from Red, to Orange, to Green, after which the speaker pronounces the prompt on screen. Click &lt;em&gt;Stop&lt;/em&gt; to stop the recording.&lt;/li&gt;
&lt;li&gt;If you are happy with the recording, click &lt;strong&gt;[&amp;raquo;]&lt;/strong&gt; to proceed to the next prompt. SpeechRecorder will then save the recording with the appropriate filename in the project folder and present the next prompt.&lt;/li&gt;
&lt;li&gt;However, if you are unhappy with the recording, click &lt;em&gt;Record&lt;/em&gt; again to overwrite the previous take.&lt;/li&gt;
&lt;li&gt;See these &lt;a href=&#34;#tips-n-tricks&#34;&gt;tips n tricks&lt;/a&gt; about how to control audio quality.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;trim-silence&#34;&gt;Trim silence&lt;/h3&gt;
&lt;p&gt;Even though SpeechRecorder gives you separate recordings for individual items, it does often produce recordings with plenty of leading and trailing silence (i.e., before and after the critical word/sentence). There are several options to trim these silent intervals:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Change SpeechRecorder settings&lt;/strong&gt;. Go to &lt;em&gt;Project &amp;gt; Preferences&lt;/em&gt;, click the &lt;em&gt;Recording&lt;/em&gt; tab and then the &lt;em&gt;Options&lt;/em&gt; tab. Reduce the &lt;code&gt;default prerecording delay&lt;/code&gt; (default is 1000 ms) and the &lt;code&gt;default postrecording delay&lt;/code&gt; (default is 500 ms) to lower values. &lt;strong&gt;NOTE:&lt;/strong&gt; this will also affect the traffic light behavior (i.e., recordings may start instantly).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Use forced alignment&lt;/strong&gt;. Because SpeechRecorder allows the user to re-record items that were fluffed, it typically produces recordings that are easily forced-aligned. We tend to use &lt;a href=&#34;https://hrbosker.github.io/resources/other-resources/#videoaudio-editing&#34;&gt;WebMAUS&lt;/a&gt; (for word- and phoneme-level annotations) or &lt;a href=&#34;https://hrbosker.github.io/resources/other-resources/#videoaudio-editing&#34;&gt;EasyAlign&lt;/a&gt; (for syllable-level annotations). In both cases, you provide the forced-aligner with the .wav file and a .txt file that contains the orthographic content of the recording (i.e., the prompts that your speaker was asked to read out). The forced-aligner will then try to find the words from the .txt file in the .wav file, providing a .TextGrid file as output. Finally, you can use this TextGrid to extract only the intervals of interest, removing leading and trailing silences.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Happy recording!&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Save all</title>
      <link>https://hrbosker.github.io/resources/scripts/save-all/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/resources/scripts/save-all/</guid>
      <description>&lt;p&gt;Praat can only save one object at a time for you. If you have multiple objects in your object window you&amp;rsquo;d like to save in one go, you can use this script. It can either save objects by their object name or by their id number.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You can also &lt;a href=&#34;../save-all.praat&#34;&gt;download the script&lt;/a&gt; as a .praat file.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;################################################################################
### Hans Rutger Bosker, Radboud University
### HansRutger.Bosker@ru.nl
### Date: 23 June 2022, run in Praat 6.2.12 on Windows 10
### License: CC BY-NC 4.0
################################################################################

	###&amp;gt;&amp;gt; This script saves all selected objects to the directory &amp;#39;dir_out$&amp;#39;
	###&amp;gt;&amp;gt;	with either:
	###&amp;gt;&amp;gt;   - their object name (e.g., &amp;#34;sentence1.wav&amp;#34;)
	###&amp;gt;&amp;gt;			&amp;gt; set variable &amp;#39;save_method$&amp;#39; to &amp;#34;name&amp;#34; [default]
	###&amp;gt;&amp;gt;   - their id number in the Praat object window (e.g., &amp;#34;42.wav&amp;#34;)
	###&amp;gt;&amp;gt;			&amp;gt; set variable &amp;#39;save_method$&amp;#39; to &amp;#34;id&amp;#34;
	###&amp;gt;&amp;gt;
	###&amp;gt;&amp;gt; Sounds are saved as .wav files,
	###&amp;gt;&amp;gt; other object types (TextGrids, Spectrum, etc.) are saved
	###&amp;gt;&amp;gt; with their own extension type (.TextGrid, .Spectrum).
	###&amp;gt;&amp;gt;
	###&amp;gt;&amp;gt; Default: the script will overwrite pre-existing files.
	###&amp;gt;&amp;gt; Set variable &amp;#39;overwrite$&amp;#39; to &amp;#34;no&amp;#34; if you want Praat
	###&amp;gt;&amp;gt; to throw an error instead.



################################################################################
### Variables you will definitely need to customize:
################################################################################

### Where should the selected objects be saved?

dir_out$ = &amp;#34;C:\Users\hanbos\mysounds&amp;#34;

### Should Praat overwrite pre-existing files?

overwrite$ = &amp;#34;yes&amp;#34;
#overwrite$ = &amp;#34;no&amp;#34;

### Do you want to save each object by its object name or by its id number?
### If object name, then use &amp;#34;name&amp;#34; (e.g., &amp;#34;sentence1.wav&amp;#34;).
### If object id number, then use &amp;#34;id&amp;#34; (e.g., &amp;#34;42.wav&amp;#34;).

save_method$ = &amp;#34;name&amp;#34;
#save_method$ = &amp;#34;id&amp;#34;





################################################################################
### Before we start, let&amp;#39;s check whether you&amp;#39;ve entered sensible
### input for the variables above...
################################################################################

### Let&amp;#39;s check if the output directory exists.
### This script will throw an error if the directory doesn&amp;#39;t exist
### (i.e., it won&amp;#39;t write to a mysterious temp directory).

### First check whether the input directory ends in a backslash (if so, removed)

if right$(dir_out$,1)=&amp;#34;/&amp;#34;
	dir_out$ = left$(dir_out$,length(dir_out$)-1)
elsif right$(dir_out$,1)=&amp;#34;\&amp;#34;
	dir_out$ = left$(dir_out$,length(dir_out$)-1)
endif

### Then create a temporary txt file in the folder
### and try to write it to the output folder.

### NOTE: The &amp;#34;nocheck&amp;#34; below asks Praat not to complain if the folder
### does *not* exist. We&amp;#39;ll manually check whether the saving of this
### temp txt file has succeeded or not further down below.

temp_filename$ = dir_out$ + &amp;#34;/&amp;#34; + &amp;#34;my_temporary_Praat_file.txt&amp;#34;
nocheck writeFileLine: temp_filename$, &amp;#34;This is just to check if the directory exists&amp;#34;

### Can the file be found?

file_exists_yesno = fileReadable(temp_filename$)

if file_exists_yesno = 1
	# if you *could* read that temp txt file,
	# this confirms that the directory is valid.
	# Then you can delete it.
	deleteFile: temp_filename$
else
	# if that file wasn&amp;#39;t readable, that means that the directory wasn&amp;#39;t valid. 
	printline The folder &amp;#39;dir_out$&amp;#39; was not found
	exit Your directory doesn&amp;#39;t exist. Check spelling. The directory must *already* exist.
endif





################################################################################
################################################################################
#################################    SCRIPT    #################################
################################################################################
################################################################################

### Make sure you&amp;#39;ve selected the objects you&amp;#39;d like to save in
### the Praat object window. If nothing is selected, the script exits.

nSelected = numberOfSelected()
if nSelected = 0
	exit No objects selected.
endif

### Store the object id numbers in an array

for thisObject to nSelected
	objectArray [&amp;#39;thisObject&amp;#39;] = selected(&amp;#39;thisObject&amp;#39;)
endfor

### Loop through this array and for each id number
### select the corresponding object and save it.

for thisArrayNumber to nSelected
	objectId = objectArray [&amp;#39;thisArrayNumber&amp;#39;]
	select &amp;#39;objectId&amp;#39;
	type$ = extractWord$(selected$(), &amp;#34;&amp;#34;)
	name$ = extractLine$(selected$(), &amp;#34; &amp;#34;)
	
	if save_method$ = &amp;#34;name&amp;#34;
		if type$ = &amp;#34;Sound&amp;#34;
			does_file_exist = fileReadable(&amp;#34;&amp;#39;dir_out$&amp;#39;\&amp;#39;name$&amp;#39;.wav&amp;#34;)
			if does_file_exist = 1
				if overwrite$ = &amp;#34;no&amp;#34;
					exit The file &amp;#39;dir_out$&amp;#39;\&amp;#39;name$&amp;#39;.wav&amp;#39; already exists! If you wish to overwrite, set the variable overwrite$ to &amp;#34;yes&amp;#34;.
				endif
			endif
			Write to WAV file... &amp;#39;dir_out$&amp;#39;\&amp;#39;name$&amp;#39;.wav
		else
			does_file_exist = fileReadable(&amp;#34;&amp;#39;dir_out$&amp;#39;\&amp;#39;name$&amp;#39;.&amp;#39;type$&amp;#39;&amp;#34;)
			if does_file_exist = 1
				if overwrite$ = &amp;#34;no&amp;#34;
					exit The file &amp;#39;dir_out$&amp;#39;\&amp;#39;name$&amp;#39;.&amp;#39;type$&amp;#39; already exists! If you wish to overwrite, set the variable overwrite$ to &amp;#34;yes&amp;#34;.
				endif
			endif
			Write to text file... &amp;#39;dir_out$&amp;#39;\&amp;#39;name$&amp;#39;.&amp;#39;type$&amp;#39;
		endif
	elsif save_method$ = &amp;#34;id&amp;#34;
		if type$ = &amp;#34;Sound&amp;#34;
			does_file_exist = fileReadable(&amp;#34;&amp;#39;dir_out$&amp;#39;\&amp;#39;objectId$&amp;#39;.wav&amp;#34;)
			if does_file_exist = 1
				if overwrite$ = &amp;#34;no&amp;#34;
					exit The file &amp;#39;dir_out$&amp;#39;\&amp;#39;objectId$&amp;#39;.wav&amp;#39; already exists! If you wish to overwrite, set the variable overwrite$ to &amp;#34;yes&amp;#34;.
				endif
			endif
			Write to WAV file... &amp;#39;dir_out$&amp;#39;\&amp;#39;objectId&amp;#39;.wav
		else
			does_file_exist = fileReadable(&amp;#34;&amp;#39;dir_out$&amp;#39;\&amp;#39;objectId$&amp;#39;.&amp;#39;type$&amp;#39;&amp;#34;)
			if does_file_exist = 1
				if overwrite$ = &amp;#34;no&amp;#34;
					exit The file &amp;#39;dir_out$&amp;#39;\&amp;#39;objectId$&amp;#39;.&amp;#39;type$&amp;#39; already exists! If you wish to overwrite, set the variable overwrite$ to &amp;#34;yes&amp;#34;.
				endif
			endif
			Write to text file... &amp;#39;dir_out$&amp;#39;\&amp;#39;objectId&amp;#39;.&amp;#39;type$&amp;#39;
		endif
	endif
endfor

### Now set the selection back to what it was before running this script.

for current to nSelected
	objectId = objectArray [&amp;#39;current&amp;#39;]
	if current = 1
		select &amp;#39;objectId&amp;#39;
	else
		plus &amp;#39;objectId&amp;#39;
	endif
endfor

################################################################################
# End of script
################################################################################
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Conjuring up words that were never spoken</title>
      <link>https://hrbosker.github.io/demos/conjuring-words/</link>
      <pubDate>Mon, 11 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/demos/conjuring-words/</guid>
      <description>&lt;h2 id=&#34;remember-this-one&#34;&gt;Remember this one?&lt;/h2&gt;
&lt;p&gt;Do you remember this famous quote, starting at 00:15s?&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/Z9WDsgCIroE&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;This was Neil Armstrong, landing on the moon on July 20, 1969. But hold on, what is he saying exactly? Let&amp;rsquo;s listen to the first part of his famous quote:&lt;/p&gt;






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/conjuring-words/firstpart_mono.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;

&lt;p&gt;Is it:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&amp;ldquo;one small step for man&amp;rdquo;, or:&lt;/li&gt;
&lt;li&gt;&amp;ldquo;one small step for &lt;strong&gt;a&lt;/strong&gt; man&amp;rdquo;?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Both transcripts are grammatically viable, but they mean different things. In (1), &amp;ldquo;man&amp;rdquo; is used as a synonym of &amp;ldquo;mankind&amp;rdquo;, while in (2) &amp;ldquo;man&amp;rdquo; is used with the meaning of &amp;ldquo;person&amp;rdquo;. Only the second transcript actually fits the remainder of the quote (&amp;quot;&amp;hellip;one giant leap for mankind&amp;quot;) and Neil Armstrong himself also claimed that he had uttered the second version (&lt;a href=&#34;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0155975&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Baese-Berk et al., 2016&lt;/a&gt;). &lt;strong&gt;But how come people miss the &amp;ldquo;a&amp;rdquo; in the recording?&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;speech-is-messy&#34;&gt;Speech is messy&lt;/h2&gt;
&lt;p&gt;When we talk, we don&amp;rsquo;t produce &amp;lsquo;spaces&amp;rsquo; between words. Instead, we join all the words together, producing a connected stream of sound. This is especially true for function words, like &amp;ldquo;a&amp;rdquo;, &amp;ldquo;or&amp;rdquo;, &amp;ldquo;for&amp;rdquo;, and &amp;ldquo;and&amp;rdquo;. It is quite likely that Neil Armstrong intended to say &amp;ldquo;for &lt;strong&gt;a&lt;/strong&gt; man&amp;rdquo;, but in stringing together the sounds for the words, the &amp;ldquo;a&amp;rdquo; was &amp;rsquo;lost&amp;rsquo; in articulation. This is what speech scientists call &amp;lsquo;reductions&amp;rsquo; in speech.&lt;/p&gt;
&lt;h2 id=&#34;taking-speech-rate-into-account&#34;&gt;Taking speech rate into account&lt;/h2&gt;
&lt;p&gt;OK, so the &amp;ldquo;a&amp;rdquo; is &amp;lsquo;reduced&amp;rsquo; in the original pronunciation. But human perception is not only determined by the input signal alone. Instead, it is heavily context-dependent, taking into account such contextual factors as who is talking, why he is talking, and even how fast the speech is likely to come in!&lt;/p&gt;
&lt;p&gt;Evidence for this comes from &amp;lsquo;context effects&amp;rsquo;, whereby for instance the acoustic characteristics of a preceding sentence can influence what you hear next. Let&amp;rsquo;s take &lt;strong&gt;speech rate&lt;/strong&gt; for example. If you hear someone say &amp;ldquo;That&amp;rsquo;s one small step&amp;hellip;&amp;rdquo; at a really fast tempo, it is very likely that the next few words will be spoken at a fast rate too. And conversely: if someone happens to speak at a slow rate, the next few sounds will likely be slow too. This means people are likely to interpret the next few sounds &lt;em&gt;in line with the speech rate&lt;/em&gt; of the preceding sentence.&lt;/p&gt;
&lt;h2 id=&#34;conjuring-up-the-a&#34;&gt;Conjuring up the &amp;ldquo;a&amp;rdquo;&lt;/h2&gt;
&lt;p&gt;Now let&amp;rsquo;s see if we can make the &amp;ldquo;a&amp;rdquo; in Neil Armstrong&amp;rsquo;s quote appear and disappear by playing around with the speech rate of &lt;em&gt;only the surrounding speech&lt;/em&gt;. Note that &lt;strong&gt;we&amp;rsquo;re not changing anything about the &amp;ldquo;for (a)&amp;rdquo; part&lt;/strong&gt; of the audio clip (highlighted in red in video below). All we&amp;rsquo;ll do is speed up (compressed by 2) or slow down (compressed by 0.5) the surrounding parts of the recording. &lt;strong&gt;Do you hear &amp;ldquo;for man&amp;rdquo; or &amp;ldquo;for a man&amp;rdquo;?&lt;/strong&gt;&lt;/p&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/conjuring-words/ratenorm.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;&lt;em&gt;Click here to access the audio from the video&lt;/em&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;blockquote&gt;
&lt;p&gt;ORIGINAL CLIP&lt;/p&gt;
&lt;/blockquote&gt;
&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/conjuring-words/firstpart_mono.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;blockquote&gt;
&lt;p&gt;CONTEXT SLOWED DOWN&lt;/p&gt;
&lt;/blockquote&gt;
&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/conjuring-words/firstpart_slow.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;blockquote&gt;
&lt;p&gt;CONTEXT SPED UP&lt;/p&gt;
&lt;/blockquote&gt;
&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/conjuring-words/firstpart_fast.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;h2 id=&#34;what-happened-there&#34;&gt;What happened there?&lt;/h2&gt;
&lt;p&gt;Most listeners will report hearing &amp;ldquo;for man&amp;rdquo; in the bottom clip, with the slowed-down context, but &amp;ldquo;for &lt;strong&gt;a&lt;/strong&gt; man&amp;rdquo; in the top clip, with the sped-up context. Notably, these clips have the exact same &amp;ldquo;for a&amp;rdquo; part; they only differ in the speech rate of the surrounding context. The slow context makes listeners predict that the &amp;ldquo;for (a)&amp;rdquo; part was uttered at a slow rate too. But this critical &amp;ldquo;for (a)&amp;rdquo; does not contain a really slow &amp;ldquo;a&amp;rdquo;, so people &amp;lsquo;miss&amp;rsquo; the function word. However, in a fast context, listeners expect the critical &amp;ldquo;for (a)&amp;rdquo; to be uttered at a fast rate. This &amp;ldquo;for (a)&amp;rdquo; does indeed (kinda) match a really fast and short &amp;ldquo;a&amp;rdquo;, so people are more likely to report hearing &amp;ldquo;for &lt;strong&gt;a&lt;/strong&gt; man&amp;rdquo;.&lt;/p&gt;
&lt;h2 id=&#34;why-is-this-important&#34;&gt;Why is this important?&lt;/h2&gt;
&lt;p&gt;Context effects abound in perception, and so they also surface in speech perception. People interpret speech sounds differently depending on the surrounding speech rate, formants, and perceived pitch. But even non-acoustic aspects of the context are taken into account: people hear a sound differently depending on the talker&amp;rsquo;s gender, the talker&amp;rsquo;s hand gestures, one&amp;rsquo;s own preceding speech, and there&amp;rsquo;s even reports that a stuffed toy seen in the background can change what you hear (&lt;a href=&#34;https://www.degruyter.com/document/doi/10.1515/ling.2010.027/html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hay &amp;amp; Drager, 2010&lt;/a&gt;). All these phenomena together shape human speech perception. So if we want Automatic Speech Recognition (the Siri&amp;rsquo;s, Cortana&amp;rsquo;s, and Alexa&amp;rsquo;s of this world) to approach human-like behavior, we need to know each and every aspect that defines what words we (think we) hear. Oh, and apparently it also helps extraterrestrial communication.&lt;/p&gt;
&lt;h2 id=&#34;relevant-papers&#34;&gt;Relevant papers&lt;/h2&gt;
&lt;blockquote&gt;










  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2017).
  &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-2017-jeplmc/&#34;&gt;How our own speech rate influences our perception of others&lt;/a&gt;.
  &lt;em&gt;Journal of Experimental Psychology: Learning, Memory, and Cognition, 43&lt;/em&gt;(8), 1225-1238, doi:10.1037/xlm0000381.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_2364495_7/component/file_2472957/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/bosker-2017-jeplmc/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;













&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1037/xlm0000381&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;
&lt;blockquote&gt;










  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2017).
  &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-2017-app/&#34;&gt;Accounting for rate-dependent category boundary shifts in speech perception&lt;/a&gt;.
  &lt;em&gt;Attention, Perception &amp;amp; Psychophysics, 79&lt;/em&gt;, 333-343, doi:10.3758/s13414-016-1206-4.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_2342414_8/component/file_2398345/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/bosker-2017-app/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;













&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.3758/s13414-016-1206-4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>...annotate in Praat</title>
      <link>https://hrbosker.github.io/resources/how-to/annotate-in-praat/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/resources/how-to/annotate-in-praat/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    We&amp;rsquo;ll cover making TextGrid annotations in Praat. We&amp;rsquo;ll read a wav file in Praat, create an empty TextGrid with several tiers, add boundaries delimiting individual words and phonemes in the recording, and save the annotations to a TextGrid file.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;open-praat&#34;&gt;Open Praat&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Download Praat:&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.fon.hum.uva.nl/praat/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.fon.hum.uva.nl/praat/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;This will download a zipped folder containing a praat.exe file. Save it somewhere where you can find it later, unzip it, and open praat.exe&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Here&amp;rsquo;s a video tutorial from Matt Winn about downloading Praat: &lt;a href=&#34;https://www.youtube.com/watch?v=QonCpMS5JPg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.youtube.com/watch?v=QonCpMS5JPg&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;When you open Praat, two windows pop up. One is called Praat Objects (or: object window), and the other is called Praat Picture. Praat Objects is the most important interface for working with sound and annotations, while Praat Picture is where the figures you draw appear (e.g., waveforms or spectrograms) allowing you to save them as .png or .eps files. In most cases, you can ignore Praat Picture and close it.&lt;/li&gt;
&lt;li&gt;In the Objects window, there are fixed menus at the top (Praat, New, Open, Save) and bottom (Rename, Copy, Inspect, Info, Remove). On the right is a dynamic menu, which changes depending on which objects are selected (Sounds, TextGrids, Spectra, etc.).&lt;/li&gt;
&lt;/ul&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /resources/how-to/annotate-in-praat/objects_picture_hu1b3ccb0ffc57c9cfcac0c013fdd91d36_96046_1db0e33ff3626c79fe9e709016363b97.webp 400w,
               /resources/how-to/annotate-in-praat/objects_picture_hu1b3ccb0ffc57c9cfcac0c013fdd91d36_96046_d3a280ba0c193eeb99a67f3e35a27fdc.webp 760w,
               /resources/how-to/annotate-in-praat/objects_picture_hu1b3ccb0ffc57c9cfcac0c013fdd91d36_96046_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://hrbosker.github.io/resources/how-to/annotate-in-praat/objects_picture_hu1b3ccb0ffc57c9cfcac0c013fdd91d36_96046_1db0e33ff3626c79fe9e709016363b97.webp&#34;
               width=&#34;600&#34;
               height=&#34;472&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;&lt;em&gt;Why is it called Praat anyway?&lt;/em&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;em&gt;Praat&lt;/em&gt; was created by Paul Boersma and David Weenink, two Dutch speech scientists from Amsterdam. The Dutch word &amp;lsquo;praat&amp;rsquo; &lt;a href=&#34;https://translate.google.nl/?sl=nl&amp;amp;tl=en&amp;amp;text=praat&amp;amp;op=translate&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;/pra:t/&lt;/a&gt; is the &lt;a href=&#34;https://www.fon.hum.uva.nl/paul/papers/speakUnspeakPraat_glot2001.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;imperative form&lt;/a&gt; of the verb &amp;rsquo;to speak&amp;rsquo; (so &lt;em&gt;&amp;ldquo;speak!&amp;rdquo;&lt;/em&gt;).&lt;/p&gt;
&lt;/details&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Parts of this how-to are adapted from Aletheia Cui&amp;rsquo;s really clear and helpful &lt;a href=&#34;https://aletheiacui.github.io/tutorials/segmentation_with_praat.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Segmentation with Praat tutorial&lt;/a&gt;, including many of the screenshots. Praat also has a &lt;a href=&#34;http://www.fon.hum.uva.nl/praat/manual/Intro.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Praat manual&lt;/a&gt; itself: it is available online and offline as part of the Praat software. In Praat, go to &lt;em&gt;Help &amp;gt; Praat Intro&lt;/em&gt;. However, the search function in Praat isn&amp;rsquo;t great so often typing a question into Google is more helpful.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;open-a-sound-file&#34;&gt;Open a sound file&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Open a sound file: &lt;em&gt;Open &amp;gt; Read from file&amp;hellip;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;The file will show up as a Sound object in the object window, and upon opening will already automatically be selected (highlighted in blue)&lt;/li&gt;
&lt;li&gt;Note that Praat, like me, has a serious dislike for spaces in filenames. If your file is called &amp;ldquo;file number one.wav&amp;rdquo;, Praat will present it in the object window as &amp;ldquo;file_number_one&amp;rdquo;, replacing spaces with underscores. This will for instance affect the default name when saving the object in Praat. So here&amp;rsquo;s a free piece of advice: &lt;em&gt;better avoid spaces in filenames&amp;hellip;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;You can view the sound by clicking &lt;em&gt;View &amp;amp; Edit&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;This will show the sound waveform (oscillogram) on top and a spectrogram below that.&lt;/li&gt;
&lt;/ul&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /resources/how-to/annotate-in-praat/soundeditor_hu946c1ff79d5b8f4d64dbd230b6747996_298555_87a44014db3acbf9b47a6dbd8554d11c.webp 400w,
               /resources/how-to/annotate-in-praat/soundeditor_hu946c1ff79d5b8f4d64dbd230b6747996_298555_6dff94be66fd37544cfe8db90c130f2d.webp 760w,
               /resources/how-to/annotate-in-praat/soundeditor_hu946c1ff79d5b8f4d64dbd230b6747996_298555_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://hrbosker.github.io/resources/how-to/annotate-in-praat/soundeditor_hu946c1ff79d5b8f4d64dbd230b6747996_298555_87a44014db3acbf9b47a6dbd8554d11c.webp&#34;
               width=&#34;600&#34;
               height=&#34;404&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;h2 id=&#34;zooming-and-playing&#34;&gt;Zooming and playing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;You can zoom in to a different parts of the recording by clicking and dragging inside the waveform to select a part and click &lt;code&gt;[sel]&lt;/code&gt; or [CTRL+N] in the bottom left to zoom in to this part. You can also use &lt;code&gt;bak&lt;/code&gt; [CTRL+B] to go back to the previous view, &lt;code&gt;[all]&lt;/code&gt; [CTRL+A] to view the entire sound file, and &lt;code&gt;[in]&lt;/code&gt; [CTRL+I] and &lt;code&gt;[out]&lt;/code&gt; [CTRL+O] to gradually zoom in and out.&lt;/li&gt;
&lt;li&gt;You can play parts of the sound by selecting an interval and hitting [TAB] to play and [ESCAPE] to stop the playback. This is identical to clicking on the interval appearing in the first gray bar at the very bottom of the TextGrid window. Clicking on the second gray bar will play the visible window, while clicking on the third and last gray bar will play the entire sound.&lt;/li&gt;
&lt;li&gt;Now close the Sound window because we do not only want to &lt;em&gt;view&lt;/em&gt; the sound, we also want to &lt;em&gt;annotate&lt;/em&gt; it in a TextGrid.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;create-a-textgrid&#34;&gt;Create a TextGrid&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Go back to the Praat object window, select the Sound by clicking on it (if it wasn&amp;rsquo;t selected already), and click &lt;em&gt;Annotate &amp;gt; To TextGrid&amp;hellip;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Praat will then ask you for the names of tiers. Each TextGrid can have multiple tiers, combining for instance word-level annotations (longer intervals demarcating individual words), syllable-level annotations (shorter intervals demarcating the syllables inside the words), and phoneme-level annotations (short intervals demarcating the individual sounds).&lt;/li&gt;
&lt;li&gt;Tiers inside TextGrids come in two flavors: interval tiers and point tiers. Interval tiers allow you to add &amp;lsquo;boundaries&amp;rsquo; that demarcate certain acoustic events (words, syllables, phonemes), indicating where they are and how long they are. For word-level annotations in an interval tier, you&amp;rsquo;d need two boundaries to demarcate a single word: one at its onset and another at its offset. Point tiers allow you to add &amp;lsquo;points&amp;rsquo; that identify certain individual time points, but not the intervals between time points. &lt;strong&gt;Interval tiers are the most commonly used type of tier&lt;/strong&gt;, so we&amp;rsquo;ll primarily focus on interval tiers.&lt;/li&gt;
&lt;li&gt;Enter the names of your interval tiers in the top field, separated by space (e.g., &lt;code&gt;word phoneme&lt;/code&gt;), and click OK. If you want any of these to be point tiers instead of interval tiers, copy the name of the point tier in the second field (e.g., &lt;code&gt;word phoneme F1 F2&lt;/code&gt; in the first field, defining &lt;code&gt;F1 F2&lt;/code&gt; in the second field as point tiers).&lt;/li&gt;
&lt;/ul&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /resources/how-to/annotate-in-praat/to_tg_hu8ee2ec56f2975846d71d30f3dca084f2_98091_f4abe1f1146350bdac8c0a1107007309.webp 400w,
               /resources/how-to/annotate-in-praat/to_tg_hu8ee2ec56f2975846d71d30f3dca084f2_98091_bf28b811b1fe8870296d1fb479840903.webp 760w,
               /resources/how-to/annotate-in-praat/to_tg_hu8ee2ec56f2975846d71d30f3dca084f2_98091_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://hrbosker.github.io/resources/how-to/annotate-in-praat/to_tg_hu8ee2ec56f2975846d71d30f3dca084f2_98091_f4abe1f1146350bdac8c0a1107007309.webp&#34;
               width=&#34;600&#34;
               height=&#34;320&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;You&amp;rsquo;ll find a new object appear in the object window, namely a(n empty) TextGrid object with the same name as the Sound object. Also, it is selected automatically (highlighted in blue).&lt;/li&gt;
&lt;li&gt;Select both the Sound object and the TextGrid object (clicking while holding [SHIFT] or [CTRL]) together, and click &lt;em&gt;View and Edit&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /resources/how-to/annotate-in-praat/view_edit_hu56544f61f252b41f9159be213f8b0a30_235320_cbed4e8850b60798b2bdcad2daa5757d.webp 400w,
               /resources/how-to/annotate-in-praat/view_edit_hu56544f61f252b41f9159be213f8b0a30_235320_c4be26d213fa656f4455cddcfa1830d4.webp 760w,
               /resources/how-to/annotate-in-praat/view_edit_hu56544f61f252b41f9159be213f8b0a30_235320_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://hrbosker.github.io/resources/how-to/annotate-in-praat/view_edit_hu56544f61f252b41f9159be213f8b0a30_235320_cbed4e8850b60798b2bdcad2daa5757d.webp&#34;
               width=&#34;600&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;You&amp;rsquo;ll see a new window, with the waveform on top, then a spectrogram, and then the various tiers of the TextGrid appearing below. You&amp;rsquo;ll find the names for the various tiers (that you defined in the previous step) appear on the right.&lt;/li&gt;
&lt;/ul&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /resources/how-to/annotate-in-praat/tg_window_hu0428fc244bf14a2c34e1b264ed061132_316880_b9bc643e46a7a99d148904814aab5011.webp 400w,
               /resources/how-to/annotate-in-praat/tg_window_hu0428fc244bf14a2c34e1b264ed061132_316880_20d34257e7ce2e1f90664f56affc4110.webp 760w,
               /resources/how-to/annotate-in-praat/tg_window_hu0428fc244bf14a2c34e1b264ed061132_316880_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://hrbosker.github.io/resources/how-to/annotate-in-praat/tg_window_hu0428fc244bf14a2c34e1b264ed061132_316880_b9bc643e46a7a99d148904814aab5011.webp&#34;
               width=&#34;600&#34;
               height=&#34;633&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;Now you can start editing the TextGrid by adding &amp;lsquo;boundaries&amp;rsquo; (in interval tiers) and &amp;lsquo;points&amp;rsquo; (in point tiers). We&amp;rsquo;ll first focus on word-level annotations, identifying the onset and offset of individual words.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;segment-words&#34;&gt;Segment words&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Select a time window in the waveform that contains your word of interest, and click &lt;code&gt;[sel]&lt;/code&gt; [CTRL+N].&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s often wise to select a rather generous interval around a word of interest. This allows you to then select the &amp;rsquo;true&amp;rsquo; onset and offset of the word with greater precision and without missing anything.&lt;/li&gt;
&lt;li&gt;Now that you see the word in greater detail, we&amp;rsquo;ll segment it.&lt;/li&gt;
&lt;li&gt;Click inside the waveform at the word onset and drag to the word offset. This will select the word inside the waveform. Hit [CTRL+,] to move the left boundary of the selection to a zero-crossing, [CTRL+.] to move the right boundary of the selection to a zero-crossing, and then [ENTER] to create an interval in the top tier of the TextGrid (top tier selected by default).&lt;/li&gt;
&lt;li&gt;Instead of adding an entire interval (i.e, two boundaries) in one go, you can also click on an individual time point in the waveform, hit [CTRL+0] to move to the nearest zero-crossing, and then hit [ENTER] to insert a single boundary.&lt;/li&gt;
&lt;li&gt;You can still adjust the boundaries by dragging them around with the mouse. However, there&amp;rsquo;s no keyboard shortcut to move a pre-existing boundary to a zero-crossing: you&amp;rsquo;ll have to do this manually by selecting it and going to &lt;em&gt;Boundary &amp;gt; Move to nearest zero crossing&lt;/em&gt;. Consequently, it&amp;rsquo;s often easier to simply remove a boundary [ALT + BACKSPACE], click at the new location, hit [CTRL+0] and [ENTER] to place the new boundary.&lt;/li&gt;
&lt;/ul&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /resources/how-to/annotate-in-praat/add_interval_huedf5541d0db3d0b2753ea11aa888fd67_256852_c899439b2b372b5d284cfe91e203ff8d.webp 400w,
               /resources/how-to/annotate-in-praat/add_interval_huedf5541d0db3d0b2753ea11aa888fd67_256852_a4e66d305367484eceda033f848690db.webp 760w,
               /resources/how-to/annotate-in-praat/add_interval_huedf5541d0db3d0b2753ea11aa888fd67_256852_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://hrbosker.github.io/resources/how-to/annotate-in-praat/add_interval_huedf5541d0db3d0b2753ea11aa888fd67_256852_c899439b2b372b5d284cfe91e203ff8d.webp&#34;
               width=&#34;600&#34;
               height=&#34;511&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Always select time points at positive-going zero-crossings!&lt;/strong&gt; This is particularly important when you want to extract intervals from a sound file and manipulate them (but less important if you only want to measure certain acoustic features, such as intensity or F0, without extracting/manipulating anything).&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-0&#34;&gt;
  &lt;summary&gt;&lt;em&gt;Why should I?&lt;/em&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;p&gt;Sound is a wave of air pressure. Imagine we have two pure tones of 440 Hz, but one starts at a positive-going zero-crossing (&lt;code&gt;phase = 0¬∞&lt;/code&gt;; signal A) and the other at a point of high air pressure (&lt;code&gt;phase = 1/2œÄ&lt;/code&gt;; signal B). If we then concatenate these two sounds (signal C), and play the concatenation (see below figure), our speakers need to jump from 0 to 1 near instantaneously. This will result in an audible click. &lt;strong&gt;NOTE:&lt;/strong&gt; concatenating two tones that both start at a zero-crossing, but one starts at a &lt;em&gt;positive-going&lt;/em&gt; zero-crossing and the other at a &lt;em&gt;negative-going&lt;/em&gt; zero-crossing, also results in audible artifacts (signal D). Therefore, it&amp;rsquo;s advisable to consistently select time points at &lt;em&gt;positive-going zero-crossings&lt;/em&gt;. If you want to manually inspect this, select a tiny time interval around a boundary in Praat and click &lt;code&gt;[sel]&lt;/code&gt; [CTRL+N].&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /resources/how-to/annotate-in-praat/zerocrossings_hufec699dee811b6475d5f5a4d49e10963_284571_4e8af0f1c49eb867e7b173ccc3adff54.webp 400w,
               /resources/how-to/annotate-in-praat/zerocrossings_hufec699dee811b6475d5f5a4d49e10963_284571_15a1eb025ede2e3860056b4e69d9d6fb.webp 760w,
               /resources/how-to/annotate-in-praat/zerocrossings_hufec699dee811b6475d5f5a4d49e10963_284571_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://hrbosker.github.io/resources/how-to/annotate-in-praat/zerocrossings_hufec699dee811b6475d5f5a4d49e10963_284571_4e8af0f1c49eb867e7b173ccc3adff54.webp&#34;
               width=&#34;600&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt;&lt;em&gt;This is signal C, can you hear a &amp;lsquo;click&amp;rsquo; midway in the recording?&lt;/em&gt;&lt;/p&gt;
&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/resources/how-to/annotate-in-praat/concat1.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;p&gt;&lt;em&gt;This is signal D, can you again hear a &amp;lsquo;click&amp;rsquo; midway in the recording?&lt;/em&gt;&lt;/p&gt;
&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/resources/how-to/annotate-in-praat/concat2.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/details&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;segment-phonemes&#34;&gt;Segment phonemes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The two boundaries you just added demarcate the word, but they also form the onset of the first phoneme, and the offset of the last phoneme inside the word. Extend the first boundary in the first &lt;em&gt;word&lt;/em&gt; tier to the second &lt;em&gt;phoneme&lt;/em&gt; tier by clicking on it (blue line turns red) and then click the small circle appearing just below it (inside the second tier). This will place a boundary in the second tier at the exact same time point as in the first tier.&lt;/li&gt;
&lt;li&gt;Now place additional individual boundaries at the onsets and offsets of the various phonemes inside the word in the second &lt;em&gt;phoneme&lt;/em&gt; tier. First click anywhere on the second tier to select it, then click on a time point in the waveform, and hit [ENTER] to place a boundary.&lt;/li&gt;
&lt;li&gt;You can play intervals to verify if they are accurate. Click on an interval to select it and hit [TAB] to play it and [ESCAPE] to stop the playback. This is identical to clicking on the interval appearing in the first gray bar at the very bottom of the TextGrid window. Clicking on the second gray bar will play the visible window, while clicking on the third and last gray bar will play the entire sound.&lt;/li&gt;
&lt;li&gt;Now let&amp;rsquo;s label the individual intervals. Click on an interval and simply type in a label. It will appear inside the interval. You can also edit it in the utmost top left of the TextGrid window.&lt;/li&gt;
&lt;li&gt;The final result could then look something like this:&lt;/li&gt;
&lt;/ul&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /resources/how-to/annotate-in-praat/pat_segmented_hu87f8ccd721fd1472408c416483e2ac11_509251_5315503b0fc7e11a0e63a548ba1dec05.webp 400w,
               /resources/how-to/annotate-in-praat/pat_segmented_hu87f8ccd721fd1472408c416483e2ac11_509251_3935f0d40eb8664271d0acac9bd14f62.webp 760w,
               /resources/how-to/annotate-in-praat/pat_segmented_hu87f8ccd721fd1472408c416483e2ac11_509251_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://hrbosker.github.io/resources/how-to/annotate-in-praat/pat_segmented_hu87f8ccd721fd1472408c416483e2ac11_509251_5315503b0fc7e11a0e63a548ba1dec05.webp&#34;
               width=&#34;600&#34;
               height=&#34;511&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;h2 id=&#34;working-with-textgrids&#34;&gt;Working with TextGrids&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;You can quickly scroll through TextGrids by clicking on an interval and then hitting [ALT + ‚Üí] to see the next one, or [ALT + ‚Üê] to see the previous one. Similarly, use [ALT + ‚Üì] and [ALT + ‚Üë] to go up or down a tier.&lt;/li&gt;
&lt;li&gt;You can easily remove boundaries by selecting an interval and hitting [ALT + BACKSPACE]. This will remove the left boundary of the interval.&lt;/li&gt;
&lt;li&gt;If you want to remove multiple boundaries in a row, you hit [ALT + BACKSPACE], [ALT + ‚Üí], [ALT + BACKSPACE], [ALT + ‚Üí], etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;saving-the-textgrid&#34;&gt;Saving the TextGrid&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Save frequently! If Praat crashes, you will lose all your recent work unless you save regularly.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Option 1:&lt;/strong&gt; Save from the TextGrid Editor window. Just hit [Ctrl+S] which will bring up a dialogue window that allows you to specify the name and location of the new .TextGrid file. &lt;strong&gt;NOTE:&lt;/strong&gt; Even though you see the waveform in the TextGrid Editor window, saving in this way &lt;em&gt;only&lt;/em&gt; saves the TextGrid (i.e., everything in the tiers), &lt;em&gt;not&lt;/em&gt; the Sound!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Option 2:&lt;/strong&gt; Save from the Praat object window. Select the TextGrid object (highlighting it in blue) and go to &lt;em&gt;Save &amp;gt; Save as text file&lt;/em&gt;. This will bring up the same dialogue window. Do &lt;strong&gt;not&lt;/strong&gt; select both the Sound and TextGrid objects; you only want to save your annotations inside the tiers in the TextGrid.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;lets-automate-this&#34;&gt;Let&amp;rsquo;s automate this&lt;/h2&gt;
&lt;p&gt;Praat has a scripting interface that allows you to enter code to perform certain functions in a much faster and more efficient way. See our &lt;a href=&#34;https://hrbosker.github.io/resources/how-to/script-in-praat/&#34;&gt;How to script in Praat&lt;/a&gt; to get you acquainted with the Praat scripting interface.&lt;/p&gt;
&lt;p&gt;Once you&amp;rsquo;re familiar with Praat scripting, check out our &lt;a href=&#34;https://hrbosker.github.io/resources/scripts/annotate/&#34;&gt;annotation script&lt;/a&gt;. This script reads all files in a given folder, presents them to the user for manual annotation, and when the user clicks &lt;em&gt;Next&lt;/em&gt; to proceed Praat will automatically save the user&amp;rsquo;s annotations and present the next file for annotation. This speeds up an annotation workflow considerably.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Happy Praat&amp;rsquo;ing!&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Annotate</title>
      <link>https://hrbosker.github.io/resources/scripts/annotate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/resources/scripts/annotate/</guid>
      <description>&lt;p&gt;This script streamlines an annotation workflow: it presents a TextGrid for manual annotation to the user, you perform some changes, and when you click Next, it automatically saves the changes and presents the next TextGrid, and so on. This is particularly useful for when you have forced aligned TextGrids (e.g., from &lt;a href=&#34;https://hrbosker.github.io/resources/other-resources/#videoaudio-editing&#34;&gt;WebMAUS&lt;/a&gt; or &lt;a href=&#34;https://hrbosker.github.io/resources/other-resources/#videoaudio-editing&#34;&gt;EasyAlign&lt;/a&gt;) that you&amp;rsquo;d like to manually evaluate and edit.&lt;/p&gt;
&lt;p&gt;Moreover, the script keeps track of who annotated what, can continue where you left off yesterday, allows users to enter comments about their annotations, and blinds file names to avoid human annotation biases. The script can be updated to present new empty TextGrids (instead of any pre-existing ones, in case you only have .wav files) or to automatically perform changes to TextGrid tiers/intervals before presenting them for manual annotation.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You can also &lt;a href=&#34;../annotate.praat&#34;&gt;download the script&lt;/a&gt; as a .praat file.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;################################################################################
### Hans Rutger Bosker, Radboud University
### HansRutger.Bosker@ru.nl
### Date: 30 June 2022, run in Praat 6.2.12 on Windows 10
### License: CC BY-NC 4.0
################################################################################

	###&amp;gt;&amp;gt; This script reads a directory containing sound files with pre-existing TextGrids,
	###&amp;gt;&amp;gt;	for instance resulting from a forced aligner (e.g., WebMAUS or EasyAlign).
	###&amp;gt;&amp;gt;	IMPORTANT: Every Sound should have a pre-existing TextGrid file **with the same name**!
	###&amp;gt;&amp;gt;	It opens every Sound + Textgrid combination, presents it to the user for editing,
	###&amp;gt;&amp;gt;	allows the user to enter comments about the annotations, and then saves the
	###&amp;gt;&amp;gt;	edited TextGrid with &amp;#34;_edited&amp;#34; suffix in the subfolder &amp;#39;edited_textgrids&amp;#39;.
	###&amp;gt;&amp;gt;	User comments are tracked in the file &amp;#39;annotation_log.txt&amp;#39; in the same subfolder.
	###&amp;gt;&amp;gt;	
	###&amp;gt;&amp;gt;	&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; If you **do not** yet have pre-existing TextGrids (i.e., only sound files),	&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;
	###&amp;gt;&amp;gt;	&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; you can adjust the script to read all .wav files, create new empty TextGrids,	&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;
	###&amp;gt;&amp;gt;	&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; and present those for editing and saving...									&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;
	###&amp;gt;&amp;gt;	&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; See the line with &amp;#34;CREATE EMPTY TEXTGRIDS&amp;#34;									&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;
	###&amp;gt;&amp;gt;
	###&amp;gt;&amp;gt; This script can be run by multiple users simultaneously, for instance when
	###&amp;gt;&amp;gt;   multiple annotators are working on the same shared folder. It keeps track
	###&amp;gt;&amp;gt;   of what files have already been edited: it only presents TextGrids for editing
	###&amp;gt;&amp;gt;   that do not yet have an &amp;#34;_edited&amp;#34; version pre-existing in the subfolder.
	###&amp;gt;&amp;gt;   This also means that users can close the script or Praat at anytime
	###&amp;gt;&amp;gt;   without losing data. Then, next time someone runs the script, it will
	###&amp;gt;&amp;gt;   start with the files that are &amp;#39;left over&amp;#39; from the previous run.
	###&amp;gt;&amp;gt;	NOTE: This checking of which files already exist can slow the script down
	###&amp;gt;&amp;gt;	when working with folders with &amp;gt;5000 files...
	###&amp;gt;&amp;gt;
	###&amp;gt;&amp;gt; At present, the script **only** presents pre-existing tiers and intervals
	###&amp;gt;&amp;gt;   for editing (e.g., adding boundaries, dragging boundaries around, etc.).
	###&amp;gt;&amp;gt;   This script can be augmented by automatically adding tiers or intervals
	###&amp;gt;&amp;gt;   before the TextGrid is presented for editing, so users can annotate
	###&amp;gt;&amp;gt;   new tiers/intervals. See the line with &amp;#34;ADD/REMOVE TIERS HERE&amp;#34;.

################################################################################
### Variables you will definitely need to customize:
################################################################################

### Where can the Sound and TextGrid files be found?

dir_in$ = &amp;#34;C:\Users\hanbos\mysounds&amp;#34;

### Do you want to use &amp;#39;blinded&amp;#39; objects in Praat to avoid human biases in annotation?
### Default value: &amp;#34;yes&amp;#34;
### Change to &amp;#34;no&amp;#34; if you want to use original object names.

blinded$ = &amp;#34;yes&amp;#34;





################################################################################
### Before we start, let&amp;#39;s check whether you&amp;#39;ve entered sensible
### input for the variables above...
################################################################################

### Let&amp;#39;s check if the input directory exists.
### This script will throw an error if the directory doesn&amp;#39;t exist
### (i.e., it won&amp;#39;t write to a mysterious temp directory).

### First check whether the input directory ends in a backslash (if so, removed)

if right$(dir_in$,1)=&amp;#34;/&amp;#34;
	dir_in$ = left$(dir_in$,length(dir_in$)-1)
elsif right$(dir_in$,1)=&amp;#34;\&amp;#34;
	dir_in$ = left$(dir_in$,length(dir_in$)-1)
endif

### Then create a temporary txt file in the folder
### and try to write it to the input folder.

### NOTE: The &amp;#34;nocheck&amp;#34; below asks Praat not to complain if the folder
### does *not* exist. We&amp;#39;ll manually check whether the saving of this
### temp txt file has succeeded or not further down below.

temp_filename$ = dir_in$ + &amp;#34;/&amp;#34; + &amp;#34;my_temporary_Praat_file.txt&amp;#34;
nocheck writeFileLine: temp_filename$, &amp;#34;This is just to check if the directory exists&amp;#34;

### Can the file be found?

file_exists_yesno = fileReadable(temp_filename$)

if file_exists_yesno = 1
	# if you *could* read that temp txt file,
	# this confirms that the directory is valid.
	# Then you can delete it.
	deleteFile: temp_filename$
else
	# if that file wasn&amp;#39;t readable, that means that the directory wasn&amp;#39;t valid. 
	printline The folder &amp;#39;dir_in$&amp;#39; was not found
	exit Your directory doesn&amp;#39;t exist. Check spelling. The directory must *already* exist.
endif

## Let&amp;#39;s also check whether a subfolder with edited TextGrids already exists,
## for instance when the script has been run and exited before.

temp_filename$ = dir_in$ + &amp;#34;/edited_textgrids/&amp;#34; + &amp;#34;my_temporary_Praat_file.txt&amp;#34;
nocheck writeFileLine: temp_filename$, &amp;#34;This is just to check if the subfolder exists&amp;#34;

### Can the file be found?

subfolderfile_exists_yesno = fileReadable(temp_filename$)

if subfolderfile_exists_yesno = 1
	# if you *could* read that temp txt file,
	# this confirms that the subfolder already exists.
	# Then you can delete it.
	deleteFile: temp_filename$
else
	# if it didn&amp;#39;t yet exist, let&amp;#39;s create the subfolder
	createFolder: &amp;#34;&amp;#39;dir_in$&amp;#39;/edited_textgrids&amp;#34;
endif





################################################################################
################################################################################
#################################    SCRIPT    #################################
################################################################################
################################################################################

## Let&amp;#39;s keep track of who annotated which file. This can be helpful when
## multiple annotators run the same script on the same shared folder.

beginPause: &amp;#34;Please enter your name:&amp;#34;
	text: &amp;#34;annotator&amp;#34;, &amp;#34;&amp;#34;
clicked = endPause: &amp;#34;Next&amp;#34;, 1

## Now we create a list of TextGrid files in the input directory:

Create Strings as file list: &amp;#34;list_of_files&amp;#34;, &amp;#34;&amp;#39;dir_in$&amp;#39;/*.TextGrid&amp;#34;

	#######################################################################################
	## CREATE EMPTY TEXTGRIDS
	########################
	## If you do not yet have pre-existing TextGrids (but a folder with only sound files instead),
	## you can read the sound files in the directory and create empty TextGrids for the user
	## to edit.
	## Adjust this script as follows:
	## - Change *.TextGrid to *.wav in the line above.
	## - Change *.TextGrid to *.wav in the line below starting with &amp;#34;extposition$&amp;#34;
	## - Replace this line: Read from file... &amp;#39;dir_in$&amp;#39;\&amp;#39;name$&amp;#39;.TextGrid
	##		with this line: To TextGrid: &amp;#34;manual&amp;#34;, &amp;#34;&amp;#34;
	#######################################################################################

nfiles = Get number of strings
if nfiles = 0
	exit The directory &amp;#39;dir_in$&amp;#39; does not contain any TextGrid files.
endif

## By randomizing this file list, it allows for multiple users to simultanously work
## on the same folder without overwriting previous annotations. It also reduces the risk
## of human biases in annotations (e.g., annotator fatigue affecting one condition more
## than another condition).

Randomize

## Now we&amp;#39;ll loop through the list and present individual files...

for i from 1 to &amp;#39;nfiles&amp;#39;
	select Strings list_of_files
	
	fileplusext$ = Get string... &amp;#39;i&amp;#39;
	extposition = index(fileplusext$, &amp;#34;.TextGrid&amp;#34;)
	name$ = left$(fileplusext$, (&amp;#39;extposition&amp;#39;-1))

	outname$ = &amp;#34;&amp;#39;name$&amp;#39;_edited&amp;#34;
	fulloutname$ = &amp;#34;&amp;#39;dir_in$&amp;#39;/edited_textgrids/&amp;#39;outname$&amp;#39;.TextGrid&amp;#34;

	## Let&amp;#39;s check if an &amp;#34;_edited&amp;#34; version already exists.
	## The script only presents those files for editing that do not yet have been edited before.
	
	editedfile_exists_yesno = fileReadable(fulloutname$)
	if editedfile_exists_yesno
		do_nothing = 1
	else
		Read from file... &amp;#39;dir_in$&amp;#39;\&amp;#39;name$&amp;#39;.wav
		if blinded$ = &amp;#34;yes&amp;#34;
			Rename... current_Sound
		endif
		sound_name$ = selected$(&amp;#34;Sound&amp;#34;)
		## If a filename contains spaces, Praat replaces these spaces with underscores.
		## Example: &amp;#34;file number 1.wav&amp;#34; in a given folder becomes
		##			&amp;#34;file_number_1.wav&amp;#34; in the Praat object window.
		## Therefore, it is important **not** to use a filename variable (here: &amp;#39;name$&amp;#39;)
		## in &amp;#39;selecting commands&amp;#39; in Praat, like &amp;#39;select&amp;#39; and &amp;#39;plus&amp;#39;!
		## Better still: do not use spaces in filenames!
		
		Read from file... &amp;#39;dir_in$&amp;#39;\&amp;#39;name$&amp;#39;.TextGrid
		if blinded$ = &amp;#34;yes&amp;#34;
			Rename... current_TextGrid
		endif
		tg_name$ = selected$(&amp;#34;TextGrid&amp;#34;)

		#######################################################################################
		## ADD/REMOVE TIERS HERE
		########################
		## This is where we you could adjust the script to automatically add/remove tiers
		## and/or automatically adjust intervals (setting them to the nearest zero crossings?).
		## Duplicating tiers can be helpful when you want to view original vs. manually edited
		## tiers below/above each other in one and the same edited TextGrid.
		## Example:
		## &amp;gt; Duplicate tier... 1 1 newtier
		## [ARGUMENTS: position_of_tier_to_duplicate position_for_new_tier name_of_new_tier]
		#######################################################################################

		plus Sound &amp;#39;sound_name$&amp;#39;
		Edit

		beginPause: &amp;#34;Please check and edit this TextGrid.&amp;#34;
			comment: &amp;#34;Please check and edit the annotations.&amp;#34;
			text: &amp;#34;Comments&amp;#34;, &amp;#34;&amp;#34;
		clicked = endPause: &amp;#34;Next&amp;#34;, 1
		
		editor TextGrid &amp;#39;tg_name$&amp;#39;
			Close
		endeditor

		select TextGrid &amp;#39;tg_name$&amp;#39;
		Write to text file... &amp;#39;fulloutname$&amp;#39;
		plus Sound &amp;#39;sound_name$&amp;#39;
		Remove

		appendFileLine: &amp;#34;&amp;#39;dir_in$&amp;#39;/edited_textgrids/annotation_log.txt&amp;#34;, annotator$, tab$, name$, tab$, comments$
	endif
endfor

select Strings list_of_files
Remove

################################################################################
# End of script
################################################################################
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Repackaging speech</title>
      <link>https://hrbosker.github.io/demos/repackaging/</link>
      <pubDate>Fri, 08 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/demos/repackaging/</guid>
      <description>&lt;h2 id=&#34;how-fast-can-your-ears-go&#34;&gt;How fast can your ears go?&lt;/h2&gt;
&lt;p&gt;Listen to this clip of a talker saying the telephone number &lt;em&gt;496-0356&lt;/em&gt;&amp;hellip;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;496-0356&lt;/em&gt; &amp;ndash; [original]






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/repackaging/4960356_nopause_mono.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You&amp;rsquo;ll probably have no problem understanding the same digits when it&amp;rsquo;s compressed by a factor of 2 (i.e., twice as fast)&amp;hellip;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;496-0356&lt;/em&gt; &amp;ndash; compressed by 2






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/repackaging/4960356_nopause_mono_k2.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And when it&amp;rsquo;s compressed by a factor of 3?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;496-0356&lt;/em&gt; &amp;ndash; compressed by 3






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/repackaging/4960356_nopause_mono_k3.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And compressed by 4?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;496-0356&lt;/em&gt; &amp;ndash; compressed by 4






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/repackaging/4960356_nopause_mono_k4.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Or even by 5?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;496-0356&lt;/em&gt; &amp;ndash; compressed by 5






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/repackaging/4960356_nopause_mono_k5.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Research has demonstrated that compression rates up to 3 are still doable (kinda&amp;hellip;) but intelligibility breaks down quite dramatically for higher compression rates (e.g., &lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00652/full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ghitza, 2014&lt;/a&gt;; &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2018-lcn&#34;&gt;Bosker &amp;amp; Ghitza, 2018&lt;/a&gt;). So the last two clips are unintelligible to most listeners.&lt;/p&gt;
&lt;p&gt;This has been suggested to be due to how our brain works. Our brain is known to &amp;rsquo;track&amp;rsquo; incoming speech by aligning its &amp;lsquo;brain waves&amp;rsquo; (neural oscillations in the &lt;em&gt;theta&lt;/em&gt; range, 3-9 Hz) to the syllable rhythm of the speech (amplitude modulations in the temporal envelope). But when the syllables come in too rapidly (&amp;gt;9 Hz), the brain waves can&amp;rsquo;t keep up, resulting in poor intelligibility.&lt;/p&gt;
&lt;h2 id=&#34;making-unintelligible-speech-intelligible-again&#34;&gt;Making unintelligible speech intelligible again&lt;/h2&gt;
&lt;p&gt;But there&amp;rsquo;s a trick to help the brain keep up. Let&amp;rsquo;s take the unintelligible clip with the telephone number &lt;em&gt;496-0356&lt;/em&gt; compressed by a factor of 5. Here it is again:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;496-0356&lt;/em&gt; &amp;ndash; compressed by 5






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/repackaging/4960356_nopause_mono_k5.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That&amp;rsquo;s tough, right? No wonder with a syllable rate of over 12 Hz!&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s chop this clip up into short snippets of 66 ms (&amp;ldquo;packages&amp;rdquo;, cf. top panel in the figure at the top of this page) and space them apart by 100 ms (i.e., inserting silent intervals). This brings the package rate down to around 6 Hz. &lt;strong&gt;Can your brain keep up with that?&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;496-0356&lt;/em&gt; &amp;ndash; repackaged






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/repackaging/4960356_nopause_mono_repack.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;What wizardry! What was unintelligible before is made (more&amp;hellip;) intelligible by adding some &amp;lsquo;breathing space&amp;rsquo; for the brain. &lt;em&gt;Note that the speech signal itself did not change&lt;/em&gt;: it is the same acoustic content as before, but just presented at a slower pace so your brain can keep up!&lt;/p&gt;
&lt;h2 id=&#34;and-now-the-exam&#34;&gt;And now the exam!&lt;/h2&gt;
&lt;p&gt;Here is a new telephone number, also consisting of 7 digits. Can you tell me &lt;strong&gt;what the last four digits are?&lt;/strong&gt;&lt;/p&gt;






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/repackaging/4960592_nopause_mono_k5.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;

&lt;blockquote&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-8&#34;&gt;
  &lt;summary&gt;&lt;em&gt;Click here to see the correct answer&amp;hellip;&lt;/em&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;em&gt;0592&lt;/em&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;/blockquote&gt;
&lt;p&gt;And what about this one?&lt;/p&gt;






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/repackaging/4980137_nopause_mono_k5.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;

&lt;blockquote&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-10&#34;&gt;
  &lt;summary&gt;&lt;em&gt;Click here to see the correct answer&amp;hellip;&lt;/em&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;em&gt;0137&lt;/em&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;/blockquote&gt;
&lt;p&gt;Presumably everybody has a hard time correctly hearing these digits, because these are again recordings that have been compressed by a factor of 5!&lt;/p&gt;
&lt;p&gt;But now try these:&lt;/p&gt;






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/repackaging/4960723_nopause_mono_repack.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;

&lt;blockquote&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-12&#34;&gt;
  &lt;summary&gt;&lt;em&gt;Click here to see the correct answer&amp;hellip;&lt;/em&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;em&gt;0723&lt;/em&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;/blockquote&gt;






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/repackaging/8790164_nopause_mono_repack.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;

&lt;blockquote&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-14&#34;&gt;
  &lt;summary&gt;&lt;em&gt;Click here to see the correct answer&amp;hellip;&lt;/em&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;em&gt;0164&lt;/em&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;/blockquote&gt;
&lt;p&gt;That probably sounded much more intelligible. These are two examples of &amp;lsquo;repackaged speech&amp;rsquo;: first compressed by a factor of 5, chopped up into 66 ms snippets, and then spaced apart by 100 ms. And your brain was presumably very grateful for that extra breathing space (&amp;ldquo;my pleasure, brain&amp;hellip;&amp;rdquo;).&lt;/p&gt;
&lt;h2 id=&#34;why-is-this-important&#34;&gt;Why is this important?&lt;/h2&gt;
&lt;p&gt;This phenomenon can tell us something about what acoustic aspects support speech intelligibility. If we know what aspects of speech are critical for proper intelligibility, then that knowledge would be helpful, for instance, (i) for speech synthesizers, such as Automatic Announcement Systems in public transport, to generate speech signals that human listeners can understand well, (ii) for hearing aids to &amp;rsquo;enrich&amp;rsquo; incoming speech signals and present those optimized signals to the listening brain, or (iii) for communication with the elderly who often experience difficulty with speech perception, especially in noise.&lt;/p&gt;
&lt;h2 id=&#34;relevant-papers&#34;&gt;Relevant papers&lt;/h2&gt;
&lt;blockquote&gt;










  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/oded-ghitza/&#34;&gt;Oded Ghitza&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2018).
  &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2018-lcn/&#34;&gt;Entrained theta oscillations guide perception of subsequent speech: Behavioral evidence from rate normalization&lt;/a&gt;.
  &lt;em&gt;Language, Cognition and Neuroscience,33&lt;/em&gt;(8), 955-967, doi:10.1080/23273798.2018.1439179.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_2538752_11/component/file_2630351/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/bosker-etal-2018-lcn/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;













&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1080/23273798.2018.1439179&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>...script in Praat</title>
      <link>https://hrbosker.github.io/resources/how-to/script-in-praat/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/resources/how-to/script-in-praat/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Here&amp;rsquo;s a brief intro into the Praat scripting language. We&amp;rsquo;ll cover how to write and run a script, point you to some tutorials, highlight some of the strange quirks in the Praat scripting language, and provide some scripts we find useful ourselves.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;run-your-first-script&#34;&gt;Run your first script&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Open Praat, and go to &lt;em&gt;Praat &amp;gt; New Praat script&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /resources/how-to/script-in-praat/open_praatscript_hu29fd31b82b883422b501bc54a996c18a_102016_648680a9fd9ccb6d0d83159659fbf19e.webp 400w,
               /resources/how-to/script-in-praat/open_praatscript_hu29fd31b82b883422b501bc54a996c18a_102016_79ec0d38c66cbe4e36be51cf33f4663e.webp 760w,
               /resources/how-to/script-in-praat/open_praatscript_hu29fd31b82b883422b501bc54a996c18a_102016_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://hrbosker.github.io/resources/how-to/script-in-praat/open_praatscript_hu29fd31b82b883422b501bc54a996c18a_102016_648680a9fd9ccb6d0d83159659fbf19e.webp&#34;
               width=&#34;668&#34;
               height=&#34;562&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;You&amp;rsquo;ll find a rather empty-looking window popping up on your screen, with the menus &lt;em&gt;File, Edit, Search, Convert, Font, Run&lt;/em&gt; at the top.&lt;/li&gt;
&lt;li&gt;Now type this into that scripting window:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Create Sound from formula: &amp;#34;demo&amp;#34;, 1, 0, 1, 44100, &amp;#34;1/2 * sin(2*pi*377*x)&amp;#34;
Play
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Now turn on your speakers&amp;hellip;&lt;/li&gt;
&lt;li&gt;&amp;hellip;and click &lt;em&gt;Run &amp;gt; Run&lt;/em&gt; or hit [CTRL+R] to execute the script.&lt;/li&gt;
&lt;li&gt;Congratulations, you&amp;rsquo;ve run your first script that creates and plays a short 377 Hz tone.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;every-line-is-a-click&#34;&gt;Every line is a click&lt;/h2&gt;
&lt;p&gt;The Praat scripting language is a Graphical User Interface (GUI) scripting language. This means that, put rather bluntly, &lt;strong&gt;every line in the script is like a click in the Praat object window&lt;/strong&gt;. The two lines in the code above are identical to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;selecting &lt;em&gt;New &amp;gt; Sound &amp;gt; Create Sound from formula&amp;hellip;&lt;/em&gt;, and entering some parameters&lt;/li&gt;
&lt;li&gt;clicking Play&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This also means that you can click around in Praat and then ask Praat to give you the code for those particular clicks. That is, Praat actually keeps track of every click you perform.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Go to the scripting window&lt;/li&gt;
&lt;li&gt;click &lt;em&gt;Edit &amp;gt; Paste history&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This asks Praat to paste every action you performed since opening Praat. This is particularly handy when you don&amp;rsquo;t know how to script a certain action. For instance, you wanna know how to open a sound file in Praat using a script?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Go to the Praat object window&lt;/li&gt;
&lt;li&gt;click &lt;em&gt;Open &amp;gt; Read from file&amp;hellip;&lt;/em&gt; and open a sound file&lt;/li&gt;
&lt;li&gt;Now go to the scripting window&lt;/li&gt;
&lt;li&gt;click &lt;em&gt;Edit &amp;gt; Paste history&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It should show you something along the lines of:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Read from file: &amp;#34;C:\Users\hanbos\mysounds\demo.wav&amp;#34;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;but then with a different directory and filename.&lt;/p&gt;
&lt;p&gt;So remember: &lt;strong&gt;perform the functions in the object window, paste history in the scripting window, and edit the code from there&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;tutorials&#34;&gt;Tutorials&lt;/h2&gt;
&lt;p&gt;Praat offers a scripting tutorial itself. Go to &lt;em&gt;Help &amp;gt; Praat Intro&lt;/em&gt; and scroll down to find &lt;em&gt;Scripting&lt;/em&gt;. Alternatively, go to &lt;a href=&#34;https://www.fon.hum.uva.nl/praat/manual/Scripting.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.fon.hum.uva.nl/praat/manual/Scripting.html&lt;/a&gt;. This tutorial is not too bad actually. Other third-party tutorials are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.eleanorchodroff.com/tutorial/PraatScripting.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.eleanorchodroff.com/tutorial/PraatScripting.pdf&lt;/a&gt;: some quick intro slides by Eleanor Chodroff&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://praatscripting.lingphon.net&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://praatscripting.lingphon.net&lt;/a&gt;: a &lt;strong&gt;comprehensive written tutorial&lt;/strong&gt; by J√∂rg Mayer&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://praatscriptingtutorial.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://praatscriptingtutorial.com/&lt;/a&gt;: a &lt;strong&gt;comprehensive written tutorial&lt;/strong&gt; by Daniel Riggs&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.mauriciofigueroa.cl/04_scripts/04_scripts.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.mauriciofigueroa.cl/04_scripts/04_scripts.html&lt;/a&gt;: a &lt;strong&gt;comprehensive written tutorial&lt;/strong&gt; by Mauricio A. Figueroa Candia&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;strange-quirks&#34;&gt;Strange quirks&lt;/h2&gt;
&lt;p&gt;Praat has some peculiarities that make the Praat scripting language stand out compared to other languages, like &lt;em&gt;python&lt;/em&gt; and &lt;em&gt;R&lt;/em&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Over the years, Praat has had three types of syntax. Current Praat versions are compatible with older and newer syntax types, and mixes thereof.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# This line of code extracts the first 100 ms of a sound object...
# ... in Praat versions 5.3.43 and older; before April 2013
Extract part... 0 0.1 rectangular 1 no
# ... in Praat versions between 5.3.44 and 5.3.62; April 2013 - January 2014
do(&amp;#34;Extract part&amp;#34;, 0, 0.1, &amp;#34;rectangular&amp;#34;, 1, &amp;#34;no&amp;#34;)
# ... in Praat versions 5.3.63 and later; after January 2014
Extract part: 0, 0.1, &amp;#34;rectangular, 1, &amp;#34;no&amp;#34;
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Praat variables &lt;strong&gt;always&lt;/strong&gt; start in lowercase. Capitals are reserved for functions:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;play = 2
for i to play
    Play
endfor
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Praat does not distinguish between single and double equal signs:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;intensityLevel = 75
if intensityLevel = 75
    newIntensityLevel = 80
endif
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Praat uses single quotes to access the value of a variable. This is, for instance, important when concatenating the values of different string variables:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;myDirectory$ = &amp;#34;C:\Users\hanbos\mysounds&amp;#34;
myFilename$ = &amp;#34;demo.wav&amp;#34;
Read from file: &amp;#34;&amp;#39;myDirectory$&amp;#39;\&amp;#39;myFilename$&amp;#39;&amp;#34;
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Praat&amp;rsquo;s spelling is &lt;code&gt;elsif&lt;/code&gt;, &lt;strong&gt;not&lt;/strong&gt; &lt;code&gt;elseif&lt;/code&gt; (don&amp;rsquo;t ask me why&amp;hellip;)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;if intensityLevel = 80
    Scale intensity: 75
elsif intensityLevel = 75
    Scale intensity: 80
else
    Scale intensity: 65
endif
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Different objects in Praat have different functions. For Sound objects, you can run functions like &lt;code&gt;Play&lt;/code&gt;, &lt;code&gt;Resample...&lt;/code&gt;, &lt;code&gt;Scale intensity...&lt;/code&gt;, etc., while for TextGrid objects, you can run functions like &lt;code&gt;Duplicate tier...&lt;/code&gt;, &lt;code&gt;Insert boundary...&lt;/code&gt;, etc. Therefore, it is important to &lt;strong&gt;keep track of which object is selected&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;myDirectory$ = &amp;#34;C:\Users\hanbos\mysounds&amp;#34;
myFilename$ = &amp;#34;demo.wav&amp;#34;
Read from file: &amp;#34;&amp;#39;myDirectory$&amp;#39;\&amp;#39;myFilename$&amp;#39;&amp;#34;
To TextGrid: &amp;#34;words&amp;#34;, &amp;#34;&amp;#34;
Play
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&amp;hellip; will throw an error because the newly created TextGrid is automatically selected after &lt;code&gt;To TextGrid:&lt;/code&gt; and Praat cannot play TextGrids. The solution is:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;myDirectory$ = &amp;#34;C:\Users\hanbos\mysounds&amp;#34;
myFilename$ = &amp;#34;demo.wav&amp;#34;
Read from file: &amp;#34;&amp;#39;myDirectory$&amp;#39;\&amp;#39;myFilename$&amp;#39;&amp;#34;
To TextGrid: &amp;#34;words&amp;#34;, &amp;#34;&amp;#34;
select Sound demo
Play
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;ready-made-scripts&#34;&gt;Ready-made scripts&lt;/h2&gt;
&lt;p&gt;See our &lt;a href=&#34;https://hrbosker.github.io/resources/scripts/&#34;&gt;Scripts&lt;/a&gt; archive for snippets of code we frequently use. Note, however, that &lt;strong&gt;they require customization&lt;/strong&gt; for each individual new project. &lt;em&gt;Use at your own risk!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Other scripts resources available online are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.praatvocaltoolkit.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vocal Toolkit plugin&lt;/a&gt; is a plugin for Praat. When installed, you can call various new functions from a button within Praat. However, it&amp;rsquo;s a little risky if you don&amp;rsquo;t know the ins-and-outs of a particular function, so &lt;strong&gt;always check the raw code&lt;/strong&gt; here:
&lt;ul&gt;
&lt;li&gt;[WINDOWS] &amp;ldquo;C:\Users\&lt;em&gt;(Username)&lt;/em&gt;\Praat\plugin_VocalToolkit&amp;rdquo;&lt;/li&gt;
&lt;li&gt;[MAC] &amp;ldquo;/Users/&lt;em&gt;(UserName)&lt;/em&gt;/Library/Preferences/Praat Prefs/&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.mattwinn.com/praat.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Matt Winn&amp;rsquo;s Listen Lab&lt;/a&gt; with some really fun &lt;a href=&#34;https://www.youtube.com/playlist?list=PL6niCBwOhjHga4bCS83VJ2uKzQ8ZjEVeG&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Youtube Praat tutorials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://holgermitterer.eu/research.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Holger Mitterer&amp;rsquo;s website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/stylerw/styler_praat_scripts&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Will Styler&amp;rsquo;s repo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;praatvscode&#34;&gt;PraatVSCode&lt;/h2&gt;
&lt;p&gt;The scripting interface in Praat itself is not the best. It&amp;rsquo;s basically a plain text editor with a Run button. There&amp;rsquo;s no syntax highlighting, no autocompletion, no regular expression search options&amp;hellip;&lt;/p&gt;
&lt;p&gt;Therefore, many users tend to write and edit their scripts in other editors, like TextPad, Notepad++, or Sublime. Some of these even provide things like syntax highlighting and autocompletion, see &lt;a href=&#34;https://praatpfanne.lingphon.net/praat-ressourcen/resources-english/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this overview&lt;/a&gt; for editor plugins for Sublime, Kate, Atom, Notepad++, Vim, and Ace. However, &lt;strong&gt;none of these can run Praat code&lt;/strong&gt;. Instead, users need to write their code in Notepad++, copy it, paste it into Praat, and then hit Run. You can&amp;rsquo;t imagine how cumbersome this is and it can introduce all sorts of human errors.&lt;/p&gt;
&lt;p&gt;A great solution is &lt;a href=&#34;https://hrbosker.github.io/resources/tools/#praatvscode&#34;&gt;PraatVSCode&lt;/a&gt;:&lt;/p&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/orhunulusahin/praatvscode/blob/main/assets/syntax_after.png?raw=true&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;600&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt;Created by our very own &lt;a href=&#34;https://hrbosker.github.io/author/orhun-ulusahin/&#34;&gt;Orhun Ulu≈üahin&lt;/a&gt;, PraatVSCode is an extension for Visual Studio Code, providing syntax highlighting, autocompletion, and even an array of code snippets that writes itself. Moreover, it allows &lt;strong&gt;running of scripts by Praat from inside Visual Studio Code&lt;/strong&gt;. &lt;a href=&#34;https://hrbosker.github.io/resources/tools/#praatvscode&#34;&gt;Try it now!&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Happy coding!&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Batch processing</title>
      <link>https://hrbosker.github.io/resources/scripts/batch-processing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/resources/scripts/batch-processing/</guid>
      <description>&lt;p&gt;This script is an in-house template / starting point for batch processing multiple files. Adapt it to your own needs to apply a particular function to multiple files or multiple time intervals within each file.&lt;/p&gt;
&lt;p&gt;In its current form, the script reads each .wav file &lt;em&gt;plus&lt;/em&gt; accompanying TextGrid in a given input directory, extracts all non-empty intervals individually, and then loops over those to find the ones labelled &amp;ldquo;vowel&amp;rdquo;. It then allows the user to apply a particular function to those intervals (such as &lt;code&gt;Scale intensity: 65&lt;/code&gt;), after which it concatenates the individual intervals back together, and saves the output in an output directory.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; In its current form, the script does not run any function on its input. It really only serves as a starting point, including snippets of code we regularly use and now do not need to look up every time we want to do batch processing in Praat.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You can also &lt;a href=&#34;../batch-processing.praat&#34;&gt;download the script&lt;/a&gt; as a .praat file.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;################################################################################
### Hans Rutger Bosker, Radboud University
### HansRutger.Bosker@ru.nl
### Date: 6 July 2022, run in Praat 6.2.12 on Windows 10
### License: CC BY-NC 4.0
################################################################################


	###&amp;gt;&amp;gt; This script is a starting point for batch processing a set of files.
	###&amp;gt;&amp;gt;	The script basically reads files in an input directory and runs a
	###&amp;gt;&amp;gt;	a to-be-defined function [see &amp;#39;Perform your function here&amp;#39; below]
	###&amp;gt;&amp;gt;	and writes the output to an output directory . This saves me having
	###&amp;gt;&amp;gt;	to look up how to create a file list, how to loop over files, etc.
	###&amp;gt;&amp;gt;	
	###&amp;gt;&amp;gt; Since this was basically intended for in-house use, I&amp;#39;ve added in bits
	###&amp;gt;&amp;gt;	and pieces that I find useful to have ready-to-go, such as:
	###&amp;gt;&amp;gt;	&amp;#39;beginPause&amp;#39; for manually specifying variables.

################################################################################
### Variables you will definitely need to customize:
################################################################################

### Where can the files be found?

dir_in$ = &amp;#34;C:\Users\hanbos\mysounds&amp;#34;

### Where should the output files be saved?

dir_out$ = &amp;#34;C:\Users\hanbos\mysounds\output&amp;#34;





################################################################################
### Let&amp;#39;s check whether the directories specified above exist...
################################################################################

### Let&amp;#39;s check if the input directory exists.
### This script will throw an error if the directory doesn&amp;#39;t exist
### (i.e., it won&amp;#39;t write to a mysterious temp directory).

### First check whether the input directory ends in a backslash (if so, removed)

if right$(dir_in$,1)=&amp;#34;/&amp;#34;
	dir_in$ = left$(dir_in$,length(dir_in$)-1)
elsif right$(dir_in$,1)=&amp;#34;\&amp;#34;
	dir_in$ = left$(dir_in$,length(dir_in$)-1)
endif

### Then create a temporary txt file in the folder
### and try to write it to the input folder.

### NOTE: The &amp;#34;nocheck&amp;#34; below asks Praat not to complain if the folder
### does *not* exist. We&amp;#39;ll manually check whether the saving of this
### temp txt file has succeeded or not further down below.

temp_filename$ = dir_in$ + &amp;#34;/&amp;#34; + &amp;#34;my_temporary_Praat_file.txt&amp;#34;
nocheck writeFileLine: temp_filename$, &amp;#34;This is just to check if the directory exists&amp;#34;

### Can the file be found?

file_exists_yesno = fileReadable(temp_filename$)

if file_exists_yesno = 1
	# if you *could* read that temp txt file,
	# this confirms that the directory is valid.
	# Then you can delete it.
	deleteFile: temp_filename$
else
	# if that file wasn&amp;#39;t readable, that means that the directory wasn&amp;#39;t valid. 
	printline The folder &amp;#39;dir_in$&amp;#39; was not found
	exit Your input directory doesn&amp;#39;t exist. Check spelling. The directory must *already* exist.
endif

## Now re-do this for the output directory:

if right$(dir_out$,1)=&amp;#34;/&amp;#34;
	dir_out$ = left$(dir_out$,length(dir_out$)-1)
elsif right$(dir_out$,1)=&amp;#34;\&amp;#34;
	dir_out$ = left$(dir_out$,length(dir_out$)-1)
endif

### Then create a temporary txt file in the folder
### and try to write it to the input folder.

### NOTE: The &amp;#34;nocheck&amp;#34; below asks Praat not to complain if the folder
### does *not* exist. We&amp;#39;ll manually check whether the saving of this
### temp txt file has succeeded or not further down below.

temp_filename$ = dir_out$ + &amp;#34;/&amp;#34; + &amp;#34;my_temporary_Praat_file.txt&amp;#34;
nocheck writeFileLine: temp_filename$, &amp;#34;This is just to check if the directory exists&amp;#34;

### Can the file be found?

file_exists_yesno = fileReadable(temp_filename$)

if file_exists_yesno = 1
	# if you *could* read that temp txt file,
	# this confirms that the directory is valid.
	# Then you can delete it.
	deleteFile: temp_filename$
else
	# if that file wasn&amp;#39;t readable, that means that the directory wasn&amp;#39;t valid. 
	printline The folder &amp;#39;dir_out$&amp;#39; was not found
	exit Your output directory doesn&amp;#39;t exist. Check spelling. The directory must *already* exist.
endif





###########################################################################
##	FORM TO MANUALLY SPECIFY VARIABLES
###########################################################################
#beginPause: &amp;#34;Enter settings&amp;#34;
#	comment: &amp;#34;Provide instructions here&amp;#34;
#	real: &amp;#34;minPitch&amp;#34;, 70
#	real: &amp;#34;maxPitch&amp;#34;, 250
#	choice: &amp;#34;method&amp;#34;, 1
#	   option: &amp;#34;Flip F0 contour&amp;#34;
#	   option: &amp;#34;Expand/Contract F0 contour&amp;#34;
#	   option: &amp;#34;Flatten F0 contour&amp;#34;
#clicked = endPause (&amp;#34;Cancel&amp;#34;, &amp;#34;OK&amp;#34;, 2)
###########################################################################
###########################################################################





## Let&amp;#39;s create a list of all the files in the input directory.

Create Strings as file list: &amp;#34;list_of_files&amp;#34;, &amp;#34;&amp;#39;dir_in$&amp;#39;/*.wav&amp;#34;

nfiles = Get number of strings
if nfiles = 0
	exit The directory &amp;#39;dir_in$&amp;#39; does not contain any .wav files.
endif

## Now we&amp;#39;ll loop through the list...

for i from 1 to &amp;#39;nfiles&amp;#39;
	select Strings list_of_files
	
	fileplusext$ = Get string... &amp;#39;i&amp;#39;
	extposition = index(fileplusext$, &amp;#34;.wav&amp;#34;)
	name$ = left$(fileplusext$, (&amp;#39;extposition&amp;#39;-1))

	Read from file... &amp;#39;dir_in$&amp;#39;\&amp;#39;name$&amp;#39;.wav
	Read from file... &amp;#39;dir_in$&amp;#39;\&amp;#39;name$&amp;#39;.TextGrid
	plus Sound &amp;#39;name$&amp;#39;
	Extract non-empty intervals... 1 no
	
	nSelected = numberOfSelected()

	## Assign an object number to each object (e.g., 1-5),
	## and save the id numbers of each object to an array.

	for thisObject to nSelected
		objectArray [&amp;#39;thisObject&amp;#39;] = selected(&amp;#39;thisObject&amp;#39;)
	endfor

	for j to nSelected
		curr_objectId = objectArray [&amp;#39;j&amp;#39;]
		select &amp;#39;curr_objectId&amp;#39;
		curr_objectName$ = selected$(&amp;#34;Sound&amp;#34;)
		
		if curr_objectName$ = &amp;#34;vowel&amp;#34;

			########################################################################
			# Perform your function here!
			#	Example: Scale intensity... 65
			########################################################################

		endif
	endfor

	for j from 1 to nSelected
		curr_objectId = objectArray [&amp;#39;j&amp;#39;]
		if j = 1
			select &amp;#39;curr_objectId&amp;#39;
		else
			plus &amp;#39;curr_objectId&amp;#39;
		endif
	endfor
	Concatenate recoverably

	select Sound chain
	Write to WAV file... &amp;#39;dir_out$&amp;#39;\&amp;#39;name$&amp;#39;_manipulated.wav
	select TextGrid chain
	Write to text file... &amp;#39;dir_out$&amp;#39;\&amp;#39;name$&amp;#39;_manipulated.TextGrid

	## Cleaning up...
	for j from 1 to nSelected
		curr_objectId = objectArray [&amp;#39;j&amp;#39;]
		if j = 1
			select &amp;#39;curr_objectId&amp;#39;
		else
			plus &amp;#39;curr_objectId&amp;#39;
		endif
	endfor
	plus Sound chain
	plus TextGrid chain
	plus Sound &amp;#39;name$&amp;#39;
	plus TextGrid &amp;#39;name$&amp;#39;
	Remove

endfor

select Strings list_of_files
Remove

################################################################################
# End of script
################################################################################
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Cocktail party listening</title>
      <link>https://hrbosker.github.io/demos/cocktail-party/</link>
      <pubDate>Thu, 07 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/demos/cocktail-party/</guid>
      <description>&lt;h2 id=&#34;a-very-dull-cocktail-party&#34;&gt;A very dull cocktail party&lt;/h2&gt;
&lt;p&gt;In everday life, we often encounter situations where there&amp;rsquo;s more than just one talker speaking, like having a conversation in a busy bar, listening to a presenter at a crowded poster session, or talking to someone on the phone while walking on the street. Somehow our brain has little trouble honing in on that one talker we want to attend to, while ignoring others. &lt;strong&gt;How do we that?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Speech researchers like ourselves tend to investigate this research question by creating terribly dull &amp;lsquo;cocktail parties&amp;rsquo;. These cocktail parties include one listener, two talkers (A and B), and a lamentable lack of any cocktails. Still, such a situation does allow the researcher to play with particular auditory and visual cues to see if that helps or hinders the listener to attend to Talker A and ignore Talker B.&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-0&#34;&gt;
  &lt;summary&gt;&lt;em&gt;Why call this a &amp;lsquo;cocktail party&amp;rsquo; in the first place?&lt;/em&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;a href=&#34;https://asa.scitation.org/doi/abs/10.1121/1.1907229&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Colin Cherry&lt;/a&gt; came up with &amp;rsquo;the cocktail party problem&amp;rsquo; in 1953: &amp;ldquo;how do we recognize what one person is saying when others are speaking at the same time&amp;rdquo;. His experiments involved playing speech from two different talkers over the same speaker, covering such topics as &amp;ldquo;the really complex nature of the causes and uses of birds&amp;rsquo; colors&amp;rdquo; and about how &amp;ldquo;religious convictions, legal systems, and politics have been so successful in accomplishing their aims&amp;rdquo;. How prof. Cherry arrived at the term &amp;lsquo;cocktail party&amp;rsquo; in light of these topics remains an outright mystery. &lt;a href=&#34;https://www.youtube.com/watch?v=mCx4T8MKbjk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scientists and parties&amp;hellip;&lt;/a&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;h2 id=&#34;the-time-between-your-ears&#34;&gt;The time between your ears&lt;/h2&gt;
&lt;p&gt;One such cue is the &lt;em&gt;interaural time difference&lt;/em&gt; (ITD). This is the difference in arrival time of a sound between two ears. Imagine Talker A is talking on your left, while Talker B is talking on your right. Their speech needs to travel through the air before reaching your ears, which takes (a very short amount of) time. Their opposite locations in space mean that the speech of Talker A will hit your left ear a fraction of a second earlier than your right ear. Likewise, the speech of Talker B will hit your right ear earlier than your left ear. Remarkably, your brain can use this difference in arrival time (ITD) to locate speakers in space, helping you to separate the speech from Talker A from the speech from Talker B.&lt;/p&gt;
&lt;p&gt;We can try to construct a situation where ITD is the &lt;em&gt;only cue&lt;/em&gt; to speakers&amp;rsquo; locations in space, creating a &lt;em&gt;virtual auditory scene&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Put on your headphones/ear buds&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Do not use speakers&lt;/strong&gt;; this works better with headphones&lt;/li&gt;
&lt;li&gt;Let&amp;rsquo;s take two recordings: one from a female Google, another from a male Alexa&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;FEMALE GOOGLE&lt;/strong&gt;






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/cocktail-party/female_g_left.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;

&lt;strong&gt;MALE ALEXA&lt;/strong&gt;






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/cocktail-party/male_a_right.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Now let&amp;rsquo;s simply mix these two recordings:&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;GOOGLE/ALEXA MIX&lt;/strong&gt;






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/cocktail-party/multitalker_mid.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;When listening to this mixture, we can already notice three things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;They&amp;rsquo;re lying.&lt;/em&gt; In defiance of their words, the positioning of the two talkers in (virtual) space is acutally identical. Most listeners would judge the two talkers as being positioned &amp;lsquo;right in front of them&amp;rsquo;, or even &amp;rsquo;talking inside their heads&amp;rsquo;. That is because &lt;em&gt;both voices reach both of your ears instantaneously&lt;/em&gt;. The audio clip is a mono file with only one channel, containing the mixed speech from both talkers. The browser sends this single channel to both sides of your headphones (if all went well&amp;hellip;), so the female speech reaches your left ear at the same point in time as it reaches your right ear (ITD = 0 ms), and the same for the male speech. In reality, this hardly ever happens (if at all), unless a speaker is standing perfectly in front of you.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Our brains are remarkable.&lt;/em&gt; Despite the unrealistic virtual positions of the two talkers, we can still freely direct our attention to the female talker (and ignore the male talker), or to the male talker (and ignore the female talker). Apparently, we can use other cues (i.e., other than ITDs) to attend one talker and ignore others, such as their (markedly different) pitch.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;This truly is a dull party&amp;hellip;&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;virtual-reality&#34;&gt;Virtual reality&lt;/h2&gt;
&lt;p&gt;Now let&amp;rsquo;s see if we can move these two talkers around in virtual space. Imagine the female talker is on your left and the male is on your right. This would mean that the female speech reaches your left ear before it reaches your right ear. And vice versa: the male speech would reach your right ear before reaching your left.&lt;/p&gt;
&lt;p&gt;We can mimick this by applying ITDs to the individual speech signals. First, we take the mono clip above and copy it to another channel, resulting in a stereo clip with two identical channels (see illustration in the clip below). Second, we take &lt;em&gt;the female speech&lt;/em&gt; (given in red below) and delay it &lt;em&gt;in the right channel&lt;/em&gt; by 0.0006 seconds = 0.6 milliseconds = 600 Œºs. Finally, we take &lt;em&gt;the male speech&lt;/em&gt; (in blue) and delay it &lt;em&gt;in the left channel&lt;/em&gt; by the same minute amount of time. Now we have a stereo clip with opposite ITDs for the two talkers!&lt;/p&gt;
&lt;p&gt;In the clip below, you&amp;rsquo;ll first hear the &amp;lsquo;old&amp;rsquo; mixture we had initially, followed by the &amp;rsquo;new&amp;rsquo; clip with manipulated ITDs. &lt;strong&gt;Can you hear the difference?&lt;/strong&gt;&lt;/p&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/cocktail-party/itd.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-5&#34;&gt;
  &lt;summary&gt;&lt;em&gt;But I unexpectedly hear the talkers in the wrong locations!&lt;/em&gt;&lt;/summary&gt;
  &lt;p&gt;If you do hear the difference between the two clips in the video, but you incorrectly hear the female talker &lt;em&gt;on the right&lt;/em&gt; and the male talker &lt;em&gt;on your left&lt;/em&gt; in the second clip, perhaps you&amp;rsquo;re wearing your headphones &amp;rsquo;the wrong way around&amp;rsquo;&amp;hellip;?!? (with L in your right ear, and vice versa?)&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;This is quite a different experience:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Now they&amp;rsquo;re finally speaking the truth!&lt;/em&gt; While in the first mixture (with ITD = 0 ms) the two talkers sound as coming from the same central position, in the second mixture (with ITD = 600 Œºs) you should have heard &lt;strong&gt;the female voice on your left and the male voice on your right&lt;/strong&gt;. Note, however, that this &lt;em&gt;does not mean&lt;/em&gt; that &amp;rsquo;the female voice was in your left ear&amp;rsquo;. The speech from either talker was presented to &lt;em&gt;both your ears&lt;/em&gt; (see illustration of &amp;lsquo;L / R&amp;rsquo; channels in the clip). Your brain &amp;lsquo;constructed&amp;rsquo; the talkers&amp;rsquo; virtual positions based on the ITD.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Our brains are even more remarkable!&lt;/em&gt; Your brain told you that the female talker is on the left because it can pick up on less than a millisecond of a time difference!&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Can we get to the cocktails now?&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;why-is-this-important&#34;&gt;Why is this important?&lt;/h2&gt;
&lt;p&gt;Speech perception is presumably the hardest in noisy listening conditions. Even if we manage to understand what our interlocutor is trying to say in the midst of all the other speech, it takes our brain considerable effort and resources to do so. Knowing which acoustic and visual cues help humans &amp;rsquo;tune in&amp;rsquo; to talkers (such as rhythm; Bosker &amp;amp; Cooke, &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2018-jasa&#34;&gt;2018&lt;/a&gt;; &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2020-jasa&#34;&gt;2020&lt;/a&gt;) can inspire innovations in hearing aid technology and telephony. Likewise, knowing which acoustic aspects are hard to ignore for listeners (i.e., &amp;lsquo;immune&amp;rsquo; to selective attention, such as speech rate; Bosker et al., &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2020-scirep&#34;&gt;2020a&lt;/a&gt;; &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2020-app&#34;&gt;2020b&lt;/a&gt;) can motivate signal processing algorithms that aim to filter these acoustic properties from unattended talkers.&lt;/p&gt;
&lt;h2 id=&#34;relevant-papers&#34;&gt;Relevant papers&lt;/h2&gt;
&lt;blockquote&gt;










  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/matthias-j.-sjerps/&#34;&gt;Matthias J. Sjerps&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/eva-reinisch/&#34;&gt;Eva Reinisch&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2020).
  &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2020-scirep/&#34;&gt;Temporal contrast effects in human speech perception are immune to selective attention&lt;/a&gt;.
  &lt;em&gt;Scientific Reports, 10&lt;/em&gt;, 5607, doi:10.1038/s41598-020-62613-8.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3214645_6/component/file_3251038/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/bosker-etal-2020-scirep/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/dp7ck/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1038/s41598-020-62613-8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;
&lt;blockquote&gt;










  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/matthias-j.-sjerps/&#34;&gt;Matthias J. Sjerps&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/eva-reinisch/&#34;&gt;Eva Reinisch&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2020).
  &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2020-app/&#34;&gt;Spectral contrast effects are modulated by selective attention in ‚Äòcocktail party‚Äô settings&lt;/a&gt;.
  &lt;em&gt;Attention, Perception &amp;amp; Psychophysics, 82&lt;/em&gt;, 1318-1332, doi:10.3758/s13414-019-01824-2.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3107080_5/component/file_3249634/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/bosker-etal-2020-app/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/3n5cv/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.3758/s13414-019-01824-2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;
&lt;blockquote&gt;










  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/martin-cooke/&#34;&gt;Martin Cooke&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2020).
  &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2020-jasa/&#34;&gt;Enhanced amplitude modulations contribute to the Lombard intelligibility benefit: Evidence from the Nijmegen Corpus of Lombard Speech&lt;/a&gt;.
  &lt;em&gt;The Journal of the Acoustical Society of America, 147&lt;/em&gt;(2), 721-730, doi:10.1121/10.0000646.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3186181_3/component/file_3186182/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/bosker-etal-2020-jasa/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://hdl.handle.net/1839/21ee5744-b5dc-4eed-9693-c37e871cdaf6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1121/10.0000646&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;
&lt;blockquote&gt;










  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/martin-cooke/&#34;&gt;Martin Cooke&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2018).
  &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2018-jasa/&#34;&gt;Talkers produce more pronounced amplitude modulations when speaking in noise&lt;/a&gt;.
  &lt;em&gt;The Journal of the Acoustical Society of America, 143&lt;/em&gt;(2), EL121-EL126, doi:10.1121/1.5024404.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_2537243_6/component/file_2554157/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/bosker-etal-2018-jasa/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;













&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1121/1.5024404&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>...run a power analysis</title>
      <link>https://hrbosker.github.io/resources/how-to/run-a-power-analysis/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/resources/how-to/run-a-power-analysis/</guid>
      <description>&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    This R script runs a simulation-based power analysis for a simple 2AFC experimental design. This is &lt;strong&gt;by no means&lt;/strong&gt; a one-size-fits-all solution to all your power needs. &lt;em&gt;Use at your own risk!&lt;/em&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The R script shared here is first and foremost intended as a &lt;strong&gt;lab template&lt;/strong&gt;, providing code that we adjust each time we run a new 2AFC (two alternative forced choice) experiment. This means &lt;strong&gt;it requires customization&lt;/strong&gt; for each individual new project.&lt;/p&gt;
&lt;p&gt;It is based on &lt;a href=&#34;https://doi.org/10.3758/s13428-021-01546-0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kumle et al. (2021, BRM)&lt;/a&gt;, adapting code from its github repo (Scenario #3) to fit our needs. It runs a simulation-based GLMM power analysis with 1000 iterations for a 2AFC design with {10, 20, 30, 40, 50} participants, each presented with 100 trials sampling from a single 5-step phonetic continuum. The estimates for fixed effects and random effects are drawn from pilot data, but can also be adopted from previous literature.&lt;/p&gt;
&lt;p&gt;The script can be adjusted to run LMM power analysis (instead of GLMM) and to include &amp;gt;1 random effects; see comments in script.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Download the &lt;a href=&#34;R_power_analysis.R&#34;&gt;R_power_analysis.R&lt;/a&gt; script here.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;##### Hans Rutger Bosker, Radboud University Nijmegen&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;##### SPEAC research group, hrbosker.github.io&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;##### HansRutger.Bosker@ru.nl&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;##### License: CC BY-NC 4.0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;##### Last updated: July 20, 2022&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;##### Code adapted from Kumle et al. (2021, Behavior Research Methods)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;##### doi: 10.3758/s13428-021-01546-0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;##### who introduced the R package &amp;#39;mixedpower&amp;#39; comparing it to &amp;#39;simr&amp;#39;.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;##### Specifically: https://lkumle.github.io/power_notebooks/Scenario3_notebook.html&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#####           and https://github.com/lkumle/analyses_power_tutorial/blob/master/Scenario%203/Analysis_Scenario3.R&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;##### In its present form, the script runs a simulation-based GLMM power analysis&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;##### for a 2AFC experimental design with 20 participants and 100 trials per pp.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;##### BinomResp is the binomial dependent variable of 0s and 1s.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;##### Predictors are Group (between-participant) and ScaledStep (within-participant).&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#####	&amp;gt; For LMMs, see: https://lkumle.github.io/power_notebooks/Scenario3_notebook.html &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;##### 	&amp;gt; For adding additional random effects, see suggestions in script below.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;########################################################################################&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# REQUIRED PACKAGES&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;########################################################################################&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;library&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;lme4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;library&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;simr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# for comparison to mixedpower&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;##### Apparently, &amp;#39;mixedpower&amp;#39; does not live on the &amp;#39;default&amp;#39; package R server.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;##### In order to install it, you need another package &amp;#39;remotes&amp;#39; to access it.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;##### Note: you only need to install these packages once. Once they&amp;#39;re installed,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;##### simply running the library() statements suffices.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#install.packages(&amp;#34;remotes&amp;#34;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#library(remotes)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#remotes::install_github(&amp;#34;DejanDraschkow/mixedpower&amp;#34;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;library&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mixedpower&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;########################################################################################&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# RETRIEVING ESTIMATES FROM PILOT DATA&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;########################################################################################&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;pilot&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;read.table&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;pilotdata.txt&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sep&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;\t&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;header&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pilot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# ppid = participant number (hence, a unique entry for each pp)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# trialnr = test item order (1:100)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# step = step on the test continuum: 1,2,3,4,5&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# stepscaled = scaled version of step (z-scored), resulting from: scale(pilot$step)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# group = group, with 2 levels: Group_1 or Group_2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# group_devcod = deviance coded &amp;#39;group&amp;#39;: -0.5 is Group_1, +0.5 is Group_2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# BinomResp = binomial coding of participants&amp;#39; responses (0s and 1s)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;pilot_m&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;glmer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BinomResp&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;~&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;group_devcod&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;stepscaled&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                   &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;|&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ppid&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                 &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pilot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;family&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;binomial&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;summary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pilot_m&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;########################################################################################&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# CREATING ARTIFICIAL DATA&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;########################################################################################&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;##### This creates a dataset in which 20 participants are presented 100 trials each.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;##### Half of the participants are assigned to Group 1, the other half to Group 2.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;##### The 100 trials per participant include steps 1, 2, 3, 4, and 5 from a phonetic continuum.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;##### So each step is repeated 20 times per participant.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 1: RANDOM EFFECTS&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# including variables used as random effects in artificial data&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;number_of_participants&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;20&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;number_of_trials_per_participant&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;100&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;artificial_data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;expand.grid&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;TrialID&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;number_of_trials_per_participant&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                               &lt;span class=&#34;n&#34;&gt;ParticipantID&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;number_of_participants&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# At present, the script is designed for an analysis with only 1 random intercept.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# If requiring &amp;gt;1 random effects&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# ...adjust the &amp;#39;artificial_data&amp;#39;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# ...add an estimate for &amp;#39;estim_ran_effs&amp;#39;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# ...adjust the &amp;#39;artificial_glmer&amp;#39;.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;							   
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 2. FIXED EFFECTS&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#  generate continuum steps&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;continuum_steps&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# repeat for every ParticipantID in data (e.g., 20 times)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;artificial_data[&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;Continuum_step&amp;#34;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;rep&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;continuum_steps&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;number_of_participants&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;artificial_data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;$&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ScaledStep&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;scale&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;artificial_data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;$&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Continuum_step&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# include group&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;artificial_data[&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;Group&amp;#34;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;0.5&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;artificial_data[artificial_data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;$&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ParticipantID&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0.5&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;number_of_participants&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;$&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Group&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;-0.5&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;summary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;as.factor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;artificial_data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;$&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Group&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 3. VERIFICATION OF DATAFRAME DESIGN&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;artificial_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;summary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;artificial_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;table&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;artificial_data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;$&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ParticipantID&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;artificial_data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;$&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Continuum_step&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;table&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;artificial_data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;$&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ParticipantID&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;artificial_data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;$&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;TrialID&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;table&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;artificial_data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;$&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ParticipantID&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;artificial_data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;$&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Group&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;########################################################################################&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# CREATING ARTIFICIAL MODEL&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;########################################################################################&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;##### See summary(pilot_m) above for random and fixed effect estimates.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# summary(pilot_m)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;##### estim_fix_effs &amp;lt;- c(intercept, firstSimpleEffect, secondSimpleEffect, interaction)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;estim_fix_effs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0.125&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;0.838&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;-1.072&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;##### estim_ran_effs &amp;lt;- c(firstRandomIntercept, secondRandomIntercept)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;estim_ran_effs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0.627&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;##### NOTE: it is very important that the order of estimates in &amp;#39;estim_fix_effs&amp;#39; and &amp;#39;estim_ran_effs&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;##### 	is identical to the other of predictors in &amp;#39;artificial_glmer&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#####	and in &amp;#39;power_S3&amp;#39; below.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;artificial_glmer&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;makeGlmer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;formula&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;BinomResp&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;~&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Group&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ScaledStep&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                                &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;|&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ParticipantID&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                              &lt;span class=&#34;n&#34;&gt;fixef&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;estim_fix_effs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;VarCorr&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;estim_ran_effs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                              &lt;span class=&#34;n&#34;&gt;family&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;binomial&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;artificial_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;summary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;artificial_glmer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;##### NOTE: at present, only testing for two simple effects without interaction&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;##### and a single random intercept for Participants.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;########################################################################################&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# POWER ANALYSIS&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;########################################################################################&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# USING THE PACKAGE simr&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#power_simr &amp;lt;- powerSim(fit = artificial_glmer, test = fixed(&amp;#34;Group&amp;#34;), nsim = 1000)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#print(power_simr)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# USING THE PACKAGE mixedpower&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# SESOI = Smallest Effect Size Of Interest&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# A simple justification strategy is to reduce all beta coefficients by 15%&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;power_mixed&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;mixedpower&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;artificial_glmer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;artificial_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                       &lt;span class=&#34;n&#34;&gt;fixed_effects&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;Group&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;ScaledStep&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                       &lt;span class=&#34;n&#34;&gt;simvar&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;ParticipantID&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;steps&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;30&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;40&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;50&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                       &lt;span class=&#34;n&#34;&gt;critical_value&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n_sim&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                       &lt;span class=&#34;n&#34;&gt;SESOI&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;estim_fix_effs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0.85&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;power_mixed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;multiplotPower&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;power_mixed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;################################################################################&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# End of script&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;################################################################################&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;em&gt;Happy R&amp;rsquo;ing!&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Move to zero-crossings</title>
      <link>https://hrbosker.github.io/resources/scripts/move-to-zero-crossings/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/resources/scripts/move-to-zero-crossings/</guid>
      <description>&lt;p&gt;This script automatically moves all boundaries in a given tier in a TextGrid file to zero-crossings, which is important for extracting sound intervals. Specifically, it adds a tier &amp;rsquo;to0x&amp;rsquo; at the top of the TextGrid that is identical to a given input tier, except that all boundaries are at zero-crossings.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You can also &lt;a href=&#34;../move-to-zero-crossings.praat&#34;&gt;download the script&lt;/a&gt; as a .praat file.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;################################################################################
### Hans Rutger Bosker, Radboud University
### HansRutger.Bosker@ru.nl
### Date: 22 July 2022, run in Praat 6.2.12 on Windows 10
### License: CC BY-NC 4.0
################################################################################

	###&amp;gt;&amp;gt; This script takes a single .wav and matching .TextGrid file as input
	###&amp;gt;&amp;gt;   and moves all boundaries in one particular tier to zero-crossings.

	###&amp;gt;&amp;gt;   Specifically, it *adds* a new &amp;#39;to0x&amp;#39; tier at the top of the TextGrid
	###&amp;gt;&amp;gt;   that is identical to a given tier except that all boundaries are
	###&amp;gt;&amp;gt;   at zero-crossings. The old TextGrid is then overwritten with the
	###&amp;gt;&amp;gt;   new extended TextGrid, but no information is lost (only added).

	###&amp;gt;&amp;gt;   This script works best if the script also subtracts the mean from
	###&amp;gt;&amp;gt;   the audio signal, and overwrites the original audio:
	###&amp;gt;&amp;gt;	&amp;#39;method&amp;#39; option 1 (default).

	###&amp;gt;&amp;gt;	If you need to move the boundaries for more than one TextGrid file,
	###&amp;gt;&amp;gt;	you can merge this script with &amp;#39;batch-processing.praat&amp;#39;,
	###&amp;gt;&amp;gt;	see: https://hrbosker.github.io/resources/scripts/batch-processing/


################################################################################
### Form to enter variables
################################################################################
beginPause: &amp;#34;Enter settings&amp;#34;
	comment: &amp;#34;Provide the directory of the sound and TextGrid file (no slash at end of string)&amp;#34;
	text: &amp;#34;directory&amp;#34;, &amp;#34;C:/Users/hanbos/mysounds&amp;#34;
	comment: &amp;#34;Provide the name of the sound file&amp;#34;
	comment: &amp;#34;(should be identical to TextGrid name)&amp;#34;
	comment: &amp;#34;NOTE: do NOT include any extension (no .wav)&amp;#34;
	text: &amp;#34;filename&amp;#34;, &amp;#34;syll1&amp;#34;
	comment: &amp;#34;Provide the interval tier number (typically 1)&amp;#34;
	real: &amp;#34;tierNumber&amp;#34;, 1
	choice: &amp;#34;method&amp;#34;, 1
	   option: &amp;#34;Subtract mean and overwrite audio file&amp;#34;
	   option: &amp;#34;Do not subtract mean&amp;#34;
clicked = endPause (&amp;#34;Cancel&amp;#34;, &amp;#34;OK&amp;#34;, 2)



################################################################################
### Before we start, let&amp;#39;s check whether you&amp;#39;ve entered sensible
### input for the variables above...
################################################################################

### Let&amp;#39;s check if the directory exists.
### This script will throw an error if the directory doesn&amp;#39;t exist
### (i.e., it won&amp;#39;t write to a mysterious temp directory).

### First check whether the input directory ends in a backslash (if so, removed)

if right$(directory$,1)=&amp;#34;/&amp;#34;
	directory$ = left$(directory$,length(directory$)-1)
elsif right$(directory$,1)=&amp;#34;\&amp;#34;
	directory$ = left$(directory$,length(directory$)-1)
endif

### Then create a temporary txt file in the folder
### and try to write it to the input folder.

### NOTE: The &amp;#34;nocheck&amp;#34; below asks Praat not to complain if the folder
### does *not* exist. We&amp;#39;ll manually check whether the saving of this
### temp txt file has succeeded or not further down below.

temp_filename$ = directory$ + &amp;#34;/&amp;#34; + &amp;#34;my_temporary_Praat_file.txt&amp;#34;
nocheck writeFileLine: temp_filename$, &amp;#34;This is just to check if the directory exists&amp;#34;

### Can the file be found?

file_exists_yesno = fileReadable(temp_filename$)

if file_exists_yesno = 1
	# if you *could* read that temp txt file,
	# this confirms that the directory is valid.
	# Then you can delete it.
	deleteFile: temp_filename$

	## Let&amp;#39;s also check whether the specified wav and TextGrid filenames exist
	## inside this particular directory.

	filepath$ = directory$ + &amp;#34;/&amp;#34; + filename$ + &amp;#34;.wav&amp;#34;
	wavFileExists = fileReadable(filepath$)

	if wavFileExists = 0
		# if the wav file does not exist
		printline Could not find &amp;#39;filename$&amp;#39;.wav in the folder &amp;#39;directory$&amp;#39;
		exit Could not find &amp;#39;filename$&amp;#39;.wav in the folder &amp;#39;directory$&amp;#39;. Check spelling.
	endif

	filepath$ = directory$ + &amp;#34;/&amp;#34; + filename$ + &amp;#34;.TextGrid&amp;#34;
	tgFileExists = fileReadable(filepath$)

	if tgFileExists = 0
		# if the TextGrid file does not exist
		printline Could not find &amp;#39;filename$&amp;#39;.TextGrid in the folder &amp;#39;directory$&amp;#39;
		exit Could not find &amp;#39;filename$&amp;#39;.TextGrid in the folder &amp;#39;directory$&amp;#39;. Check spelling.
	endif
else
	# if the temporary file wasn&amp;#39;t readable, that means that the directory wasn&amp;#39;t valid. 
	printline The folder &amp;#39;directory$&amp;#39; was not found
	exit Your directory doesn&amp;#39;t exist. Check spelling. The directory must *already* exist.
endif



################################################################################
################################################################################
#################################    SCRIPT    #################################
################################################################################
################################################################################

Read from file... &amp;#39;directory$&amp;#39;\&amp;#39;filename$&amp;#39;.wav
if method = 1
	Subtract mean
	Write to WAV file... &amp;#39;directory$&amp;#39;\&amp;#39;filename$&amp;#39;.wav
endif
Read from file... &amp;#39;directory$&amp;#39;\&amp;#39;filename$&amp;#39;.TextGrid
nInts = Get number of intervals... &amp;#39;tierNumber&amp;#39;
Insert interval tier... 1 to0x

for i to (&amp;#39;nInts&amp;#39;-1)
	select TextGrid &amp;#39;filename$&amp;#39;
	intEnd = Get end point... (&amp;#39;tierNumber&amp;#39;+1) &amp;#39;i&amp;#39;
	intLab$ = Get label of interval... (&amp;#39;tierNumber&amp;#39;+1) &amp;#39;i&amp;#39;
	select Sound &amp;#39;filename$&amp;#39;
	zxEnd = Get nearest zero crossing... 1 &amp;#39;intEnd&amp;#39;
	select TextGrid &amp;#39;filename$&amp;#39;
	Insert boundary... 1 zxEnd
	Set interval text... 1 &amp;#39;i&amp;#39; &amp;#39;intLab$&amp;#39;
endfor
intLab$ = Get label of interval... (&amp;#39;tierNumber&amp;#39;+1) &amp;#39;nInts&amp;#39;
Set interval text... 1 &amp;#39;nInts&amp;#39; &amp;#39;intLab$&amp;#39;

Write to text file... &amp;#39;directory$&amp;#39;\&amp;#39;filename$&amp;#39;.TextGrid
Remove
select Sound &amp;#39;filename$&amp;#39;
Remove

################################################################################
# End of script
################################################################################
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Look and listen</title>
      <link>https://hrbosker.github.io/demos/visual-world-paradigm/</link>
      <pubDate>Wed, 06 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/demos/visual-world-paradigm/</guid>
      <description>&lt;h2 id=&#34;listening-test&#34;&gt;Listening test&lt;/h2&gt;
&lt;p&gt;In the video below, you&amp;rsquo;ll see a simple display with four objects. First, see if you know each of the four objects. Then play the video. You&amp;rsquo;ll hear a female voice asking you to press a button for one of the objects (i.e., click on it). While watching and listening, try to keep track of where your eyes go in the display&amp;hellip;&lt;/p&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/visual-world-paradigm/vwp_fluent.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;p&gt;Do you have any idea what the next word of the speaker will be? Probably not, right? Did you notice anything particular about where in the display your gaze was at? Since you probably didn&amp;rsquo;t know what object the speaker was going to name, chances are your eyes were all over the place.&lt;/p&gt;
&lt;p&gt;OK, next video. It&amp;rsquo;s the same display, but with a new audio recording. Have a look and see if you can tell which of the four objects the speaker selects&amp;hellip;&lt;/p&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/visual-world-paradigm/vwp_disfluent.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;p&gt;Well, in this case, you may already have a slight hunch, right? The speaker was hesitating at the end of her utterance, wasn&amp;rsquo;t she? Well, chances are this native speaker of British English won&amp;rsquo;t have much trouble naming common objects, like lion, ear, or bike, would she? So could it be that she&amp;rsquo;ll refer to the Italian moka pot in the top left?&lt;/p&gt;
&lt;h2 id=&#34;disfluencies-help-you-predict-whats-coming-up&#34;&gt;Disfluencies help you predict what&amp;rsquo;s coming up&lt;/h2&gt;
&lt;p&gt;Natural speech is messy. We stumble over words, lose our line of thought, and produce tons of uhm&amp;rsquo;s and uh&amp;rsquo;s. Still, these kinds of &lt;em&gt;disfluencies&lt;/em&gt; don&amp;rsquo;t occur randomly throughout an utterance. We are much more likely to stumble before rarely occurring (low-frequency), novel (not mentioned before), and complex (long) words than we are before common and simple words.&lt;/p&gt;
&lt;p&gt;Interestingly, human listeners seem to be aware of this. In our experiments, we presented listeners with displays like the ones above together with spoken instructions to click on one of the objects. While people were watching/listening, we recorded where they were looking on the screen using eye-tracking (see lab photo below). This allowed us to track their gaze on a millisecond time scale as the utterance unfolds. Results showed that &lt;strong&gt;when people heard the speaker hesitate, they were much more likely to look at a low-frequency object, like moka pot, compared to high-frequency objects&lt;/strong&gt; (&lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2014-jml&#34;&gt;Bosker et al., 2014&lt;/a&gt;).&lt;/p&gt;
















&lt;figure  id=&#34;figure-eye-tracking-lab-at-the-mpi-c-max-planck-gesellschaft-httpswwwmpinlpagempi-labs&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.mpi.nl/sites/default/files/2019-03/18085-Eyetracking_8474_small1.jpg&#34; alt=&#34;Eye-tracking lab at the MPI. (C) Max-Planck-Gesellschaft, https://www.mpi.nl/page/mpi-labs&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;600&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Eye-tracking lab at the MPI. (C) Max-Planck-Gesellschaft, &lt;a href=&#34;https://www.mpi.nl/page/mpi-labs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.mpi.nl/page/mpi-labs&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2 id=&#34;ok-lets-try-again&#34;&gt;OK, let&amp;rsquo;s try again&amp;hellip;&lt;/h2&gt;
&lt;p&gt;Here&amp;rsquo;s another video, again with the same display, but another audio recording. Once again, have a listen and see if you can tell which object the speaker will name:&lt;/p&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/visual-world-paradigm/vwp_li.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;p&gt;In the last few milliseconds of the clip, you may have discovered a glimpse of the object. Did she say &amp;ldquo;Now press the button for the li-&amp;hellip;&amp;rdquo;? Does that mean we&amp;rsquo;ve finally figured out that it&amp;rsquo;ll be the lion after all?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s find out:&lt;/p&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/visual-world-paradigm/vwp_like.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;p&gt;&lt;em&gt;Aargh!&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;filler-words-can-be-misleading&#34;&gt;Filler words can be misleading&lt;/h2&gt;
&lt;p&gt;As mentioned before, speech is messy. We don&amp;rsquo;t only produce hesitations and disfluencies, but also litter our speech with seemingly meaningless filler words, such as &amp;lsquo;you know&amp;rsquo;, &amp;lsquo;well&amp;rsquo;, and (worst of all) &amp;rsquo;like&amp;rsquo;. Our audience, in turn, is tasked with distilling from this chaos what we actually want to communicate.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;And that can be hard.&lt;/em&gt; Filler words share their sounds (&lt;em&gt;phonology&lt;/em&gt;) with many other words. The filler &amp;rsquo;like&amp;rsquo; shares its initial sounds with words such as &amp;rsquo;lion&amp;rsquo;, &amp;rsquo;lime&amp;rsquo;, &amp;rsquo;lice&amp;rsquo;, lightbulb&amp;rsquo;, etc. Our experiments have shown that listeners are actually considering these similar-sounding words (&lt;em&gt;cohort competitor&lt;/em&gt;) when encountering &amp;rsquo;like&amp;rsquo;. When presented with displays with one &amp;lsquo;cohort competitor&amp;rsquo; (e.g., lion) and three distractors, participants were biased towards looking at the lion upon hearing &amp;ldquo;&amp;hellip;for the like&amp;hellip;&amp;rdquo;. This suggests that filler words, like &amp;ldquo;like&amp;rdquo; (see what I did there?), have an impact on the efficiency of word recognition (&lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2021-discproc&#34;&gt;Bosker et al., 2021&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&#34;why-is-this-important&#34;&gt;Why is this important?&lt;/h2&gt;
&lt;p&gt;Eye-tracking can reveal the time-course of speech processing. It allows tracking people&amp;rsquo;s gaze with millisecond precision, often without participants themselves being aware of their own looking behavior. As such, it can show &lt;em&gt;when in time&lt;/em&gt; certain acoustic and/or visual cues influence speech perception. That kind of temporal information has for instance been used to discriminate between different models of word recognition.&lt;/p&gt;
&lt;h2 id=&#34;relevant-papers&#34;&gt;Relevant papers&lt;/h2&gt;
&lt;blockquote&gt;










  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/merel-maslowski/&#34;&gt;Merel Maslowski&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/antje-s.-meyer/&#34;&gt;Antje S. Meyer&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2020).
  &lt;a href=&#34;https://hrbosker.github.io/publication/maslowski-etal-2020-jephpp/&#34;&gt;Eye-tracking the time course of distal and global speech rate effects&lt;/a&gt;.
  &lt;em&gt;Journal of Experimental Psychology: Human Perception and Performance, 46&lt;/em&gt;(10), 1148-1163, doi:10.1037/xhp0000838.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3231520_4/component/file_3257762/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/maslowski-etal-2020-jephpp/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/c9fyd/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1037/xhp0000838&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;
&lt;blockquote&gt;










  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/esperanza-badaya/&#34;&gt;Esperanza Badaya&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/martin-corley/&#34;&gt;Martin Corley&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2021).
  &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2021-discproc/&#34;&gt;Discourse markers activate their, like, cohort competitors&lt;/a&gt;.
  &lt;em&gt;Discourse Processes, 58&lt;/em&gt;(9), 837-851, doi:10.1080/0163853X.2021.1924000.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3316633_4/component/file_3356284/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/bosker-etal-2021-discproc/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/rmj4e/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1080/0163853X.2021.1924000&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;
&lt;blockquote&gt;










  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/marjolein-van-os/&#34;&gt;Marjolein van Os,&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/rik-does/&#34;&gt;Rik Does&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/geertje-van-bergen/&#34;&gt;Geertje van Bergen&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2019).
  &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2019-jml/&#34;&gt;Counting ‚Äòuhm‚Äôs: how tracking the distribution of native and non-native disfluencies influences online language comprehension&lt;/a&gt;.
  &lt;em&gt;Journal of Memory and Language, 106&lt;/em&gt;, 189-202, doi:10.1016/j.jml.2019.02.006.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3029110_7/component/file_3038833/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/bosker-etal-2019-jml/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/5y2e6/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1016/j.jml.2019.02.006&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;
&lt;blockquote&gt;










  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/geertje-van-bergen/&#34;&gt;Geertje van Bergen&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2018).
  &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2018-jml/&#34;&gt;Linguistic expectation management in online discourse processing: An investigation of Dutch inderdaad ‚Äòindeed‚Äô and eigenlijk ‚Äòactually‚Äô&lt;/a&gt;.
  &lt;em&gt;Journal of Memory and Language, 103&lt;/em&gt;, 191-209, doi:10.1016/j.jml.2018.08.004.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_2640593_2/component/file_2640592/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/bosker-etal-2018-jml/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;













&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1016/j.jml.2018.08.004&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;
&lt;blockquote&gt;










  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hugo-quene/&#34;&gt;Hugo Quen√©&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/ted-sanders/&#34;&gt;Ted Sanders&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/nivja-h.-de-jong/&#34;&gt;Nivja H. de Jong&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2014).
  &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2014-jml/&#34;&gt;Native ‚Äòum‚Äôs elicit prediction of low-frequency referents, but non-native ‚Äòum‚Äôs do not&lt;/a&gt;.
  &lt;em&gt;Journal of Memory and Language, 75&lt;/em&gt;, 104-116, doi:10.1016/j.jml.2014.05.004.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_1900237_6/component/file_2034223/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/bosker-etal-2014-jml/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;













&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1016/j.jml.2014.05.004&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>...write a paper</title>
      <link>https://hrbosker.github.io/resources/how-to/write-a-manuscript/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/resources/how-to/write-a-manuscript/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Here&amp;rsquo;s a Word template that includes all basic sections of a paper, template statements (e.g., ethics, participants specs, etc.), &amp;lsquo;fields&amp;rsquo; to automatically update figure/table numbers, heading styles, and Zotero for reference management.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;word-template&#34;&gt;Word template&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Download the &lt;a href=&#34;manuscript-template.docx&#34;&gt;Word template&lt;/a&gt; here.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;tips-n-tricks&#34;&gt;Tips n tricks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Start now.&lt;/strong&gt; In the olden days, I wrote the paper after all experiments had been run, all data had been collected and analyzed, and I had made my mind up about its theoretical implications. Don&amp;rsquo;t go there! &lt;em&gt;Start writing the moment you think of your research question.&lt;/em&gt; Take the template above and just start filling in some of the gaps with text. Add a Methods section when you&amp;rsquo;ve designed the experiment, start adding some sentences to the Introduction when waiting for data to come in, add a reference immediately the moment you encounter a paper you will definitely want to cite, you can even start writing the Results and General Discussion based on your expectations, and then once the data are in, all you need to do is just fill in the numbers. It takes some commitment and diligence, but it will pay off in the end!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cut an onion.&lt;/strong&gt; A paper is like an onion: start BIG AND GRAND at the beginning, describing a major problem in human behavior, and then gradually get smaller and smaller and more detailed as you reach the end of the Introduction, describing the actual experiment, then go really nitty-gritty when you reach the Methods and Results. When you reach the General Discussion, you start zooming out again, first summarizing the experimental outcomes, then discussing their theoretical implications, and finally drawing BIG AND GRAND conclusions about life, the universe, and everything.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Build a skeleton.&lt;/strong&gt; First try to create an outline of your Introduction, using one sentence to summarize each paragraph. Start with &amp;ldquo;Speech perception is amazing&amp;rdquo; for the first paragraph, and &amp;ldquo;This experiment tested&amp;hellip;&amp;rdquo; for the final paragraph of the Introduction, and try to find your way from the first claim to the last one. For instance:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;(1)&lt;/strong&gt; &lt;em&gt;Speech perception is amazing&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;(2)&lt;/strong&gt; &lt;em&gt;People use both visual and auditory cues in speech perception&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;(3)&lt;/strong&gt; &lt;em&gt;These visual cues include both facial articulatory cues as well as hand gestures&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;(4)&lt;/strong&gt; &lt;em&gt;However, little is known about how visual cues to prosody influence speech perception&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;(5)&lt;/strong&gt; &lt;em&gt;Therefore, this experiment tested&amp;hellip;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Once you have this skeleton, all you then need to do is just &amp;lsquo;fill the paragraphs with words&amp;rsquo;. The skeleton also helps to find out which citation goes where (as in: when should I cite this seminal study I found?). Finally, according to the Onion Theory, the General Discussion is then simply the skeleton for the Introduction turned upside down:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;(5)&lt;/strong&gt; &lt;em&gt;This experiment tested ABC and found XYZ&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;(4)&lt;/strong&gt; &lt;em&gt;This suggests that people use visual gestural cues to prosody in speech perception&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;(3)&lt;/strong&gt; &lt;em&gt;How these visual cues interact with other visual cues, such as articulation on the face, remains an issue for future research&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;(2)&lt;/strong&gt; &lt;em&gt;Our outcomes emphasize the importance of both visual and auditory cues in face-to-face spoken communication.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;(1)&lt;/strong&gt; &lt;em&gt;Isn&amp;rsquo;t speech perception amazing?&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cheat.&lt;/strong&gt; Use tools such as Thesaurus for synonym searching, Zotero for reference management, fields in Word for figure and table numbers, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Now just start typing.&lt;/strong&gt; It&amp;rsquo;s easier to revise and edit something that you wrote yesterday than to fill an empty page. So why not jot down a few sketchy sentences and rework them later into something nice?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;em&gt;Did you know that&amp;hellip;&lt;/em&gt; in the olden days, &lt;a href=&#34;https://apastyle.apa.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;APA guidelines&lt;/a&gt; asked you to place figures only at the end of the manuscript, presumably for copyediting reasons. Their positions in the main text were identified by short &amp;ldquo;Insert Figure 1 about here&amp;rdquo; boxes. However, this is a real pain, for reviewers in particular (and everyone else too, really&amp;hellip;), because this means you need to flip back and forth between main text and figures to understand the results. It&amp;rsquo;s much easier and more effective if you place figures in text. Even if journal guidelines still ask you to place figures at the end of the manuscript, I usually just put them in text and wait for an editorial assistant to call me out&amp;hellip;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Happy writing!&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Interpolate F0 continuum</title>
      <link>https://hrbosker.github.io/resources/scripts/interpolate-f0/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/resources/scripts/interpolate-f0/</guid>
      <description>&lt;p&gt;This script creates an F0 continuum for two segmentally matching words (e.g., &lt;em&gt;SUBject&lt;/em&gt; vs. &lt;em&gt;subJECT&lt;/em&gt;). First, it matches the two words in duration and then interpolates the F0 contour linearly in 11 steps (steps 1 and 11 being the original contours), controlling for intensity.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You can also &lt;a href=&#34;../interpolate-F0.praat&#34;&gt;download the script&lt;/a&gt; as a .praat file.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;#____________________________________ HEADER __________________________________####
# date:         31.01.2023, run in Praat 6.2.12 on Windows 11
# author:       Ronny Bujok (adapted from Hans Rutger Bosker)
# email:        Ronny.Bujok@mpi.nl
# filename:     F0_stress continuum_interpolation.praat
# project:      Audiovisual Perception of Lexical Stress
# license:		CC BY-NC 4.0

####################################################################################################################
# This script takes recordings of segmentally identical, disyllabic minimal stress pairs with lexical stress on the first 
# (strong-weak; SW) or second (weak-strong; WS) syllable (e.g., VOORnaam vs. voorNAAM) and interpolates their F0-contours linearly. 
# The manipulated F0-contours are applied to the SW recording to create an F0-based lexical stress continuum.
# Duration and Intensity are set to mean, ambiguous values.
#
# This script requires minimal pairs to match in their name. Stress pattern in input files is denoted with &amp;#34;_sw&amp;#34; or &amp;#34;_ws&amp;#34; 
# (e.g., voornaam_sw.wav &amp;amp; voornaam_ws.wav). No other underscore characters are allowed.
# Textgrids for each recording are required, with boundaries at word onset, second syllable onset and word offset.
#
# Note that the parameters for pitch estimation are set to match a male voice. When working with a female voice,
# please adjust the arguments of the functions &amp;#34;Lengthen (overlap-add)&amp;#34; and &amp;#34;To Manipulation&amp;#34;.
####################################################################################################################


# Define input and output directory
input_directory$ = &amp;#34;C:\input folder&amp;#34;
output_directory$= &amp;#34;C:\output folder&amp;#34;

# create file list of all files with the ending &amp;#34;_sw.wav&amp;#34; (just sound files, one per minimal pair (sw))
Create Strings as file list... list &amp;#39;input_directory$&amp;#39;\*_sw.wav
n = Get number of strings

# _________________________________ CREATE FILELIST ____________________________________________________

# create list again because it removes itself after every iteration
for x from 1 to n
	Create Strings as file list... list &amp;#39;input_directory$&amp;#39;\*_sw.wav

# get only the word name without extension or stress identifier (sw or ws)
	select Strings list
	current_file$ = Get string... x
	idx = rindex (current_file$, &amp;#34;_&amp;#34;)
	word$ = left$(current_file$,idx-1)

#_____________________________________ DURATION ___________________________________________________________

# To interpolate F0 contours, recordings must be of equal length. So duration must be manipulated first. 

#---------------------------------- get values from SW word ---------------------------------------------
# open SW word and convert to mono
		wavfile_sw$ = &amp;#34;&amp;#39;word$&amp;#39;_sw&amp;#34;
		Read from file... &amp;#39;input_directory$&amp;#39;\&amp;#39;wavfile_sw$&amp;#39;.Textgrid
		Read from file... &amp;#39;input_directory$&amp;#39;\&amp;#39;wavfile_sw$&amp;#39;.wav
		Convert to mono
		Rename: &amp;#34;mono_sw&amp;#34;
		wav_sw_id = selected()
# select the mono file and the corresponding textgrid and extract all intervals
		selectObject: &amp;#34;TextGrid &amp;#39;wavfile_sw$&amp;#39;&amp;#34;
		plusObject: &amp;#34;Sound mono_sw&amp;#34;
		Extract all intervals: 1, &amp;#34;no&amp;#34;
# select the second interval (first syllable) and get the total duration and the intensity	
		select (wav_sw_id+2)  
		sw_dur1= Get total duration
		sw_int1= Get intensity (dB)
		Rename: &amp;#34;sw_1&amp;#34;
# select the third interval (second syllable) and get the total duration and the intensity		
		select (wav_sw_id+3)
		sw_dur2= Get total duration
		sw_int2= Get intensity (dB)
		Rename: &amp;#34;sw_2&amp;#34;

# select and rename the first and last interval (silences) for future reference, to concatenate the final audio
		select (wav_sw_id+1)
		Rename: &amp;#34;pre_silence_sw&amp;#34;
		select (wav_sw_id+4)
		Rename: &amp;#34;post_silence_sw&amp;#34;
		
#---------------------------------- get values from WS word -----------------------------------------

# open WS word and convert to mono		
		wavfile_ws$ = &amp;#34;&amp;#39;word$&amp;#39;_ws&amp;#34;
		Read from file... &amp;#39;input_directory$&amp;#39;\&amp;#39;wavfile_ws$&amp;#39;.Textgrid
		Read from file... &amp;#39;input_directory$&amp;#39;\&amp;#39;wavfile_ws$&amp;#39;.wav
		Convert to mono
		Rename: &amp;#34;mono_ws&amp;#34;
		wav_ws_id = selected()
# select the mono file and the corresponding textgrid and extract all intervals		
		selectObject: &amp;#34;TextGrid &amp;#39;wavfile_ws$&amp;#39;&amp;#34;
		plusObject: &amp;#34;Sound mono_ws&amp;#34;
		Extract all intervals: 1, &amp;#34;no&amp;#34;
# select the second interval (first syllable) and get the total duration and the intensity			
		select (wav_ws_id+2)
		ws_dur1= Get total duration
		ws_int1= Get intensity (dB)
		Rename: &amp;#34;ws_1&amp;#34;	
# select the third interval (second syllable) and get the total duration and the intensity		
		select (wav_ws_id+3)
		ws_dur2= Get total duration
		ws_int2= Get intensity (dB)
		Rename: &amp;#34;ws_2&amp;#34;	


#---------------------------------- Manipulate duration ------------------------------------------

# Set the duration of syllable 1 of the SW word to the average, ambiguous duration
		select Sound sw_1
		targetdur_1 = ((sw_dur1+ws_dur1)/2)
		Lengthen (overlap-add)... 75 250 (targetdur_1/sw_dur1)
		Rename: &amp;#34;sw_1_durmatched&amp;#34;			
# Set the duration of syllable 2 of the SW word to the average, ambiguous duration
        select Sound sw_2
		targetdur_2 = ((sw_dur2+ws_dur2)/2)
		Lengthen (overlap-add)... 75 250 (targetdur_2/sw_dur2)
		Rename: &amp;#34;sw_2_durmatched&amp;#34;	
		
# Set the duration of syllable 1 of the WS word to the average, ambiguous duration
        select Sound ws_1
		Lengthen (overlap-add)... 75 250 (targetdur_1/ws_dur1)
		Rename: &amp;#34;ws_1_durmatched&amp;#34;		
# Set the duration of syllable 2 of the WS word to the average, ambiguous duration
        select Sound ws_2
		Lengthen (overlap-add)... 75 250 (targetdur_2/ws_dur2)
		Rename: &amp;#34;ws_2_durmatched&amp;#34;
		


# Concatenate duration-manipulated SW word
		select Sound sw_1_durmatched
		plusObject: &amp;#34;Sound sw_2_durmatched&amp;#34;
		Concatenate recoverably
		select Sound chain
		Rename: &amp;#34;durmatched_sw&amp;#34;
# Concatenate duration-manipulated WS word
        select Sound ws_1_durmatched
		plusObject: &amp;#34;Sound ws_2_durmatched&amp;#34;
		Concatenate recoverably
		select Sound chain
		Rename: &amp;#34;durmatched_ws&amp;#34;
		
# save TextGrid with the boundary between the syllables. Necessary for intensity manipulation
		select TextGrid chain
		Rename: &amp;#34;syl_boundary&amp;#34;
		

		
#__________________________________________ F0 INTERPOLATION ________________________________________________

# Extract the pitch tiers
		select Sound durmatched_sw
		s1_dur = Get total duration
		To Manipulation... 0.01 50 300
		Extract pitch tier

		select Sound durmatched_ws
		s2_dur = Get total duration
		To Manipulation... 0.01 50 300
		Extract pitch tier

# create 10 ms time bins for interpolation
		timebinsize = 0.01
		nbins = floor(s1_dur/timebinsize)

		for currentbin from 1 to &amp;#39;nbins&amp;#39;
			currentbin_start = (currentbin*timebinsize)-timebinsize
			currentbin_end = (currentbin*timebinsize)
			currentbin_mid = (currentbin_start + currentbin_end)/2
			select PitchTier durmatched_sw
			currentbin_f0_s1 = Get value at time... currentbin_mid
			f0_bin_s1 [currentbin] = currentbin_f0_s1
			select PitchTier durmatched_ws
			currentbin_f0_s2 = Get value at time... currentbin_mid
			f0_bin_s2 [currentbin] = currentbin_f0_s2
		endfor

#define number of steps for interpolation
		nsteps = 11
		step_ratio = 1/(&amp;#39;nsteps&amp;#39;-1)
		# This means that...
		#	the stepsize is 10% of the difference between SW and WS;
		#	step 1 of the continuum is the original SW contour;
		#	step 11 of the continuum is the original WS contour. 
		

# interpolate the pitch tiers 
		for currentstep from 1 to &amp;#39;nsteps&amp;#39;
			select PitchTier durmatched_sw
			Copy... interpol_&amp;#39;currentstep&amp;#39;

			# first remove all original F0 points
			Remove points between... 0 &amp;#39;s1_dur&amp;#39;
			
			# now add a point for each time bin
			for currentbin from 1 to &amp;#39;nbins&amp;#39;
				currentbin_start = (currentbin*timebinsize)-timebinsize
				currentbin_end = (currentbin*timebinsize)
				currentbin_mid = (currentbin_start + currentbin_end)/2
				currentbin_f0_s1 = f0_bin_s1 [currentbin]
				currentbin_f0_s2 = f0_bin_s2 [currentbin]
				currentbin_f0_diff = currentbin_f0_s1 - currentbin_f0_s2
				currentbin_f0_ratio = currentbin_f0_diff * &amp;#39;step_ratio&amp;#39;
				currentbin_f0_interpol = currentbin_f0_s1 - (currentbin_f0_ratio * (&amp;#39;currentstep&amp;#39;-1))
				
				Add point... &amp;#39;currentbin_mid&amp;#39; &amp;#39;currentbin_f0_interpol&amp;#39;
			endfor
			
# select original audio (in this case SW recording) and replace the pitch contour
			select Manipulation durmatched_sw
			Copy... interpol_&amp;#39;currentstep&amp;#39;			
			plusObject: &amp;#34;PitchTier interpol_&amp;#39;currentstep&amp;#39;&amp;#34;
			Replace pitch tier
			minusObject: &amp;#34;PitchTier interpol_&amp;#39;currentstep&amp;#39;&amp;#34;
			Get resynthesis (overlap-add)
			
			interval_id = selected()

#__________________________________________ INTENSITY____________________________________________________

# extract each syllable again for intensity manipulation
			plusObject: &amp;#34;TextGrid syl_boundary&amp;#34;
			Extract all intervals: 1, &amp;#34;no&amp;#34;

# set (average) target intensity per syllable
            targetint_1 = ((sw_int1+ws_int1)/2) 
            targetint_2 = ((sw_int2+ws_int2)/2) 
			
# adjust the intensities to an average/ambigous value
			select (interval_id+1)
			Scale intensity... targetint_1
			Rename: &amp;#34;final_syl_1&amp;#34;
			
			select (interval_id+2)
			Scale intensity... targetint_2
			Rename: &amp;#34;final_syl_2&amp;#34;

#______________________________________ CONCATENATE &amp;amp; SAVE ______________________________________________

# copy final segment of the recording (silence post word offset), so Praat concatenates in the correct order
			select Sound post_silence_sw
			Copy... post_silence_sw_copy
# select remaining segments
			plusObject: &amp;#34;Sound pre_silence_sw&amp;#34;
			plusObject: &amp;#34;Sound final_syl_1&amp;#34;
			plusObject: &amp;#34;Sound final_syl_2&amp;#34;
			plusObject: &amp;#34;post_silence_sw_copy&amp;#34;
# concatenate
			Concatenate recoverably
			select Sound chain
			Rename: &amp;#34;&amp;#39;word$&amp;#39;_step_&amp;#39;currentstep&amp;#39;&amp;#34;
			Save as WAV file... &amp;#39;output_directory$&amp;#39;\&amp;#39;word$&amp;#39;_step_&amp;#39;currentstep&amp;#39;.wav
			select TextGrid chain
			Save as text file... &amp;#39;output_directory$&amp;#39;\&amp;#39;word$&amp;#39;_step_&amp;#39;currentstep&amp;#39;.TextGrid

		endfor
	
	select all
	Remove
	
endfor

################################################################################
# End of script
################################################################################
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Laurel or Yanny?</title>
      <link>https://hrbosker.github.io/demos/laurel-yanny/</link>
      <pubDate>Tue, 05 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/demos/laurel-yanny/</guid>
      <description>&lt;h2 id=&#34;same-audio-different-perception&#34;&gt;Same audio, different perception&lt;/h2&gt;
&lt;p&gt;In May 2018, social media exploded after the surfacing of &lt;a href=&#34;https://twitter.com/CloeCouture/status/996218489831473152&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;an audio clip&lt;/a&gt; that some perceived as &lt;em&gt;Laurel&lt;/em&gt;, but others as &lt;em&gt;Yanny&lt;/em&gt;. &lt;strong&gt;Listen and decide for yourself:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Laurel/Yanny &amp;ndash; [original]






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/laurel-yanny/Audio%20S7.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;#Laurelgate was quickly seen as the auditory version of &lt;a href=&#34;https://en.wikipedia.org/wiki/The_dress&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#TheDress&lt;/a&gt;, a photo going viral in 2015 of a white and gold dress, or was it black and blue? But how fixed is this divide between individuals? &lt;strong&gt;Can we turn #Yannists into #Laurelites, and vice versa?&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;higher-vs-lower-frequencies&#34;&gt;Higher vs. lower frequencies&lt;/h2&gt;
&lt;p&gt;Acoustic analysis of the original clip suggests that the higher frequencies (&amp;gt;1000 Hz) resembled the word &lt;em&gt;Yanny&lt;/em&gt;, but the lower frequencies (&amp;lt;1000 Hz) are more like &lt;em&gt;Laurel&lt;/em&gt;. This can be seen in the figure at the top of this page, where the upper part of the middle panel (&lt;em&gt;Original&lt;/em&gt;) is more like the right panel (&lt;em&gt;Yanny&lt;/em&gt;), but the lower part is more like the left panel (&lt;em&gt;Laurel&lt;/em&gt;). This is best demonstrated by artificially emphasizing/attenuating the higher vs. lower frequencies in the audio clip.&lt;/p&gt;
&lt;p&gt;In these sounds below, we gradually attenuate (&lt;em&gt;~turn down&lt;/em&gt;) the higher frequencies while we simultaneously emphasize (&lt;em&gt;~turn up&lt;/em&gt;) the lower frequencies. &lt;strong&gt;Play the sounds below, can you hear &lt;em&gt;Laurel&lt;/em&gt; turning into &lt;em&gt;Yanny&lt;/em&gt;?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;





  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/laurel-yanny/Audio%20S4.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;







  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/laurel-yanny/Audio%20S5.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;







  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/laurel-yanny/Audio%20S6.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;







  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/laurel-yanny/Audio%20S7.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;







  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/laurel-yanny/Audio%20S8.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;







  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/laurel-yanny/Audio%20S9.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;







  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/laurel-yanny/Audio%20S10.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;blockquote&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-8&#34;&gt;
  &lt;summary&gt;&lt;em&gt;Click here for audio specs&lt;/em&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Middle clip:&lt;/strong&gt; original Laurel/Yanny clip&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Manipulation:&lt;/strong&gt; filtered by 10 bandpass filters (with center frequencies: 31.5, 63, 125, 250, 500, 1000, 2000, 4000, 8000, 16000 Hz; using a Hann window with a roll-off width of 20, 20, 40, 80, 100, 100, 100, 100, 100, 100 Hz, respectively). Inverse intensity manipulation for high (&amp;gt;1000 Hz) vs. low (&amp;lt;1000 Hz) frequency bands in steps of 6 dB.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Top clip:&lt;/strong&gt; -18 dB attenuation for higher frequency bands, +18 dB emphasis for lower frequency bands.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bottom clip:&lt;/strong&gt; +18 dB emphasis for higher frequency bands, -18 dB attenuation for lower frequency bands.&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;/blockquote&gt;
&lt;p&gt;OK, so we can guide what people hear by artificially editing the higher vs. lower frequencies in the clip. &lt;strong&gt;But can we also make someone hear one and the same clip differently?&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;lets-add-laurels-telephone-number&#34;&gt;Let&amp;rsquo;s add Laurel&amp;rsquo;s telephone number&lt;/h2&gt;
&lt;p&gt;The perception of speech sounds is influenced by the surrounding acoustic context. The same sound can be perceived differently when, for instance, the acoustics of a preceding sentence are changed. Below, you will hear the original Laurel/Yanny clip, but this time preceded by a telephone number: &lt;em&gt;496-0356&lt;/em&gt;. In the first clip, we filtered out (&lt;em&gt;~removed&lt;/em&gt;) the lower frequencies in the telephone number leaving only the high frequency content. In the second clip, we filtered out the higher frequencies leaving only the low frequency content. Note: the Laurel/Yanny clip itself is identical in the two audios. &lt;strong&gt;Do you hear a different name after each telephone number?&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;High-pass&lt;/strong&gt; filtered telephone number + Laurel/Yanny






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/laurel-yanny/hi_ambig.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Low-pass&lt;/strong&gt; filtered telephone number + Laurel/Yanny






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/laurel-yanny/lo_ambig.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;numbing-your-ears&#34;&gt;Numbing your ears&lt;/h2&gt;
&lt;p&gt;In a crowd-sourced experiment with &amp;gt;500 online participants, we found that the same people were more likely to report hearing &lt;em&gt;Laurel&lt;/em&gt; for the first clip, but &lt;em&gt;Yanny&lt;/em&gt; for the second clip. This is because the high-frequency content in the telephone number in the first clip &lt;em&gt;&amp;rsquo;numbs your ears&amp;rsquo;&lt;/em&gt; for any following high-frequency content, thus making the lower frequencies stand out more, biasing perception towards &lt;em&gt;Laurel&lt;/em&gt;. And vice versa, the low-frequency content in the telephone number in the second clip &lt;em&gt;&amp;rsquo;numbs your ears&amp;rsquo;&lt;/em&gt; for any following low-frequency content, thus making the higher frequencies stand out more, biasing perception towards &lt;em&gt;Yanny&lt;/em&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-11&#34;&gt;
  &lt;summary&gt;&lt;em&gt;Really? Convince me&amp;hellip;&lt;/em&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;p&gt;This is Figure 1 from &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-2018-jasa&#34;&gt;Bosker (2018, &lt;em&gt;JASA&lt;/em&gt;)&lt;/a&gt; showing people&amp;rsquo;s responses in panel C. The blue line shows the proportion of &lt;em&gt;Yanny&lt;/em&gt; responses after a high-pass filtered telephone number (~first clip above), which is higher than the red line illustrating people&amp;rsquo;s responses for the &lt;strong&gt;same Laurel/Yanny clips&lt;/strong&gt; after a low-pass filtered telephone number (~second clip above).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://asa.scitation.org/na101/home/literatum/publisher/aip/journals/content/jas/2018/jas.2018.144.issue-6/1.5070144/20181206/images/large/1.5070144.figures.online.f2.jpeg
&#34; alt=&#34;Figure 1, Bosker 2018 JASA&#34; width=&#34;800&#34;/&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;why-is-this-important&#34;&gt;Why is this important?&lt;/h2&gt;
&lt;p&gt;These social media phenomena are great examples of how &lt;em&gt;our perception of the world is strongly context-dependent&lt;/em&gt;. What we perceive is &lt;em&gt;not&lt;/em&gt; wholly determined by the input signal alone, but also by the context in which the signal is perceived, including the sounds heard previously, our prior expectations, who is talking, etc. etc. As such, they highlight the subtle intricacies of human perception.&lt;/p&gt;
&lt;h2 id=&#34;relevant-papers&#34;&gt;Relevant papers&lt;/h2&gt;
&lt;blockquote&gt;










  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2018).
  &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-2018-jasa/&#34;&gt;Putting Laurel and Yanny in context&lt;/a&gt;.
  &lt;em&gt;The Journal of the Acoustic Society of America, 144&lt;/em&gt;(6), EL503-EL508, doi:10.1121/1.5070144.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3005418_7/component/file_3012156/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/bosker-2018-jasa/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/63wdh/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1121/1.5070144&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>...get published</title>
      <link>https://hrbosker.github.io/resources/how-to/get-published/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/resources/how-to/get-published/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    This describes which journal to choose, which suggested (or dispreferred?) reviewers to mention, an example cover letter, how to read the submission system, when to contact the editorial office about your submission, and how to respond to reviewers.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    I currently serve as Associate Editor of the journal &lt;em&gt;Language &amp;amp; Speech&lt;/em&gt;. However, the statements below do not reflect the official opinion of &lt;em&gt;Language &amp;amp; Speech&lt;/em&gt; nor any other journal I ever edited or reviewed for. They merely describe my personal experiences with submitting, reviewing, and editing papers, so take them with a grain of salt&amp;hellip;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;what-journal-do-i-pick&#34;&gt;What journal do I pick?&lt;/h2&gt;
&lt;p&gt;Nowadays, with Twitter, Psyarxiv, and Google Scholar, what journal a paper appears in has become a lot less important. Your peers will see it online no matter where it is published. Moreover, science as a whole is moving towards valuing the impact of an article on its own merits rather than weighing its value by the impact factor of the journal it appeared in. Still, in general, one can say that the broader the interest and audience of the journal, the more prestigious it is considered, and the harder it is to get a paper accepted (think &lt;em&gt;Nature&lt;/em&gt; vs. &lt;em&gt;Language and Speech&lt;/em&gt;, for instance). That said, the story is a little different for open access journals of broad interest. Even though these journals have papers from a wide range of disciplines, it is - at least in my experience - easier to get into these than their non-open broad-interest counterparts, hurting their prestige a bit.&lt;/p&gt;
&lt;p&gt;As speech researchers, we are livin&amp;rsquo; on the edge&amp;hellip; of the fields of experimental psychology, neurobiology, and the speech sciences. Consequently, we need to consider journals from differents fields. Is your experimental study more appealing and relevant to a psychology crowd, or to hardcore phoneticians, or nutty neuroscientists? Also remember there is no single perfect journal for your study; in fact, in all likelihood you may have to go through several before seeing it in print. Here are some things to consider&amp;hellip;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider a journal&amp;rsquo;s prestige because that will influence your chances of getting in.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider the journal&amp;rsquo;s values, for instance in terms of open access and open data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider the implications of your study: some papers simply fit better in a specialist journal than in one of broad interest.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider your CV: it&amp;rsquo;s probably better if your CV lists publications in different journals from different fields compared to only publishing in a single specialist journal.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider journal-specific criteria and guidelines, such as what article types do they have (Regular Article vs. Brief Reports), what word limits do they have, etc.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Finally, &lt;em&gt;before submitting&lt;/em&gt; check &lt;strong&gt;whether your institution has an agreement with the target journal about open access fees&lt;/strong&gt;. Many academic institutions will have signed contracts with publishers, waiving open access fees for their employees. However, often certain conditions apply, such as applying only to first-authors, or to authors with an institute email address. Make sure you know about these conditions before submitting as whatever details you submit will decide whether or not your paper falls under those institutional agreements.&lt;/p&gt;
&lt;h2 id=&#34;submitting-your-paper&#34;&gt;Submitting your paper&lt;/h2&gt;
&lt;p&gt;Most journals work with an online submission system, such as &lt;em&gt;Manuscript Central&lt;/em&gt;. Go to the website of your journal of interest, search for &amp;lsquo;Submit your paper here&amp;rsquo; or something similar, and create an account with the system.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    It helps if you create your account carefully because many of your account&amp;rsquo;s details will carry over to your submissions. For instance, &lt;strong&gt;add your ORCID number&lt;/strong&gt; to your account so your ORCID iD will automatically appear on your publications. &lt;strong&gt;Use your institute email address&lt;/strong&gt; so your institute will pay the open access fees.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Then create a new submission, which typically involves&amp;hellip;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;hellip;selecting an article type&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Is this a Regular Article (most experimental studies are) or a Review Paper (without new empirical data)? Or is it a Brief Report (check out the journal guidelines) or even a Registered Report (pre-registered study in an open repository, like OSF)?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;hellip;copying some manuscript details&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Copy the title, abstract, keywords, etc. into the relevant boxes. Note that most submission systems are simple text only, so formatting in the title or abstract may get lost. Make sure you enter these details only when the manuscript is in its definitive form. You don&amp;rsquo;t want to have a different title in the system vs. in the manuscript itself. This is particularly relevant for &lt;strong&gt;keywords&lt;/strong&gt;. Some journals allow you to select keywords yourself (see journal guidelines for how many and what character to use to separate different keywords) but sometimes they ask you to select from a dropdown menu. If the latter, you want to make sure you don&amp;rsquo;t have different keywords in the manuscript.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;hellip;ticking a few boxes&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;No, we did not submit this paper anywhere else; yes, we adhered to all relevant ethical requirements and guidelines; no, we do not have any conflicts of interest.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;hellip;uploading all relevant files&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Upload your manuscript (.docx), any supplementary information (.pdf), raw figure files (.eps or .pdf), and a cover letter (.pdf). Check with the journal what file format to use. If possible, use vector-based formats like .eps and .pdf for figures because they look nicer in print. However, because Word doesn&amp;rsquo;t like these formats, I do use .png or .jpg images inside the manuscript&amp;rsquo;s Word document. Cover letters are stemming from the olden days when they were used to include &amp;rsquo;not published elsewhere&amp;rsquo;-statements, author contact details, etc., but nowaways these details live inside the submission system. Still, it&amp;rsquo;s nice to have one, especially to highlight the importance, novelty, and main take-away messages from the paper. For broad-interest journals with lots of desk rejections, the cover letter is important; you&amp;rsquo;ll really need to sell your study.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Download a &lt;a href=&#34;cover-letter-template.docx&#34;&gt;cover letter template&lt;/a&gt; here.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Sometimes journals ask you to upload a &amp;ldquo;PDF for reviewers&amp;rdquo;. This is identical to the main manuscript, except that it is in .pdf format and has all figures and tables in the text. In the olden days, &lt;a href=&#34;https://apastyle.apa.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;APA guidelines&lt;/a&gt; asked you to place figures only at the end of the manuscript, presumably for copyediting reasons. Their positions in the main text were then identified by the authors by including short &amp;ldquo;Insert Figure 1 about here&amp;rdquo; boxes. However, this is a real pain for readers, and reviewers in particular (and everyone else too, really&amp;hellip;), because this means you need to flip back and forth between main text and figures to understand the results. It&amp;rsquo;s much easier and more effective if you place figures in text. Even if journal guidelines still ask you to place figures at the end of the manuscript, I usually just put them in text anyway and wait for an editorial assistant to call me out&amp;hellip;
  &lt;/div&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;hellip;adding suggested reviewers&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Sometimes this is mandatory, sometimes this is optional. If optional, it&amp;rsquo;s fine to leave it open but it can help speed up the review process. It is not uncommon for editors to have to invite &amp;gt;20 reviewers before finally finding two who accept, so giving the editor a hand doesn&amp;rsquo;t hurt. &lt;strong&gt;How to select them?&lt;/strong&gt; Often these are people whose work you cite in the paper. However, it&amp;rsquo;s probably better not to go for the big fish, as these people typically already receive many review requests and are more likely to decline. Why not consider selecting a postdoc from their lab? Why not consider diversity and gender balance in your suggested reviewers? Note, by the way, that it is uncommon to suggest reviewers who have not yet obtained a PhD. Also do not suggest researchers you&amp;rsquo;ve worked with in the past.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Some editorial boards are critical of suggested reviewers because of scams in the past. There have been cases when authors created fake online profiles of &amp;ldquo;Dr. A.B.C. Smith&amp;rdquo;, linked &lt;a href=&#34;mailto:abcsmith@university.com&#34;&gt;abcsmith@university.com&lt;/a&gt; to their own email inbox, and then suggested Dr. Smith as a possible reviewer, &amp;hellip; well, you get the idea. Nevertheless, it doesn&amp;rsquo;t hurt to provide a few suggestions (unless explicitly asked not to, of course); it&amp;rsquo;s the editor&amp;rsquo;s job to simply be careful in their review invitations.
  &lt;/div&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;hellip;add dispreferred reviewers&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is optional; they&amp;rsquo;re left empty in most cases. I have never filled these out myself and honestly I do not even know how most editors will use this information. Will they invite them anyway to get a different perspective on the paper, or will they honor the authors&amp;rsquo; request not to invite them? Even if you have a name in mind for this category, I guess you can still leave this box empty because chances are slim that the editor will specifically invite that person anyway.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&amp;hellip;clicking [SUBMIT]!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Fingers crossed! Wait at least 3 months before contacting the editorial office about any updates (don&amp;rsquo;t use the editor&amp;rsquo;s personal email address). Be curteous and kindly ask whether they can inform you about your manuscript&amp;rsquo;s current status. They&amp;rsquo;ll likely inform you that they&amp;rsquo;re waiting for one reviewer to get back to them, and if you&amp;rsquo;re lucky they may give you an indication of when they expect all reviews to be in. The submission system itself usually also tells you what the status of your submission is, but these labels (like &amp;ldquo;With Editor&amp;rdquo;, &amp;ldquo;Under Review&amp;rdquo;, &amp;ldquo;Waiting for Editorial Decision&amp;rdquo;) usually do not tell you very much. Your paper can be &amp;ldquo;Under Review&amp;rdquo; the moment the first reviewer accepts, but then the editor may have considerable trouble finding the second one, who knows&amp;hellip;?!?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reviews-are-in-now-what&#34;&gt;Reviews are in! Now what?&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;It&amp;rsquo;s a random Friday afternoon, you&amp;rsquo;re just about to close your laptop for the weekend, when you see this email appear in your inbox, with the obscure subject &amp;ldquo;Decision on XYZ-01234&amp;rdquo;. &lt;strong&gt;What do you do?&lt;/strong&gt; Do you open it immediately, do you leave it be for the weekend, do you forward it to your supervisors without opening?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I probably have never read a decision letter and felt delighted afterwards. Here&amp;rsquo;s a project you&amp;rsquo;ve spent years of your time on and now these anonymous strangers get to shoot at it! Do you really want the reviews to spoil your weekend? Why not leave it for now and book a timeslot in your agenda to carefully read it in full?&lt;/p&gt;
&lt;p&gt;After the first read, it can really take some mental effort to take a deep breath and think about the reviews in a constructive manner. It may help to ask your supervisors and/or co-authors &amp;ndash; who may have had the pleasure of receiving reviews more often than you have &amp;ndash; for their opinions and perspectives. Forward the entire decision letter to them asap and schedule a meeting to go through it together, and in the meanwhile let the general impression sink in. Take some distance, there&amp;rsquo;s no real rush; leave it be for a while, take a few days to consider different perspectives. Keep in mind that &amp;lsquo;major revisions&amp;rsquo; does not mean &amp;rsquo;this study has major flaws&amp;rsquo; but is in practice the standard status of any manuscript sent back for revisions. In fact, in my (by now 12-year-long) academic career, I&amp;rsquo;ve received a &amp;lsquo;minor revisions&amp;rsquo; decision only once. All in all, this may help to avoid becoming grumpy and defensive, to put things into perspective, and adopt a constructive mindset.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How to respond?&lt;/strong&gt; Well, on a practical level, copy the entire decision letter into a new document. Then paste &lt;strong&gt;***RESPONSE #1***&lt;/strong&gt; beneath the first comment, &lt;strong&gt;***RESPONSE #2***&lt;/strong&gt; below the second, etc. Then go through the document and write down how you&amp;rsquo;d like to respond to individual comments. This can be really informal at first (&lt;em&gt;&amp;ldquo;Need to look up refs&amp;rdquo;&lt;/em&gt;) but it helps to get an idea of how much work needs to be done. Also create a copy of the original manuscript, rename it, and start Track Changes to keep track of what&amp;rsquo;s been changed (useful for you and useful for co-authors).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tips n tricks:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Number your responses. This makes it easier to refer back to points already addressed earlier. Do not respond &amp;ldquo;Please see our earlier response above&amp;rdquo; but be specific: &amp;ldquo;Please see our Response #9&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Use &amp;lsquo;fields&amp;rsquo; in Word. Instead of manually typing from 1 to 134, copy the same statement including a field below each comment, and hit CTRL-A and then F9 to automatically update the fields.&lt;/li&gt;
&lt;li&gt;Use a different font or color for your responses so it is immediately clear what was the original comment and what is your response. Note however that some journals do not allow you to upload a &amp;lsquo;Response to reviewers&amp;rsquo; document but instead ask you to copy your responses into a plain text box. This will remove any formatting, so also use *** or ### to mark your responses.&lt;/li&gt;
&lt;li&gt;Look up a paper in a journal with open reviews (e.g., recent papers in &lt;em&gt;eLife, Royal Society Open Science, Nature Communications&lt;/em&gt;). You&amp;rsquo;ll be able to see how others have dealt with critical comments and still managed to get their paper published!&lt;/li&gt;
&lt;li&gt;Make your reviewers&amp;rsquo; life easy: add page numbers to every response that involved changes in the manuscript. Tip: initially use something easily findable, like &lt;code&gt;#pagenumber&lt;/code&gt;, and only replace these with the actual page numbers at the very end, right before you resubmit.&lt;/li&gt;
&lt;li&gt;Also: keep it snappy. Don&amp;rsquo;t respond to single-line comments with pages-and-pages of words. Also don&amp;rsquo;t spend too many words thanking reviewers at the beginning of each and every response. Instead, keep it efficient. Jump straight into the issue without further ado. Reviewers, when receiving your rebuttal, will want to know how you handled their questions and critiques; they don&amp;rsquo;t want to have to wade through lines and lines of (sometimes feigned) gratitude.&lt;/li&gt;
&lt;li&gt;Your responses to substantial comments are best also included (likely in a trimmed down version) in the manuscript. Remember that, for most journals, the reviews aren&amp;rsquo;t open. This means that the points raised by the reviewers and your responses to them aren&amp;rsquo;t accessible to other readers. Still, they may have the same questions or concerns as your reviewers. Therefore, aim for an accurate and complete published record in which you explicitly acknowledge critical perspectives (&amp;ldquo;An anonymous reviewer mentioned that&amp;hellip;&amp;rdquo;) and your responses to them (&amp;ldquo;However, we should point out that&amp;hellip;&amp;rdquo;).&lt;/li&gt;
&lt;li&gt;Give your reviewers the benefit of the doubt. Try to see the whole reviewing entreprise as something collaborative (&lt;em&gt;&amp;hellip;we&amp;rsquo;re all in this together to make it the best possible paper&lt;/em&gt;) instead of something competitive (&lt;em&gt;&amp;hellip;how can we make the author&amp;rsquo;s life as difficult as possible?&lt;/em&gt;). These annoying and troublesome strangers actually voluntarily invested a few hours if not days reading and thinking about your paper. Imagine them receiving your responses after several months have gone by, not exactly remembering what was wrong with the manuscript and why, only to find multiple &amp;rsquo;no-thank-you&amp;rsquo;s from you. Even if you are entirely correct, this would already annoy even the most constructive of reviewers. Take them seriously and show you&amp;rsquo;ve given their comments considerable thought.&lt;/li&gt;
&lt;li&gt;In the unlikely event you receive comments that are outright disrespectful, don&amp;rsquo;t respond in the same tone of voice. Be polite, courteous, and constructive; that way you will likely change the tone of the discussion. At the same time, you demonstrate to the editor &amp;ndash; who in the end is the one who makes the final decision on your submission &amp;ndash; that you have considered the criticism seriously and have responded in a mature manner, winning them over to your side.&lt;/li&gt;
&lt;li&gt;In the likely event you receive particularly useful and insightful comments, do feel free to explicitly express your gratitude (e.g., &amp;ldquo;We found this comment particularly insightful because XYZ&amp;rdquo;). However, don&amp;rsquo;t overdo it. Only include a handful of these thank-you-notes and why not try to distribute them equally across all reviewers, not to upset anyone&amp;hellip;&lt;/li&gt;
&lt;li&gt;If after reading and re-reading and discussing the reviews with your co-authors, some comment remains unclear to you, do get in touch with the editor to ask for clarification. &lt;strong&gt;Do not ask&lt;/strong&gt; &amp;ldquo;if we only do ABC and not XYZ, will you then accept our paper?&amp;rdquo;. Rather, &amp;ldquo;we do not fully understand this comment about ABC because XYZ. Could you please clarify what criticism this comment raises?&amp;rdquo;. Do not get in touch with reviewers directly if you know their identity.&lt;/li&gt;
&lt;li&gt;Finally, if your paper is reviewed but rejected by the editor, do revise your manuscript before sending it out to another journal. Don&amp;rsquo;t simply send out the old manuscript to a new journal; it could be the new journal happens to invite (some of) the same reviewers as before. But even if they don&amp;rsquo;t, the initial reviewers may see the paper out in print in another journal some day, still including some of the same mistakes or misinterpretations they pointed out initially. Again, take their comments seriously, revise, and then resubmit.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Happy publishing!&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lombard speech</title>
      <link>https://hrbosker.github.io/demos/lombard-speech/</link>
      <pubDate>Mon, 04 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/demos/lombard-speech/</guid>
      <description>&lt;h2 id=&#34;lets-do-a-little-test&#34;&gt;Let&amp;rsquo;s do a little test&amp;hellip;&lt;/h2&gt;
&lt;p&gt;First, let&amp;rsquo;s take care of your audio settings:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;put on your headphones/ear buds/speakers&lt;/li&gt;
&lt;li&gt;turn your volume way down&lt;/li&gt;
&lt;li&gt;play this sound&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;SOME WHITE NOISE&amp;hellip;






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/lombard-speech/whitenoise.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&amp;hellip;and adjust your volume until it&amp;rsquo;s at a &lt;strong&gt;loud but still comfortable&lt;/strong&gt; level.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;OK, now we&amp;rsquo;ll do a short reading test:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;play the video below&lt;/li&gt;
&lt;li&gt;you&amp;rsquo;ll see a counter counting down from 3&amp;hellip;&lt;/li&gt;
&lt;li&gt;&amp;hellip;and then it will present a simple sentence on screen&lt;/li&gt;
&lt;li&gt;your task is simply to &lt;strong&gt;read out the sentence &lt;em&gt;aloud&lt;/em&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ready? Go!&lt;/p&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/lombard-speech/lombard_test.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;p&gt;Why thank you! OK, now let&amp;rsquo;s do this again. Make sure to &lt;strong&gt;keep wearing your headphones&lt;/strong&gt;, play the next video, and read out the sentence aloud.&lt;/p&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/lombard-speech/lombard_test_noise.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;p&gt;&lt;em&gt;Surprise!&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;whats-going-on&#34;&gt;What&amp;rsquo;s going on?&lt;/h2&gt;
&lt;p&gt;Perhaps you noticed your voice sounding somewhat different the second time, when there was loud babble from 16 other talkers playing in your ears, compared to the first time (in quiet). This phenomenon is called &lt;strong&gt;Lombard speech&lt;/strong&gt; (or: &lt;em&gt;Lombard effect&lt;/em&gt;; &lt;em&gt;Lombard reflex&lt;/em&gt;). It&amp;rsquo;s the type of speech people produce when speaking in noise.&lt;/p&gt;
&lt;p&gt;But perhaps you didn&amp;rsquo;t quite hear yourself all too well because it&amp;rsquo;s hard to listen to your own voice when there&amp;rsquo;s other sounds around. So here&amp;rsquo;s two clips from a male speaker of British English (and a rather posh one, if I may say so&amp;hellip;) giving you some really useful dietary advice. The first is from when he was &lt;strong&gt;speaking in quiet&lt;/strong&gt;: this is called &amp;lsquo;plain speech&amp;rsquo;. The second clip is a recording of the same sentence but this time the talker heard loud noise over headphones, &lt;strong&gt;speaking in noise&lt;/strong&gt;: &amp;lsquo;Lombard speech&amp;rsquo;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PLAIN SPEECH&lt;/strong&gt;






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/lombard-speech/MKH800_19_0005.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LOMBARD SPEECH&lt;/strong&gt;






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/lombard-speech/MKH800_25_0005.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;retrieved from the &lt;a href=&#34;https://datashare.ed.ac.uk/handle/10283/343&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Acted clear speech corpus&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;what-is-lombard-speech&#34;&gt;What is Lombard speech?&lt;/h2&gt;
&lt;p&gt;In the clips above, you can clearly hear the difference between &amp;lsquo;plain speech&amp;rsquo; and &amp;lsquo;Lombard speech&amp;rsquo;. And this speaker is not the odd-one-out: many vocal learning species, like dolphins, seals, and birds adjust their vocalizations when encountering noise.&lt;/p&gt;
&lt;p&gt;In humans, Lombard speech sounds louder, higher pitched, is a little slower, with more pronounced higher frequencies, and clearer vowels. In our own research, we demonstrated that Lombard speech is also more rhythmic, having a stronger &amp;lsquo;beat&amp;rsquo; to it compared to plain speech (see &lt;a href=&#34;#relevant-papers&#34;&gt;refs&lt;/a&gt; below).&lt;/p&gt;
&lt;h2 id=&#34;lombard-speech-rulez&#34;&gt;Lombard speech rulez&lt;/h2&gt;
&lt;p&gt;Speech perception studies have demonstrated that these &amp;lsquo;vocal adjustments&amp;rsquo; people make when speaking in noise actually have a purpose: they make you more intelligible! When you take the plain and Lombard clips above, scale their intensities to be exactly the same, and then mix them with loud babble, this is what you get:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PLAIN SPEECH in BABBLE&lt;/strong&gt; (SNR = -6 dB)






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/lombard-speech/noise_plain_snr_m6.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LOMBARD SPEECH in BABBLE&lt;/strong&gt; (SNR = -6 dB)






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/lombard-speech/noise_lombard_snr_m6.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You may experience that it&amp;rsquo;s easier to pick out the male target talker from the babble in the second (Lombard) clip than in the first (plain) clip. Apparently, &amp;lsquo;speaking up&amp;rsquo; actually helps!&lt;/p&gt;
&lt;h2 id=&#34;why-is-this-important&#34;&gt;Why is this important?&lt;/h2&gt;
&lt;p&gt;Lombard speech is more intelligible in noise than plain speech. This means that speech researchers can &amp;lsquo;borrow&amp;rsquo; acoustic aspects of Lombard speech to boost speech intelligibility, for instance in hearing aids. So next time you wanna make sure your message comes across in that busy bar, you&amp;rsquo;d better boost your F0, raise your spectral tilt, and increase your vowel dispersion; got it?!&lt;/p&gt;
&lt;h2 id=&#34;relevant-papers&#34;&gt;Relevant papers&lt;/h2&gt;
&lt;blockquote&gt;










  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/martin-cooke/&#34;&gt;Martin Cooke&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2018).
  &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2018-jasa/&#34;&gt;Talkers produce more pronounced amplitude modulations when speaking in noise&lt;/a&gt;.
  &lt;em&gt;The Journal of the Acoustical Society of America, 143&lt;/em&gt;(2), EL121-EL126, doi:10.1121/1.5024404.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_2537243_6/component/file_2554157/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/bosker-etal-2018-jasa/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;













&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1121/1.5024404&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;
&lt;blockquote&gt;










  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/martin-cooke/&#34;&gt;Martin Cooke&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2020).
  &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2020-jasa/&#34;&gt;Enhanced amplitude modulations contribute to the Lombard intelligibility benefit: Evidence from the Nijmegen Corpus of Lombard Speech&lt;/a&gt;.
  &lt;em&gt;The Journal of the Acoustical Society of America, 147&lt;/em&gt;(2), 721-730, doi:10.1121/10.0000646.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3186181_3/component/file_3186182/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/bosker-etal-2020-jasa/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://hdl.handle.net/1839/21ee5744-b5dc-4eed-9693-c37e871cdaf6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1121/10.0000646&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title> Tracking talker-specific cues to lexical stress: Evidence from perceptual learning</title>
      <link>https://hrbosker.github.io/publication/severijnen-etal-2023-jephpp/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/severijnen-etal-2023-jephpp/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Come join us at the &#39;Dag van de Fonetiek&#39;</title>
      <link>https://hrbosker.github.io/news/22-12-15-dagvdfonetiek2022/</link>
      <pubDate>Thu, 15 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/22-12-15-dagvdfonetiek2022/</guid>
      <description>&lt;p&gt;&lt;strong&gt;This &amp;lsquo;Phonetics Day&amp;rsquo; is the annual meeting of the &amp;lsquo;Dutch Society for Phonetic Sciences&amp;rsquo; [NVFW], taking place in Utrecht on December 16, 2022.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;open-to-all&#34;&gt;Open to all!&lt;/h2&gt;
&lt;p&gt;The &amp;lsquo;Dag van de Fonetiek&amp;rsquo; 2022 will take place on &lt;strong&gt;Friday Dec 16, 2022&lt;/strong&gt;, in the Sweelinckzaal at Drift 21, Utrecht, The Netherlands. It is an event celebrating everything &amp;lsquo;speechy&amp;rsquo; and is free and open to all: members, non-members, scientists, students, anyone! This year, you can hear &lt;a href=&#34;https://hrbosker.github.io/author/ronny-bujok/&#34;&gt;Ronny Bujok&lt;/a&gt; talk about whether beat gestures recalibrate lexical stress perception, and &lt;a href=&#34;https://hrbosker.github.io/author/orhun-ulusahin/&#34;&gt;Orhun Ulu≈üahin&lt;/a&gt; has some intriguing results about how listeners track a talker&amp;rsquo;s pitch!&lt;/p&gt;
&lt;p&gt;See &lt;a href=&#34;https://www.nvfw.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.nvfw.org/&lt;/a&gt; for the full program. We hope to see you there!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Paper accepted in JEP:HPP!</title>
      <link>https://hrbosker.github.io/news/22-12-12-paper-jephpp/</link>
      <pubDate>Mon, 12 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/22-12-12-paper-jephpp/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Congratulazioni a Giulio e Giuseppe for successfully publishing their collaborative project &amp;ldquo;Tracking talker-specific cues to lexical stress: Evidence from perceptual learning&amp;rdquo;!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;accepted&#34;&gt;Accepted!&lt;/h2&gt;
&lt;p&gt;Today we heard that the paper &amp;ldquo;Tracking talker-specific cues to lexical stress: Evidence from perceptual learning&amp;rdquo; has been accepted for publication in the &lt;em&gt;Journal of Experimental Psychology: Human Perception and Performance (JEP:HPP)&lt;/em&gt;, authored by &lt;a href=&#34;https://hrbosker.github.io/author/giulio-severijnen/&#34;&gt;Giulio Severijnen&lt;/a&gt;, Giuseppe Di Dona, &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;, and James McQueen. The fulltext and all data are publicly available at the links provided at the bottom of this post.&lt;/p&gt;
&lt;h2 id=&#34;whats-it-about&#34;&gt;What&amp;rsquo;s it about?&lt;/h2&gt;
&lt;p&gt;The joint first-authors Giulio and Giuseppe set out to test whether listeners track how different talkers cue lexical stress. They first exposed two groups of listeners to two talkers: Talker A and B. These two talkers consistently used different cues to signal lexical stress in Dutch (e.g., differentiating &lt;em&gt;PLAto&lt;/em&gt; from &lt;em&gt;plaTEAU&lt;/em&gt;). Group 1 always heard Talker A use F0 to cue stressed syllables in Dutch, while Talker B always used intensity. Conversely, Group 2 heard the reverse talker-cue mappings: Talker A always used intensity, and Talker B always F0.&lt;/p&gt;
&lt;p&gt;After this (admittedly strange) exposure phase, participants were given an (admittedly even stranger) test phase. They were presented with audio recordings from the two talkers but this time the F0 and intensity cues had been artificially manipulated to &amp;lsquo;point in different directions&amp;rsquo;. For instance, while F0 would clearly cue stress on the first syllable of the word, intensity cues would signal stress on the second syllable. Critically, these &amp;lsquo;mixed items&amp;rsquo; were perceived by listeners according to the talker-cue mappings they had learnt during exposure. That is, Group 1 had learnt that Talker A always used F0 in the exposure phase and therefore, at test, when they heard Talker A produce a mixed item, they were more likely to perceive stress on the syllable marked by F0. However, Group 2 was more likely to perceive the exact same mixed item as having stress on the syllable marked by intensity.&lt;/p&gt;
&lt;h2 id=&#34;why-is-this-important&#34;&gt;Why is this important?&lt;/h2&gt;
&lt;p&gt;These findings support Bayesian models of spoken word recognition. These predict that listeners can adjust their prior beliefs about the perceptual weight of different phonetic cues on the basis of short-term regularities in a talker-specific fashion. This had already been observed for segmental contrasts (e.g., the perception of different consonants and vowels). Now we demonstrate that people also track suprasegmental variability in prosody, such as lexical stress.&lt;/p&gt;
&lt;h2 id=&#34;full-reference&#34;&gt;Full reference&lt;/h2&gt;
&lt;blockquote&gt;










  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/giulio-severijnen/&#34;&gt;Giulio Severijnen&lt;/a&gt;&lt;/span&gt;&lt;i class=&#34;author-notes fas fa-info-circle&#34; data-toggle=&#34;tooltip&#34; title=&#34;Equal contribution&#34;&gt;&lt;/i&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/giuseppe-di-dona/&#34;&gt;Giuseppe Di Dona&lt;/a&gt;&lt;/span&gt;&lt;i class=&#34;author-notes fas fa-info-circle&#34; data-toggle=&#34;tooltip&#34; title=&#34;Equal contribution&#34;&gt;&lt;/i&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/james-m.-mcqueen/&#34;&gt;James M. McQueen&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2023).
  &lt;a href=&#34;https://hrbosker.github.io/publication/severijnen-etal-2023-jephpp/&#34;&gt; Tracking talker-specific cues to lexical stress: Evidence from perceptual learning&lt;/a&gt;.
  &lt;em&gt;Journal of Experimental Psychology: Human Perception and Performance&lt;/em&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3479620_2/component/file_3484213/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/severijnen-etal-2023-jephpp/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/dczx9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;












&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>GESPIN 2023 in Nijmegen</title>
      <link>https://hrbosker.github.io/news/22-11-01-gespin2023/</link>
      <pubDate>Tue, 01 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/22-11-01-gespin2023/</guid>
      <description>&lt;p&gt;&lt;strong&gt;The Gesture and Speech in Interaction [GESPIN] conference is coming to Nijmegen on September 13-15, 2023.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;broadening-perspectives-integrating-views&#34;&gt;Broadening perspectives, integrating views&lt;/h2&gt;
&lt;p&gt;&amp;hellip;that is the theme of the 2023 edition. This promises a highly interdisciplinary event, approaching the interaction of gesture and speech from the perspectives of language development, neurobiology, biomechanics, animal models, and many other fields. Keynotes are: Nuria Esteve Gibert, Yifei He, Susanne Fuchs, and Franz Goller. It is co-organized by CLS, the Donders Institute, and MPI. Paper submission opens January 10th, 2023 and the deadline is &lt;strong&gt;March 15th, 2023&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;See &lt;a href=&#34;https://gespin2023.nl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gespin2023.nl&lt;/a&gt; for all the details. We hope to see you there!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Acoustic correlates of Dutch lexical stress re-examined: Spectral tilt is not always more reliable than intensity</title>
      <link>https://hrbosker.github.io/publication/severijnen-etal-2022-speechprosody/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/severijnen-etal-2022-speechprosody/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Encoding speechrate in challenging listening conditions: white noise and reverberation</title>
      <link>https://hrbosker.github.io/publication/reinisch-bosker-app/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/reinisch-bosker-app/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Evidence for selective adaptation and recalibration in the perception of lexical stress</title>
      <link>https://hrbosker.github.io/publication/bosker-2022-langspeech/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-2022-langspeech/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>Visible lexical stress cues on the face do not influence audiovisual speech perception</title>
      <link>https://hrbosker.github.io/publication/bujok-etal-2022-sp/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bujok-etal-2022-sp/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>A tool for efficient and accurate segmentation of speech data: Announcing POnSS</title>
      <link>https://hrbosker.github.io/publication/rodd-etal-2021-brm/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/rodd-etal-2021-brm/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>Beat gestures influence which speech sounds you hear</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2021-procroysocb/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2021-procroysocb/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Discourse markers activate their, like, cohort competitors</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2021-discproc/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2021-discproc/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Listeners track talker-specific prosody to deal with talker-variability</title>
      <link>https://hrbosker.github.io/publication/severijnen-etal-2021-brainres/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/severijnen-etal-2021-brainres/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>The contribution of amplitude modulations in speech to perceived charisma</title>
      <link>https://hrbosker.github.io/publication/bosker-2021-bookchapter/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-2021-bookchapter/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Using fuzzy string matching for automated assessment of listener transcripts in speech intelligibility studies</title>
      <link>https://hrbosker.github.io/publication/bosker-2021-brm/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-2021-brm/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Biasing the perception of spoken words with transcranial alternating current stimulation</title>
      <link>https://hrbosker.github.io/publication/kosem-etal-2020-jcn/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/kosem-etal-2020-jcn/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Contextual speech rate influences morphosyntactic prediction and integration</title>
      <link>https://hrbosker.github.io/publication/kaufeld-etal-2020-lcn/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/kaufeld-etal-2020-lcn/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>Control of speaking rate is achieved by switching between qualitatively distinct cognitive ‚Äògaits‚Äô: Evidence from simulation</title>
      <link>https://hrbosker.github.io/publication/rodd-etal-2020-psychreview/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/rodd-etal-2020-psychreview/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>Enhanced amplitude modulations contribute to the Lombard intelligibility benefit: Evidence from the Nijmegen Corpus of Lombard Speech</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2020-jasa/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2020-jasa/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Eye-tracking the time course of distal and global speech rate effects</title>
      <link>https://hrbosker.github.io/publication/maslowski-etal-2020-jephpp/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/maslowski-etal-2020-jephpp/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Fluency in dialogue: Turn‚Äêtaking behavior shapes perceived fluency in native and nonnative speech</title>
      <link>https://hrbosker.github.io/publication/vanos-etal-2020-ll/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/vanos-etal-2020-ll/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>How visual cues to speech rate influence speech perception</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2020-qjep/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2020-qjep/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Knowledge-based and signal-based cues are weighted flexibly during spoken language comprehension</title>
      <link>https://hrbosker.github.io/publication/kaufeld-etal-2020-jeplmc/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/kaufeld-etal-2020-jeplmc/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Linguistic structure and meaning organize neural oscillations into a content-specific hierarchy</title>
      <link>https://hrbosker.github.io/publication/kaufeld-etal-2020-jneuro/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/kaufeld-etal-2020-jneuro/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Spectral contrast effects are modulated by selective attention in ‚Äòcocktail party‚Äô settings</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2020-app/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2020-app/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Temporal contrast effects in human speech perception are immune to selective attention</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2020-scirep/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2020-scirep/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Counting ‚Äòuhm‚Äôs: how tracking the distribution of native and non-native disfluencies influences online language comprehension</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2019-jml/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2019-jml/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Deriving the onset and offset times of planning units from acoustic and articulatory measurements</title>
      <link>https://hrbosker.github.io/publication/rodd-etal-2019-jasa/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/rodd-etal-2019-jasa/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>How the tracking of habitual rate influences speech perception</title>
      <link>https://hrbosker.github.io/publication/maslowski-etal-2019-jeplmc/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/maslowski-etal-2019-jeplmc/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Listeners normalize speech for contextual speech rate even without an explicit recognition task</title>
      <link>https://hrbosker.github.io/publication/maslowski-etal-2019-jasa/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/maslowski-etal-2019-jasa/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Entrained theta oscillations guide perception of subsequent speech: Behavioral evidence from rate normalization</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2018-lcn/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2018-lcn/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Linguistic expectation management in online discourse processing: An investigation of Dutch inderdaad ‚Äòindeed‚Äô and eigenlijk ‚Äòactually‚Äô</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2018-jml/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2018-jml/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>Listening to yourself is special: Evidence from global speech rate tracking</title>
      <link>https://hrbosker.github.io/publication/maslowski-etal-2018/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/maslowski-etal-2018/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Neural entrainment determines the words we hear</title>
      <link>https://hrbosker.github.io/publication/kosem-etal-2018-currbiol/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/kosem-etal-2018-currbiol/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>Putting Laurel and Yanny in context</title>
      <link>https://hrbosker.github.io/publication/bosker-2018-jasa/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-2018-jasa/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Talkers produce more pronounced amplitude modulations when speaking in noise</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2018-jasa/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2018-jasa/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>Accounting for rate-dependent category boundary shifts in speech perception</title>
      <link>https://hrbosker.github.io/publication/bosker-2017-app/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-2017-app/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>An entrained rhythm‚Äôs frequency, not phase, influences temporal sampling of speech</title>
      <link>https://hrbosker.github.io/publication/bosker-kosem-2017-interspeech/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-kosem-2017-interspeech/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Cognitive load makes speech sound fast, but does not modulate acoustic context effects</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2017-jml/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2017-jml/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT --&gt;
</description>
    </item>
    
    <item>
      <title>Foreign languages sound fast: evidence from implicit rate normalization</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2017-frontiers/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2017-frontiers/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>How our own speech rate influences our perception of others</title>
      <link>https://hrbosker.github.io/publication/bosker-2017-jeplmc/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-2017-jeplmc/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>The role of temporal amplitude modulations in the political arena: Hillary Clinton vs. Donald Trump</title>
      <link>https://hrbosker.github.io/publication/bosker-2017-interspeech/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-2017-interspeech/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Whether long-term tracking of speech rate affects perception depends on who is talking</title>
      <link>https://hrbosker.github.io/publication/maslowski-etal-2017-interspeech/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/maslowski-etal-2017-interspeech/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Listening under cognitive load makes speech sound fast</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2016-spire/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2016-spire/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Our own speech rate influences speech perception</title>
      <link>https://hrbosker.github.io/publication/bosker-2016-sp/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-2016-sp/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Both native and non-native disfluencies trigger listeners‚Äô attention</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2015/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2015/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Normalization for speechrate in native and nonnative speech</title>
      <link>https://hrbosker.github.io/publication/bosker-reinisch-2015/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-reinisch-2015/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Native ‚Äòum‚Äôs elicit prediction of low-frequency referents, but non-native ‚Äòum‚Äôs do not</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2014-jml/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2014-jml/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>Native speakers‚Äô perceptions of fluency and accent in L2 speech</title>
      <link>https://hrbosker.github.io/publication/pinget-etal-2014-lt/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/pinget-etal-2014-lt/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>Perceptual adaptation to segmental and syllabic reductions in continuous spoken Dutch</title>
      <link>https://hrbosker.github.io/publication/poellmann-etal-2014-jphon/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/poellmann-etal-2014-jphon/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>The perception of fluency in native and nonnative speech</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2014-ll/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2014-ll/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>The processing and evaluation of fluency in native and non-native speech</title>
      <link>https://hrbosker.github.io/publication/bosker-2014-thesis/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-2014-thesis/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>Choosing a threshold for silent pauses to measure second language fluency</title>
      <link>https://hrbosker.github.io/publication/dejong-bosker-2013/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/dejong-bosker-2013/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Juncture (prosodic)</title>
      <link>https://hrbosker.github.io/publication/bosker-2013-ehll-juncture/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-2013-ehll-juncture/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Sibilant consonants</title>
      <link>https://hrbosker.github.io/publication/bosker-2013-ehll-sibilants/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-2013-ehll-sibilants/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>What makes speech sound fluent? The contributions of pauses, speed and repairs</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2013-lt/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2013-lt/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>Whispered speech as input for cochlear implants</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2010/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2010/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://hrbosker.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://hrbosker.github.io/contact/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/contact/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://hrbosker.github.io/empty/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/empty/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://hrbosker.github.io/googled886c90778b4364a/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/googled886c90778b4364a/</guid>
      <description>google-site-verification: googled886c90778b4364a.html</description>
    </item>
    
    <item>
      <title></title>
      <link>https://hrbosker.github.io/people/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/people/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Research</title>
      <link>https://hrbosker.github.io/research/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/research/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
