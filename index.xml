<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SPEAC | Hans Rutger Bosker</title>
    <link>https://hrbosker.github.io/</link>
      <atom:link href="https://hrbosker.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>SPEAC | Hans Rutger Bosker</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 08 Jul 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://hrbosker.github.io/media/logo_hu4ee931c5040e3d0fbcac8149a377b250_283935_300x300_fit_lanczos_3.png</url>
      <title>SPEAC | Hans Rutger Bosker</title>
      <link>https://hrbosker.github.io/</link>
    </image>
    
    <item>
      <title>Manual McGurk effect</title>
      <link>https://hrbosker.github.io/demos/manual-mcgurk/</link>
      <pubDate>Thu, 07 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/demos/manual-mcgurk/</guid>
      <description>&lt;h2 id=&#34;do-you-hear-voornaam-or-voornaam&#34;&gt;Do you hear &lt;em&gt;VOORnaam&lt;/em&gt; or &lt;em&gt;voorNAAM&lt;/em&gt;?&lt;/h2&gt;
&lt;p&gt;In the video below, you&amp;rsquo;ll see a (randomly selected&amp;hellip;) talker say a Dutch word. Is he saying &lt;em&gt;VOORnaam&lt;/em&gt; (Eng. &amp;ldquo;first name&amp;rdquo;, with stress on the first syllable &lt;em&gt;VOOR-&lt;/em&gt;) or &lt;em&gt;voorNAAM&lt;/em&gt; (Eng. &amp;ldquo;respectable&amp;rdquo;, with stress on the second syllable &lt;em&gt;-NAAM&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;In other words: &lt;strong&gt;where do you hear the stress?&lt;/strong&gt;&lt;/p&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/manual-mcgurk/voornaam_AV_swbeat_sw_4.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;blockquote&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;&lt;em&gt;Spoiler: click here to reveal the video specs&lt;/em&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Audio:&lt;/strong&gt; &lt;em&gt;ambiguous&lt;/em&gt;; midway between &lt;em&gt;VOORnaam&lt;/em&gt; &amp;ndash; &lt;em&gt;voorNAAM&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lips:&lt;/strong&gt; head taken from a recording of &lt;em&gt;VOORnaam&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hands:&lt;/strong&gt; beat gesture aligned to the &lt;strong&gt;first&lt;/strong&gt; syllable&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now play the video below. &lt;strong&gt;Where do you hear the stress now?&lt;/strong&gt;&lt;/p&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/manual-mcgurk/voornaam_AV_wsbeat_sw_4.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;blockquote&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;&lt;em&gt;Spoiler: click here to reveal the video specs&lt;/em&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Audio:&lt;/strong&gt; &lt;em&gt;ambiguous&lt;/em&gt;; midway between &lt;em&gt;VOORnaam&lt;/em&gt; &amp;ndash; &lt;em&gt;voorNAAM&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lips:&lt;/strong&gt; head taken from a recording of &lt;em&gt;VOORnaam&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hands:&lt;/strong&gt; beat gesture aligned to the &lt;strong&gt;second&lt;/strong&gt; syllable&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;explanation&#34;&gt;Explanation&lt;/h2&gt;
&lt;p&gt;The audio in these videos is perfectly identical: it has been manipulated to be ambiguous, falling roughly midway between &lt;em&gt;VOORnaam&lt;/em&gt; and &lt;em&gt;voorNAAM&lt;/em&gt;. The head of the talker is also the same: it has been copy-pasted from a video recording of the talker saying &lt;em&gt;VOORnaam&lt;/em&gt;. &lt;strong&gt;The only difference between these two videos is the timing of the hand gesture.&lt;/strong&gt; In the first clip, the talker produces a beat gesture on the &lt;em&gt;first&lt;/em&gt; syllable, while in the second video the talker gestures on the &lt;em&gt;second&lt;/em&gt; syllable. Our experiments show that this slight change in timing has major consequences for perception. When we ask a group of Dutch participants to indicate what word they hear the talker say, the majority reports hearing &lt;em&gt;VOORnaam&lt;/em&gt; in the first clip, but &lt;em&gt;voorNAAM&lt;/em&gt; in the second clip.&lt;/p&gt;
&lt;blockquote&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-4&#34;&gt;
  &lt;summary&gt;&lt;em&gt;Really? Convince me&amp;hellip;&lt;/em&gt;&lt;/summary&gt;
  &lt;p&gt;This is Figure 1 from &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2021-procroysocb&#34;&gt;Bosker &amp;amp; Peeters (2021)&lt;/a&gt;. In the bottom left panel, you see the proportion of &amp;lsquo;I hear stress on the first syllable&amp;rsquo; responses for when the beat gesture falls on the first syllable (blue line) or on the second syllable (red line). The blue line lies above the red line, indicating an overall bias to report more &amp;lsquo;stress on first syllable&amp;rsquo; responses when the gesture falls on the first vs. second syllable. The difference between the lines is sizable, averaging around 20%.&lt;/p&gt;
&lt;img src=&#34;https://royalsocietypublishing.org/cms/asset/53082c59-4ac5-43d8-a411-9f5f5edda544/rspb20202419f01.jpg&#34; alt=&#34;Figure 1, Bosker &amp; Peeters 2021&#34; width=&#34;800&#34;/&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;how-hands-help-us-hear&#34;&gt;How hands help us hear&lt;/h2&gt;
&lt;p&gt;When we have a face-to-face conversation, we don&amp;rsquo;t only exchange sounds. We also move our head, hands, and body to the rhythm of the speech. &lt;em&gt;Beat gestures&lt;/em&gt; are relatively &amp;lsquo;simple&amp;rsquo; up-and-down hand gestures that are closely aligned to the rhythm of speech. They tend to fall on the stressed syllable in free-stress languages, such as English and Dutch. These videos demonstrate that people are sensitive to the timing of beat gestures, influencing lexical stress perception. In &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2021-procroysocb&#34;&gt;Bosker &amp;amp; Peeters (2021)&lt;/a&gt;, this effect was termed the &lt;strong&gt;manual McGurk effect&lt;/strong&gt;. That is, just like seeing a talker close their lips can make you hear the sound /b/ in the classic McGurk effect (&lt;a href=&#34;https://www.nature.com/articles/264746a0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;McGurk &amp;amp; McDonald, 1976&lt;/a&gt;), so can the timing of hand gestures influence speech perception in the manual McGurk effect.&lt;/p&gt;
&lt;h2 id=&#34;why-is-this-important&#34;&gt;Why is this important?&lt;/h2&gt;
&lt;p&gt;The manual McGurk effect is the first demonstration of how the timing of hand gestures influences low-level speech perception. Even the simplest flicks-of-the-hands that do not convey any particular meaning of themselves can shape what words you hear. This promises that these seemingly unimportant hand gestures contribute meaningfully to audiovisual speech comprehension. Perhaps &amp;rsquo;enriching&amp;rsquo; our speech with carefully timed gestures can help our audience understand our spoken message, particularly in challenging listening conditions, such as in noise.&lt;/p&gt;
&lt;h2 id=&#34;relevant-papers&#34;&gt;Relevant papers&lt;/h2&gt;
&lt;blockquote&gt;










  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/david-peeters/&#34;&gt;David Peeters&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2021).
  &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2021-procroysocb/&#34;&gt;Beat gestures influence which speech sounds you hear&lt;/a&gt;.
  &lt;em&gt;Proceedings of the Royal Society B: Biological Sciences, 288&lt;/em&gt;, 20202419, doi:10.1098/rspb.2020.2419.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3243428_3/component/file_3280864/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/bosker-etal-2021-procroysocb/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/b7kue/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1098/rspb.2020.2419&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;
&lt;blockquote&gt;










  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/ronny-bujok/&#34;&gt;Ronny Bujok&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/antje-s.-meyer/&#34;&gt;Antje S. Meyer&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2022).
  &lt;a href=&#34;https://hrbosker.github.io/publication/bujok-etal-2022-sp/&#34;&gt;Visible lexical stress cues on the face do not influence audiovisual speech perception&lt;/a&gt;.
  In &lt;em&gt;Proceedings of Speech Prosody 2022&lt;/em&gt; (ed. S. Frota, M. Cruz, and M. Vigário), 259-263, doi:10.21437/SpeechProsody.2022-53.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3387404_1/component/file_3387405/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/bujok-etal-2022-sp/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/um7ph&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.21437/SpeechProsody.2022-53&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Ronny Bujok, Antje S. Meyer, and Hans Rutger Bosker (2022). Audiovisual perception of lexical stress: Beat gestures are stronger visual cues for lexical stress than visible articulatory cues on the face. &lt;em&gt;PsyArXiv Preprints&lt;/em&gt;, doi:&lt;a href=&#34;https://doi.org/10.31234/osf.io/y9jck&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.31234/osf.io/y9jck&lt;/a&gt;, data:&lt;a href=&#34;https://osf.io/4d9w5/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://osf.io/4d9w5/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>First how-to</title>
      <link>https://hrbosker.github.io/resources/how-to/howto1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/resources/how-to/howto1/</guid>
      <description>&lt;p&gt;This is the subtitle.&lt;/p&gt;
&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; 1-2 hours per week, for 8 weeks&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;praatcode = 1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Some disclaimer&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Save all</title>
      <link>https://hrbosker.github.io/resources/scripts/save-all/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/resources/scripts/save-all/</guid>
      <description>&lt;p&gt;Praat can only save one object at a time for you. If you have multiple objects in your object window you&amp;rsquo;d like to save in one go, you can use this script. It can either save objects by their object name or by their id number.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You can also &lt;a href=&#34;../save-all.praat&#34;&gt;download the script&lt;/a&gt; as a .praat file.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;################################################################################
### Hans Rutger Bosker, Radboud University
### HansRutger.Bosker@ru.nl
### Date: 23 June 2022, run in Praat 6.2.12 on Windows 10
### License: CC BY-NC 4.0
################################################################################

	###&amp;gt;&amp;gt; This script saves all selected objects to the directory &amp;#39;dir_out$&amp;#39;
	###&amp;gt;&amp;gt;	with either:
	###&amp;gt;&amp;gt;   - their object name (e.g., &amp;#34;sentence1.wav&amp;#34;)
	###&amp;gt;&amp;gt;			&amp;gt; set variable &amp;#39;save_method$&amp;#39; to &amp;#34;name&amp;#34; [default]
	###&amp;gt;&amp;gt;   - their id number in the Praat object window (e.g., &amp;#34;42.wav&amp;#34;)
	###&amp;gt;&amp;gt;			&amp;gt; set variable &amp;#39;save_method$&amp;#39; to &amp;#34;id&amp;#34;
	###&amp;gt;&amp;gt;
	###&amp;gt;&amp;gt; Sounds are saved as .wav files,
	###&amp;gt;&amp;gt; other object types (TextGrids, Spectrum, etc.) are saved
	###&amp;gt;&amp;gt; with their own extension type (.TextGrid, .Spectrum).
	###&amp;gt;&amp;gt;
	###&amp;gt;&amp;gt; Default: the script will overwrite pre-existing files.
	###&amp;gt;&amp;gt; Set variable &amp;#39;overwrite$&amp;#39; to &amp;#34;no&amp;#34; if you want Praat
	###&amp;gt;&amp;gt; to throw an error instead.



################################################################################
### Variables you will definitely need to customize:
################################################################################

### Where should the selected objects be saved?

dir_out$ = &amp;#34;C:\Users\hanbos\mysounds&amp;#34;

### Should Praat overwrite pre-existing files?

overwrite$ = &amp;#34;yes&amp;#34;
#overwrite$ = &amp;#34;no&amp;#34;

### Do you want to save each object by its object name or by its id number?
### If object name, then use &amp;#34;name&amp;#34; (e.g., &amp;#34;sentence1.wav&amp;#34;).
### If object id number, then use &amp;#34;id&amp;#34; (e.g., &amp;#34;42.wav&amp;#34;).

save_method$ = &amp;#34;name&amp;#34;
#save_method$ = &amp;#34;id&amp;#34;





################################################################################
### Before we start, let&amp;#39;s check whether you&amp;#39;ve entered sensible
### input for the variables above...
################################################################################

### Let&amp;#39;s check if the output directory exists.
### This script will throw an error if the directory doesn&amp;#39;t exist
### (i.e., it won&amp;#39;t write to a mysterious temp directory).

### First check whether the input directory ends in a backslash (if so, removed)

if right$(dir_out$,1)=&amp;#34;/&amp;#34;
	dir_out$ = left$(dir_out$,length(dir_out$)-1)
elsif right$(dir_out$,1)=&amp;#34;\&amp;#34;
	dir_out$ = left$(dir_out$,length(dir_out$)-1)
endif

### Then create a temporary txt file in the folder
### and try to write it to the output folder.

### NOTE: The &amp;#34;nocheck&amp;#34; below asks Praat not to complain if the folder
### does *not* exist. We&amp;#39;ll manually check whether the saving of this
### temp txt file has succeeded or not further down below.

temp_filename$ = dir_out$ + &amp;#34;/&amp;#34; + &amp;#34;my_temporary_Praat_file.txt&amp;#34;
nocheck writeFileLine: temp_filename$, &amp;#34;This is just to check if the directory exists&amp;#34;

### Can the file be found?

file_exists_yesno = fileReadable(temp_filename$)

if file_exists_yesno = 1
	# if you *could* read that temp txt file,
	# this confirms that the directory is valid.
	# Then you can delete it.
	deleteFile: temp_filename$
else
	# if that file wasn&amp;#39;t readable, that means that the directory wasn&amp;#39;t valid. 
	printline The folder &amp;#39;dir_out$&amp;#39; was not found
	exit Your directory doesn&amp;#39;t exist. Check spelling. The directory must *already* exist.
endif





################################################################################
################################################################################
#################################    SCRIPT    #################################
################################################################################
################################################################################

### Make sure you&amp;#39;ve selected the objects you&amp;#39;d like to save in
### the Praat object window. If nothing is selected, the script exits.

nSelected = numberOfSelected()
if nSelected = 0
	exit No objects selected.
endif

### Store the object id numbers in an array

for thisObject to nSelected
	objectArray [&amp;#39;thisObject&amp;#39;] = selected(&amp;#39;thisObject&amp;#39;)
endfor

### Loop through this array and for each id number
### select the corresponding object and save it.

for thisArrayNumber to nSelected
	objectId = objectArray [&amp;#39;thisArrayNumber&amp;#39;]
	select &amp;#39;objectId&amp;#39;
	type$ = extractWord$(selected$(), &amp;#34;&amp;#34;)
	name$ = extractLine$(selected$(), &amp;#34; &amp;#34;)
	
	if save_method$ = &amp;#34;name&amp;#34;
		if type$ = &amp;#34;Sound&amp;#34;
			does_file_exist = fileReadable(&amp;#34;&amp;#39;dir_out$&amp;#39;\&amp;#39;name$&amp;#39;.wav&amp;#34;)
			if does_file_exist = 1
				if overwrite$ = &amp;#34;no&amp;#34;
					exit The file &amp;#39;dir_out$&amp;#39;\&amp;#39;name$&amp;#39;.wav&amp;#39; already exists! If you wish to overwrite, set the variable overwrite$ to &amp;#34;yes&amp;#34;.
				endif
			endif
			Write to WAV file... &amp;#39;dir_out$&amp;#39;\&amp;#39;name$&amp;#39;.wav
		else
			does_file_exist = fileReadable(&amp;#34;&amp;#39;dir_out$&amp;#39;\&amp;#39;name$&amp;#39;.&amp;#39;type$&amp;#39;&amp;#34;)
			if does_file_exist = 1
				if overwrite$ = &amp;#34;no&amp;#34;
					exit The file &amp;#39;dir_out$&amp;#39;\&amp;#39;name$&amp;#39;.&amp;#39;type$&amp;#39; already exists! If you wish to overwrite, set the variable overwrite$ to &amp;#34;yes&amp;#34;.
				endif
			endif
			Write to text file... &amp;#39;dir_out$&amp;#39;\&amp;#39;name$&amp;#39;.&amp;#39;type$&amp;#39;
		endif
	elsif save_method$ = &amp;#34;id&amp;#34;
		if type$ = &amp;#34;Sound&amp;#34;
			does_file_exist = fileReadable(&amp;#34;&amp;#39;dir_out$&amp;#39;\&amp;#39;objectId$&amp;#39;.wav&amp;#34;)
			if does_file_exist = 1
				if overwrite$ = &amp;#34;no&amp;#34;
					exit The file &amp;#39;dir_out$&amp;#39;\&amp;#39;objectId$&amp;#39;.wav&amp;#39; already exists! If you wish to overwrite, set the variable overwrite$ to &amp;#34;yes&amp;#34;.
				endif
			endif
			Write to WAV file... &amp;#39;dir_out$&amp;#39;\&amp;#39;objectId&amp;#39;.wav
		else
			does_file_exist = fileReadable(&amp;#34;&amp;#39;dir_out$&amp;#39;\&amp;#39;objectId$&amp;#39;.&amp;#39;type$&amp;#39;&amp;#34;)
			if does_file_exist = 1
				if overwrite$ = &amp;#34;no&amp;#34;
					exit The file &amp;#39;dir_out$&amp;#39;\&amp;#39;objectId$&amp;#39;.&amp;#39;type$&amp;#39; already exists! If you wish to overwrite, set the variable overwrite$ to &amp;#34;yes&amp;#34;.
				endif
			endif
			Write to text file... &amp;#39;dir_out$&amp;#39;\&amp;#39;objectId&amp;#39;.&amp;#39;type$&amp;#39;
		endif
	endif
endfor

### Now set the selection back to what it was before running this script.

for current to nSelected
	objectId = objectArray [&amp;#39;current&amp;#39;]
	if current = 1
		select &amp;#39;objectId&amp;#39;
	else
		plus &amp;#39;objectId&amp;#39;
	endif
endfor

################################################################################
# End of script
################################################################################
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Laurel or Yanny?</title>
      <link>https://hrbosker.github.io/demos/laurel-yanny/</link>
      <pubDate>Thu, 07 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/demos/laurel-yanny/</guid>
      <description>&lt;h2 id=&#34;same-audio-different-perception&#34;&gt;Same audio, different perception&lt;/h2&gt;
&lt;p&gt;In May 2018, social media exploded after the surfacing of &lt;a href=&#34;https://twitter.com/CloeCouture/status/996218489831473152&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;an audio clip&lt;/a&gt; that some perceived as &lt;em&gt;Laurel&lt;/em&gt;, but others as &lt;em&gt;Yanny&lt;/em&gt;. &lt;strong&gt;Listen and decide for yourself:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Laurel/Yanny &amp;ndash; [original]






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/laurel-yanny/Audio%20S7.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;#Laurelgate was quickly seen as the auditory version of &lt;a href=&#34;https://en.wikipedia.org/wiki/The_dress&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#TheDress&lt;/a&gt;, a photo going viral in 2015 of a white and gold dress, or was it black and blue? But how fixed is this divide between individuals? &lt;strong&gt;Can we turn #Yannists into #Laurelites, and vice versa?&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;higher-vs-lower-frequencies&#34;&gt;Higher vs. lower frequencies&lt;/h2&gt;
&lt;p&gt;Acoustic analysis of the original clip suggests that the higher frequencies (&amp;gt;1000 Hz) resembled the word &lt;em&gt;Yanny&lt;/em&gt;, but the lower frequencies (&amp;lt;1000 Hz) are more like &lt;em&gt;Laurel&lt;/em&gt;. This can be seen in the figure at the top of this page, where the upper part of the middle panel (&lt;em&gt;Original&lt;/em&gt;) is more like the right panel (&lt;em&gt;Yanny&lt;/em&gt;), but the lower part is more like the left panel (&lt;em&gt;Laurel&lt;/em&gt;). This is best demonstrated by artificially emphasizing/attenuating the higher vs. lower frequencies in the audio clip.&lt;/p&gt;
&lt;p&gt;In these sounds below, we gradually attenuate (&lt;em&gt;~turn down&lt;/em&gt;) the higher frequencies while we simultaneously emphasize (&lt;em&gt;~turn up&lt;/em&gt;) the lower frequencies. &lt;strong&gt;Play the sounds below, can you hear &lt;em&gt;Laurel&lt;/em&gt; turning into &lt;em&gt;Yanny&lt;/em&gt;?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;





  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/laurel-yanny/Audio%20S4.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;







  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/laurel-yanny/Audio%20S5.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;







  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/laurel-yanny/Audio%20S6.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;







  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/laurel-yanny/Audio%20S7.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;







  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/laurel-yanny/Audio%20S8.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;







  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/laurel-yanny/Audio%20S9.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;







  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/laurel-yanny/Audio%20S10.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;blockquote&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-8&#34;&gt;
  &lt;summary&gt;&lt;em&gt;Click here for audio specs&lt;/em&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Middle clip:&lt;/strong&gt; original Laurel/Yanny clip&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Manipulation:&lt;/strong&gt; filtered by 10 bandpass filters (with center frequencies: 31.5, 63, 125, 250, 500, 1000, 2000, 4000, 8000, 16000 Hz; using a Hann window with a roll-off width of 20, 20, 40, 80, 100, 100, 100, 100, 100, 100 Hz, respectively). Inverse intensity manipulation for high (&amp;gt;1000 Hz) vs. low (&amp;lt;1000 Hz) frequency bands in steps of 6 dB.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Top clip:&lt;/strong&gt; -18 dB attenuation for higher frequency bands, +18 dB emphasis for lower frequency bands.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bottom clip:&lt;/strong&gt; +18 dB emphasis for higher frequency bands, -18 dB attenuation for lower frequency bands.&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;/blockquote&gt;
&lt;p&gt;OK, so we can guide what people hear by artificially editing the higher vs. lower frequencies in the clip. &lt;strong&gt;But can we also make someone hear one and the same clip differently?&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;lets-add-laurels-telephone-number&#34;&gt;Let&amp;rsquo;s add Laurel&amp;rsquo;s telephone number&lt;/h2&gt;
&lt;p&gt;The perception of speech sounds is influenced by the surrounding acoustic context. The same sound can be perceived differently when, for instance, the acoustics of a preceding sentence are changed. Below, you will hear the original Laurel/Yanny clip, but this time preceded by a telephone number: &lt;em&gt;496-0356&lt;/em&gt;. In the first clip, we filtered out (&lt;em&gt;~removed&lt;/em&gt;) the lower frequencies in the telephone number leaving only the high frequency content. In the second clip, we filtered out the higher frequencies leaving only the low frequency content. Note: the Laurel/Yanny clip itself is identical in the two audios. &lt;strong&gt;Do you hear a different name after each telephone number?&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;High-pass&lt;/strong&gt; filtered telephone number + Laurel/Yanny






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/laurel-yanny/hi_ambig.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Low-pass&lt;/strong&gt; filtered telephone number + Laurel/Yanny






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/laurel-yanny/lo_ambig.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;numbing-your-ears&#34;&gt;Numbing your ears&lt;/h2&gt;
&lt;p&gt;In a crowd-sourced experiment with &amp;gt;500 online participants, we found that the same people were more likely to report hearing &lt;em&gt;Laurel&lt;/em&gt; for the first clip, but &lt;em&gt;Yanny&lt;/em&gt; for the second clip. This is because the high-frequency content in the telephone number in the first clip &lt;em&gt;&amp;rsquo;numbs your ears&amp;rsquo;&lt;/em&gt; for any following high-frequency content, thus making the lower frequencies stand out more, biasing perception towards &lt;em&gt;Laurel&lt;/em&gt;. And vice versa, the low-frequency content in the telephone number in the second clip &lt;em&gt;&amp;rsquo;numbs your ears&amp;rsquo;&lt;/em&gt; for any following low-frequency content, thus making the higher frequencies stand out more, biasing perception towards &lt;em&gt;Yanny&lt;/em&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-11&#34;&gt;
  &lt;summary&gt;&lt;em&gt;Really? Convince me&amp;hellip;&lt;/em&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;p&gt;This is Figure 1 from &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-2018-jasa&#34;&gt;Bosker (2018, &lt;em&gt;JASA&lt;/em&gt;)&lt;/a&gt; showing people&amp;rsquo;s responses in panel C. The blue line shows the proportion of &lt;em&gt;Yanny&lt;/em&gt; responses after a high-pass filtered telephone number (~first clip above), which is higher than the red line illustrating people&amp;rsquo;s responses for the &lt;strong&gt;same Laurel/Yanny clips&lt;/strong&gt; after a low-pass filtered telephone number (~second clip above).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://asa.scitation.org/na101/home/literatum/publisher/aip/journals/content/jas/2018/jas.2018.144.issue-6/1.5070144/20181206/images/large/1.5070144.figures.online.f2.jpeg
&#34; alt=&#34;Figure 1, Bosker 2018 JASA&#34; width=&#34;800&#34;/&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;why-is-this-important&#34;&gt;Why is this important?&lt;/h2&gt;
&lt;p&gt;These social media phenomena are great examples of how &lt;em&gt;our perception of the world is strongly context-dependent&lt;/em&gt;. What we perceive is &lt;em&gt;not&lt;/em&gt; wholly determined by the input signal alone, but also by the context in which the signal is perceived, including the sounds heard previously, our prior expectations, who is talking, etc. etc. As such, they highlight the subtle intricacies of human perception.&lt;/p&gt;
&lt;h2 id=&#34;relevant-papers&#34;&gt;Relevant papers&lt;/h2&gt;
&lt;blockquote&gt;










  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2018).
  &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-2018-jasa/&#34;&gt;Putting Laurel and Yanny in context&lt;/a&gt;.
  &lt;em&gt;The Journal of the Acoustic Society of America, 144&lt;/em&gt;(6), EL503-EL508, doi:10.1121/1.5070144.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3005418_7/component/file_3012156/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/bosker-2018-jasa/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/63wdh/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1121/1.5070144&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Annotate</title>
      <link>https://hrbosker.github.io/resources/scripts/annotate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/resources/scripts/annotate/</guid>
      <description>&lt;p&gt;This script streamlines an annotation workflow: it presents a TextGrid for manual annotation to the user, you perform some changes, and when you click Next, it automatically saves the changes and presents the next TextGrid, and so on. This is particularly useful for when you have forced aligned TextGrids (e.g., from &lt;a href=&#34;https://hrbosker.github.io/resources/other-resources/#videoaudio-editing&#34;&gt;WebMAUS&lt;/a&gt; or &lt;a href=&#34;https://hrbosker.github.io/resources/other-resources/#videoaudio-editing&#34;&gt;EasyAlign&lt;/a&gt;) that you&amp;rsquo;d like to manually evaluate and edit.&lt;/p&gt;
&lt;p&gt;Moreover, the script keeps track of who annotated what, can continue where you left off yesterday, allows users to enter comments about their annotations, and blinds file names to avoid human annotation biases. The script can be updated to present new empty TextGrids (instead of any pre-existing ones, in case you only have .wav files) or to automatically perform changes to TextGrid tiers/intervals before presenting them for manual annotation.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You can also &lt;a href=&#34;../annotate.praat&#34;&gt;download the script&lt;/a&gt; as a .praat file.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;################################################################################
### Hans Rutger Bosker, Radboud University
### HansRutger.Bosker@ru.nl
### Date: 30 June 2022, run in Praat 6.2.12 on Windows 10
### License: CC BY-NC 4.0
################################################################################

	###&amp;gt;&amp;gt; This script reads a directory containing sound files with pre-existing TextGrids,
	###&amp;gt;&amp;gt;	for instance resulting from a forced aligner (e.g., WebMAUS or EasyAlign).
	###&amp;gt;&amp;gt;	IMPORTANT: Every Sound should have a pre-existing TextGrid file **with the same name**!
	###&amp;gt;&amp;gt;	It opens every Sound + Textgrid combination, presents it to the user for editing,
	###&amp;gt;&amp;gt;	allows the user to enter comments about the annotations, and then saves the
	###&amp;gt;&amp;gt;	edited TextGrid with &amp;#34;_edited&amp;#34; suffix in the subfolder &amp;#39;edited_textgrids&amp;#39;.
	###&amp;gt;&amp;gt;	User comments are tracked in the file &amp;#39;annotation_log.txt&amp;#39; in the same subfolder.
	###&amp;gt;&amp;gt;	
	###&amp;gt;&amp;gt;	&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; If you **do not** yet have pre-existing TextGrids (i.e., only sound files),	&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;
	###&amp;gt;&amp;gt;	&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; you can adjust the script to read all .wav files, create new empty TextGrids,	&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;
	###&amp;gt;&amp;gt;	&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; and present those for editing and saving...									&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;
	###&amp;gt;&amp;gt;	&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; See the line with &amp;#34;CREATE EMPTY TEXTGRIDS&amp;#34;									&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;
	###&amp;gt;&amp;gt;
	###&amp;gt;&amp;gt; This script can be run by multiple users simultaneously, for instance when
	###&amp;gt;&amp;gt;   multiple annotators are working on the same shared folder. It keeps track
	###&amp;gt;&amp;gt;   of what files have already been edited: it only presents TextGrids for editing
	###&amp;gt;&amp;gt;   that do not yet have an &amp;#34;_edited&amp;#34; version pre-existing in the subfolder.
	###&amp;gt;&amp;gt;   This also means that users can close the script or Praat at anytime
	###&amp;gt;&amp;gt;   without losing data. Then, next time someone runs the script, it will
	###&amp;gt;&amp;gt;   start with the files that are &amp;#39;left over&amp;#39; from the previous run.
	###&amp;gt;&amp;gt;	NOTE: This checking of which files already exist can slow the script down
	###&amp;gt;&amp;gt;	when working with folders with &amp;gt;5000 files...
	###&amp;gt;&amp;gt;
	###&amp;gt;&amp;gt; At present, the script **only** presents pre-existing tiers and intervals
	###&amp;gt;&amp;gt;   for editing (e.g., adding boundaries, dragging boundaries around, etc.).
	###&amp;gt;&amp;gt;   This script can be augmented by automatically adding tiers or intervals
	###&amp;gt;&amp;gt;   before the TextGrid is presented for editing, so users can annotate
	###&amp;gt;&amp;gt;   new tiers/intervals. See the line with &amp;#34;ADD/REMOVE TIERS HERE&amp;#34;.

################################################################################
### Variables you will definitely need to customize:
################################################################################

### Where can the Sound and TextGrid files be found?

dir_in$ = &amp;#34;C:\Users\hanbos\mysounds&amp;#34;

### Do you want to use &amp;#39;blinded&amp;#39; objects in Praat to avoid human biases in annotation?
### Default value: &amp;#34;yes&amp;#34;
### Change to &amp;#34;no&amp;#34; if you want to use original object names.

blinded$ = &amp;#34;yes&amp;#34;





################################################################################
### Before we start, let&amp;#39;s check whether you&amp;#39;ve entered sensible
### input for the variables above...
################################################################################

### Let&amp;#39;s check if the input directory exists.
### This script will throw an error if the directory doesn&amp;#39;t exist
### (i.e., it won&amp;#39;t write to a mysterious temp directory).

### First check whether the input directory ends in a backslash (if so, removed)

if right$(dir_in$,1)=&amp;#34;/&amp;#34;
	dir_in$ = left$(dir_in$,length(dir_in$)-1)
elsif right$(dir_in$,1)=&amp;#34;\&amp;#34;
	dir_in$ = left$(dir_in$,length(dir_in$)-1)
endif

### Then create a temporary txt file in the folder
### and try to write it to the input folder.

### NOTE: The &amp;#34;nocheck&amp;#34; below asks Praat not to complain if the folder
### does *not* exist. We&amp;#39;ll manually check whether the saving of this
### temp txt file has succeeded or not further down below.

temp_filename$ = dir_in$ + &amp;#34;/&amp;#34; + &amp;#34;my_temporary_Praat_file.txt&amp;#34;
nocheck writeFileLine: temp_filename$, &amp;#34;This is just to check if the directory exists&amp;#34;

### Can the file be found?

file_exists_yesno = fileReadable(temp_filename$)

if file_exists_yesno = 1
	# if you *could* read that temp txt file,
	# this confirms that the directory is valid.
	# Then you can delete it.
	deleteFile: temp_filename$
else
	# if that file wasn&amp;#39;t readable, that means that the directory wasn&amp;#39;t valid. 
	printline The folder &amp;#39;dir_in$&amp;#39; was not found
	exit Your directory doesn&amp;#39;t exist. Check spelling. The directory must *already* exist.
endif

## Let&amp;#39;s also check whether a subfolder with edited TextGrids already exists,
## for instance when the script has been run and exited before.

temp_filename$ = dir_in$ + &amp;#34;/edited_textgrids/&amp;#34; + &amp;#34;my_temporary_Praat_file.txt&amp;#34;
nocheck writeFileLine: temp_filename$, &amp;#34;This is just to check if the subfolder exists&amp;#34;

### Can the file be found?

subfolderfile_exists_yesno = fileReadable(temp_filename$)

if subfolderfile_exists_yesno = 1
	# if you *could* read that temp txt file,
	# this confirms that the subfolder already exists.
	# Then you can delete it.
	deleteFile: temp_filename$
else
	# if it didn&amp;#39;t yet exist, let&amp;#39;s create the subfolder
	createFolder: &amp;#34;&amp;#39;dir_in$&amp;#39;/edited_textgrids&amp;#34;
endif





################################################################################
################################################################################
#################################    SCRIPT    #################################
################################################################################
################################################################################

## Let&amp;#39;s keep track of who annotated which file. This can be helpful when
## multiple annotators run the same script on the same shared folder.

beginPause: &amp;#34;Please enter your name:&amp;#34;
	text: &amp;#34;annotator&amp;#34;, &amp;#34;&amp;#34;
clicked = endPause: &amp;#34;Next&amp;#34;, 1

## Now we create a list of TextGrid files in the input directory:

Create Strings as file list: &amp;#34;list_of_files&amp;#34;, &amp;#34;&amp;#39;dir_in$&amp;#39;/*.TextGrid&amp;#34;

	#######################################################################################
	## CREATE EMPTY TEXTGRIDS
	########################
	## If you do not yet have pre-existing TextGrids (but a folder with only sound files instead),
	## you can read the sound files in the directory and create empty TextGrids for the user
	## to edit.
	## Adjust this script as follows:
	## - Change *.TextGrid to *.wav in the line above.
	## - Change *.TextGrid to *.wav in the line below starting with &amp;#34;extposition$&amp;#34;
	## - Replace this line: Read from file... &amp;#39;dir_in$&amp;#39;\&amp;#39;name$&amp;#39;.TextGrid
	##		with this line: To TextGrid: &amp;#34;manual&amp;#34;, &amp;#34;&amp;#34;
	#######################################################################################

nfiles = Get number of strings
if nfiles = 0
	exit The directory &amp;#39;dir_in$&amp;#39; does not contain any TextGrid files.
endif

## By randomizing this file list, it allows for multiple users to simultanously work
## on the same folder without overwriting previous annotations. It also reduces the risk
## of human biases in annotations (e.g., annotator fatigue affecting one condition more
## than another condition).

Randomize

## Now we&amp;#39;ll loop through the list and present individual files...

for i from 1 to &amp;#39;nfiles&amp;#39;
	select Strings list_of_files
	
	fileplusext$ = Get string... &amp;#39;i&amp;#39;
	extposition = index(fileplusext$, &amp;#34;.TextGrid&amp;#34;)
	name$ = left$(fileplusext$, (&amp;#39;extposition&amp;#39;-1))

	outname$ = &amp;#34;&amp;#39;name$&amp;#39;_edited&amp;#34;
	fulloutname$ = &amp;#34;&amp;#39;dir_in$&amp;#39;/edited_textgrids/&amp;#39;outname$&amp;#39;.TextGrid&amp;#34;

	## Let&amp;#39;s check if an &amp;#34;_edited&amp;#34; version already exists.
	## The script only presents those files for editing that do not yet have been edited before.
	
	editedfile_exists_yesno = fileReadable(fulloutname$)
	if editedfile_exists_yesno
		do_nothing = 1
	else
		Read from file... &amp;#39;dir_in$&amp;#39;\&amp;#39;name$&amp;#39;.wav
		if blinded$ = &amp;#34;yes&amp;#34;
			Rename... current_Sound
		endif
		sound_name$ = selected$(&amp;#34;Sound&amp;#34;)
		## If a filename contains spaces, Praat replaces these spaces with underscores.
		## Example: &amp;#34;file number 1.wav&amp;#34; in a given folder becomes
		##			&amp;#34;file_number_1.wav&amp;#34; in the Praat object window.
		## Therefore, it is important **not** to use a filename variable (here: &amp;#39;name$&amp;#39;)
		## in &amp;#39;selecting commands&amp;#39; in Praat, like &amp;#39;select&amp;#39; and &amp;#39;plus&amp;#39;!
		## Better still: do not use spaces in filenames!
		
		Read from file... &amp;#39;dir_in$&amp;#39;\&amp;#39;name$&amp;#39;.TextGrid
		if blinded$ = &amp;#34;yes&amp;#34;
			Rename... current_TextGrid
		endif
		tg_name$ = selected$(&amp;#34;TextGrid&amp;#34;)

		#######################################################################################
		## ADD/REMOVE TIERS HERE
		########################
		## This is where we you could adjust the script to automatically add/remove tiers
		## and/or automatically adjust intervals (setting them to the nearest zero crossings?).
		## Duplicating tiers can be helpful when you want to view original vs. manually edited
		## tiers below/above each other in one and the same edited TextGrid.
		## Example:
		## &amp;gt; Duplicate tier... 1 1 newtier
		## [ARGUMENTS: position_of_tier_to_duplicate position_for_new_tier name_of_new_tier]
		#######################################################################################

		plus Sound &amp;#39;sound_name$&amp;#39;
		Edit

		beginPause: &amp;#34;Please check and edit this TextGrid.&amp;#34;
			comment: &amp;#34;Please check and edit the annotations.&amp;#34;
			text: &amp;#34;Comments&amp;#34;, &amp;#34;&amp;#34;
		clicked = endPause: &amp;#34;Next&amp;#34;, 1
		
		editor TextGrid &amp;#39;tg_name$&amp;#39;
			Close
		endeditor

		select TextGrid &amp;#39;tg_name$&amp;#39;
		Write to text file... &amp;#39;fulloutname$&amp;#39;
		plus Sound &amp;#39;sound_name$&amp;#39;
		Remove

		appendFileLine: &amp;#34;&amp;#39;dir_in$&amp;#39;/edited_textgrids/annotation_log.txt&amp;#34;, annotator$, tab$, name$, tab$, comments$
	endif
endfor

select Strings list_of_files
Remove

################################################################################
# End of script
################################################################################
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Second how-to</title>
      <link>https://hrbosker.github.io/resources/how-to/howto2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/resources/how-to/howto2/</guid>
      <description>&lt;p&gt;This is the subtitle.&lt;/p&gt;
&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; 1-2 hours per week, for 8 weeks&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;firstobject = 2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Some disclaimer&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Repackaging speech</title>
      <link>https://hrbosker.github.io/demos/repackaging/</link>
      <pubDate>Thu, 07 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/demos/repackaging/</guid>
      <description>&lt;h2 id=&#34;how-fast-can-your-ears-go&#34;&gt;How fast can your ears go?&lt;/h2&gt;
&lt;p&gt;Listen to this clip of a talker saying the telephone number &lt;em&gt;496-0356&lt;/em&gt;&amp;hellip;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;496-0356&lt;/em&gt; &amp;ndash; [original]






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/repackaging/4960356_nopause_mono.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You&amp;rsquo;ll probably have no problem understanding the same digits when it&amp;rsquo;s compressed by a factor of 2 (i.e., twice as fast)&amp;hellip;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;496-0356&lt;/em&gt; &amp;ndash; compressed by 2






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/repackaging/4960356_nopause_mono_k2.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And when it&amp;rsquo;s compressed by a factor of 3?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;496-0356&lt;/em&gt; &amp;ndash; compressed by 3






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/repackaging/4960356_nopause_mono_k3.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And compressed by 4?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;496-0356&lt;/em&gt; &amp;ndash; compressed by 4






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/repackaging/4960356_nopause_mono_k4.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Or even by 5?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;496-0356&lt;/em&gt; &amp;ndash; compressed by 5






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/repackaging/4960356_nopause_mono_k5.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Research has demonstrated that compression rates up to 3 are still doable (kinda&amp;hellip;) but intelligibility breaks down quite dramatically for higher compression rates (e.g., &lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00652/full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ghitza, 2014&lt;/a&gt;; &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2018-lcn&#34;&gt;Bosker &amp;amp; Ghitza, 2018&lt;/a&gt;). So the last two clips are unintelligible to most listeners.&lt;/p&gt;
&lt;p&gt;This has been suggested to be due to how our brain works. Our brain is known to &amp;rsquo;track&amp;rsquo; incoming speech by aligning its &amp;lsquo;brain waves&amp;rsquo; (neural oscillations in the &lt;em&gt;theta&lt;/em&gt; range, 3-9 Hz) to the syllable rhythm of the speech (amplitude modulations in the temporal envelope). But when the syllables come in too rapidly (&amp;gt;9 Hz), the brain waves can&amp;rsquo;t keep up, resulting in poor intelligibility.&lt;/p&gt;
&lt;h2 id=&#34;making-unintelligible-speech-intelligible-again&#34;&gt;Making unintelligible speech intelligible again&lt;/h2&gt;
&lt;p&gt;But there&amp;rsquo;s a trick to help the brain keep up. Let&amp;rsquo;s take the unintelligible clip with the telephone number &lt;em&gt;496-0356&lt;/em&gt; compressed by a factor of 5. Here it is again:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;496-0356&lt;/em&gt; &amp;ndash; compressed by 5






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/repackaging/4960356_nopause_mono_k5.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That&amp;rsquo;s tough, right? No wonder with a syllable rate of over 12 Hz!&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s chop this clip up into short snippets of 66 ms (&amp;ldquo;packages&amp;rdquo;, cf. top panel in the figure at the top of this page) and space them apart by 100 ms (i.e., inserting silent intervals). This brings the package rate down to around 6 Hz. &lt;strong&gt;Can your brain keep up with that?&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;496-0356&lt;/em&gt; &amp;ndash; repackaged






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/repackaging/4960356_nopause_mono_repack.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;What wizardry! What was unintelligible before is made (more&amp;hellip;) intelligible by adding some &amp;lsquo;breathing space&amp;rsquo; for the brain. &lt;em&gt;Note that the speech signal itself did not change&lt;/em&gt;: it is the same acoustic content as before, but just presented at a slower pace so your brain can keep up!&lt;/p&gt;
&lt;h2 id=&#34;and-now-the-exam&#34;&gt;And now the exam!&lt;/h2&gt;
&lt;p&gt;Here is a new telephone number, also consisting of 7 digits. Can you tell me &lt;strong&gt;what the last four digits are?&lt;/strong&gt;&lt;/p&gt;






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/repackaging/4960592_nopause_mono_k5.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;

&lt;blockquote&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-8&#34;&gt;
  &lt;summary&gt;&lt;em&gt;Click here to see the correct answer&amp;hellip;&lt;/em&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;em&gt;0592&lt;/em&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;/blockquote&gt;
&lt;p&gt;And what about this one?&lt;/p&gt;






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/repackaging/4980137_nopause_mono_k5.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;

&lt;blockquote&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-10&#34;&gt;
  &lt;summary&gt;&lt;em&gt;Click here to see the correct answer&amp;hellip;&lt;/em&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;em&gt;0137&lt;/em&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;/blockquote&gt;
&lt;p&gt;Presumably everybody has a hard time correctly hearing these digits, because these are again recordings that have been compressed by a factor of 5!&lt;/p&gt;
&lt;p&gt;But now try these:&lt;/p&gt;






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/repackaging/4960723_nopause_mono_repack.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;

&lt;blockquote&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-12&#34;&gt;
  &lt;summary&gt;&lt;em&gt;Click here to see the correct answer&amp;hellip;&lt;/em&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;em&gt;0723&lt;/em&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;/blockquote&gt;






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/repackaging/8790164_nopause_mono_repack.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;

&lt;blockquote&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-14&#34;&gt;
  &lt;summary&gt;&lt;em&gt;Click here to see the correct answer&amp;hellip;&lt;/em&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;em&gt;0164&lt;/em&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;/blockquote&gt;
&lt;p&gt;That probably sounded much more intelligible. These are two examples of &amp;lsquo;repackaged speech&amp;rsquo;: first compressed by a factor of 5, chopped up into 66 ms snippets, and then spaced apart by 100 ms. And your brain was presumably very grateful for that extra breathing space (&amp;ldquo;my pleasure, brain&amp;hellip;&amp;rdquo;).&lt;/p&gt;
&lt;h2 id=&#34;why-is-this-important&#34;&gt;Why is this important?&lt;/h2&gt;
&lt;p&gt;This phenomenon can tell us something about what acoustic aspects support speech intelligibility. If we know what aspects of speech are critical for proper intelligibility, then that knowledge would be helpful, for instance, (i) for speech synthesizers, such as Automatic Announcement Systems in public transport, to generate speech signals that human listeners can understand well, (ii) for hearing aids to &amp;rsquo;enrich&amp;rsquo; incoming speech signals and present those optimized signals to the listening brain, or (iii) for communication with the elderly who often experience difficulty with speech perception, especially in noise.&lt;/p&gt;
&lt;h2 id=&#34;relevant-papers&#34;&gt;Relevant papers&lt;/h2&gt;
&lt;blockquote&gt;










  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/oded-ghitza/&#34;&gt;Oded Ghitza&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2018).
  &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2018-lcn/&#34;&gt;Entrained theta oscillations guide perception of subsequent speech: Behavioral evidence from rate normalization&lt;/a&gt;.
  &lt;em&gt;Language, Cognition and Neuroscience,33&lt;/em&gt;(8), 955-967, doi:10.1080/23273798.2018.1439179.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_2538752_11/component/file_2630351/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/bosker-etal-2018-lcn/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;













&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1080/23273798.2018.1439179&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Batch processing</title>
      <link>https://hrbosker.github.io/resources/scripts/batch-processing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/resources/scripts/batch-processing/</guid>
      <description>&lt;p&gt;This script is an in-house template / starting point for batch processing multiple files. Adapt it to your own needs to apply a particular function to multiple files or multiple time intervals within each file.&lt;/p&gt;
&lt;p&gt;In its current form, the script reads each .wav file &lt;em&gt;plus&lt;/em&gt; accompanying TextGrid in a given input directory, extracts all non-empty intervals individually, and then loops over those to find the ones labelled &amp;ldquo;vowel&amp;rdquo;. It then allows the user to apply a particular function to those intervals (such as &lt;code&gt;Scale intensity: 65&lt;/code&gt;), after which it concatenates the individual intervals back together, and saves the output in an output directory.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; In its current form, the script does not run any function on its input. It really only serves as a starting point, including snippets of code we regularly use and now do not need to look up every time we want to do batch processing in Praat.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You can also &lt;a href=&#34;../batch-processing.praat&#34;&gt;download the script&lt;/a&gt; as a .praat file.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;################################################################################
### Hans Rutger Bosker, Radboud University
### HansRutger.Bosker@ru.nl
### Date: 6 July 2022, run in Praat 6.2.12 on Windows 10
### License: CC BY-NC 4.0
################################################################################


	###&amp;gt;&amp;gt; This script is a starting point for batch processing a set of files.
	###&amp;gt;&amp;gt;	The script basically reads files in an input directory and runs a
	###&amp;gt;&amp;gt;	a to-be-defined function [see &amp;#39;Perform your function here&amp;#39; below]
	###&amp;gt;&amp;gt;	and writes the output to an output directory . This saves me having
	###&amp;gt;&amp;gt;	to look up how to create a file list, how to loop over files, etc.
	###&amp;gt;&amp;gt;	
	###&amp;gt;&amp;gt; Since this was basically intended for in-house use, I&amp;#39;ve added in bits
	###&amp;gt;&amp;gt;	and pieces that I find useful to have ready-to-go, such as:
	###&amp;gt;&amp;gt;	&amp;#39;beginPause&amp;#39; for manually specifying variables.

################################################################################
### Variables you will definitely need to customize:
################################################################################

### Where can the files be found?

dir_in$ = &amp;#34;C:\Users\hanbos\mysounds&amp;#34;

### Where should the output files be saved?

dir_out$ = &amp;#34;C:\Users\hanbos\mysounds\output&amp;#34;





################################################################################
### Let&amp;#39;s check whether the directories specified above exist...
################################################################################

### Let&amp;#39;s check if the input directory exists.
### This script will throw an error if the directory doesn&amp;#39;t exist
### (i.e., it won&amp;#39;t write to a mysterious temp directory).

### First check whether the input directory ends in a backslash (if so, removed)

if right$(dir_in$,1)=&amp;#34;/&amp;#34;
	dir_in$ = left$(dir_in$,length(dir_in$)-1)
elsif right$(dir_in$,1)=&amp;#34;\&amp;#34;
	dir_in$ = left$(dir_in$,length(dir_in$)-1)
endif

### Then create a temporary txt file in the folder
### and try to write it to the input folder.

### NOTE: The &amp;#34;nocheck&amp;#34; below asks Praat not to complain if the folder
### does *not* exist. We&amp;#39;ll manually check whether the saving of this
### temp txt file has succeeded or not further down below.

temp_filename$ = dir_in$ + &amp;#34;/&amp;#34; + &amp;#34;my_temporary_Praat_file.txt&amp;#34;
nocheck writeFileLine: temp_filename$, &amp;#34;This is just to check if the directory exists&amp;#34;

### Can the file be found?

file_exists_yesno = fileReadable(temp_filename$)

if file_exists_yesno = 1
	# if you *could* read that temp txt file,
	# this confirms that the directory is valid.
	# Then you can delete it.
	deleteFile: temp_filename$
else
	# if that file wasn&amp;#39;t readable, that means that the directory wasn&amp;#39;t valid. 
	printline The folder &amp;#39;dir_in$&amp;#39; was not found
	exit Your input directory doesn&amp;#39;t exist. Check spelling. The directory must *already* exist.
endif

## Now re-do this for the output directory:

if right$(dir_out$,1)=&amp;#34;/&amp;#34;
	dir_out$ = left$(dir_out$,length(dir_out$)-1)
elsif right$(dir_out$,1)=&amp;#34;\&amp;#34;
	dir_out$ = left$(dir_out$,length(dir_out$)-1)
endif

### Then create a temporary txt file in the folder
### and try to write it to the input folder.

### NOTE: The &amp;#34;nocheck&amp;#34; below asks Praat not to complain if the folder
### does *not* exist. We&amp;#39;ll manually check whether the saving of this
### temp txt file has succeeded or not further down below.

temp_filename$ = dir_out$ + &amp;#34;/&amp;#34; + &amp;#34;my_temporary_Praat_file.txt&amp;#34;
nocheck writeFileLine: temp_filename$, &amp;#34;This is just to check if the directory exists&amp;#34;

### Can the file be found?

file_exists_yesno = fileReadable(temp_filename$)

if file_exists_yesno = 1
	# if you *could* read that temp txt file,
	# this confirms that the directory is valid.
	# Then you can delete it.
	deleteFile: temp_filename$
else
	# if that file wasn&amp;#39;t readable, that means that the directory wasn&amp;#39;t valid. 
	printline The folder &amp;#39;dir_out$&amp;#39; was not found
	exit Your output directory doesn&amp;#39;t exist. Check spelling. The directory must *already* exist.
endif





###########################################################################
##	FORM TO MANUALLY SPECIFY VARIABLES
###########################################################################
#beginPause: &amp;#34;Enter settings&amp;#34;
#	comment: &amp;#34;Provide instructions here&amp;#34;
#	real: &amp;#34;minPitch&amp;#34;, 70
#	real: &amp;#34;maxPitch&amp;#34;, 250
#	choice: &amp;#34;method&amp;#34;, 1
#	   option: &amp;#34;Flip F0 contour&amp;#34;
#	   option: &amp;#34;Expand/Contract F0 contour&amp;#34;
#	   option: &amp;#34;Flatten F0 contour&amp;#34;
#clicked = endPause (&amp;#34;Cancel&amp;#34;, &amp;#34;OK&amp;#34;, 2)
###########################################################################
###########################################################################





## Let&amp;#39;s create a list of all the files in the input directory.

Create Strings as file list: &amp;#34;list_of_files&amp;#34;, &amp;#34;&amp;#39;dir_in$&amp;#39;/*.wav&amp;#34;

nfiles = Get number of strings
if nfiles = 0
	exit The directory &amp;#39;dir_in$&amp;#39; does not contain any .wav files.
endif

## Now we&amp;#39;ll loop through the list...

for i from 1 to &amp;#39;nfiles&amp;#39;
	select Strings list_of_files
	
	fileplusext$ = Get string... &amp;#39;i&amp;#39;
	extposition = index(fileplusext$, &amp;#34;.wav&amp;#34;)
	name$ = left$(fileplusext$, (&amp;#39;extposition&amp;#39;-1))

	Read from file... &amp;#39;dir_in$&amp;#39;\&amp;#39;name$&amp;#39;.wav
	Read from file... &amp;#39;dir_in$&amp;#39;\&amp;#39;name$&amp;#39;.TextGrid
	plus Sound &amp;#39;name$&amp;#39;
	Extract non-empty intervals... 1 no
	
	nSelected = numberOfSelected()

	## Assign an object number to each object (e.g., 1-5),
	## and save the id numbers of each object to an array.

	for thisObject to nSelected
		objectArray [&amp;#39;thisObject&amp;#39;] = selected(&amp;#39;thisObject&amp;#39;)
	endfor

	for j to nSelected
		curr_objectId = objectArray [&amp;#39;j&amp;#39;]
		select &amp;#39;curr_objectId&amp;#39;
		curr_objectName$ = selected$(&amp;#34;Sound&amp;#34;)
		
		if curr_objectName$ = &amp;#34;vowel&amp;#34;

			########################################################################
			# Perform your function here!
			#	Example: Scale intensity... 65
			########################################################################

		endif
	endfor

	for j from 1 to nSelected
		curr_objectId = objectArray [&amp;#39;j&amp;#39;]
		if j = 1
			select &amp;#39;curr_objectId&amp;#39;
		else
			plus &amp;#39;curr_objectId&amp;#39;
		endif
	endfor
	Concatenate recoverably

	select Sound chain
	Write to WAV file... &amp;#39;dir_out$&amp;#39;\&amp;#39;name$&amp;#39;_manipulated.wav
	select TextGrid chain
	Write to text file... &amp;#39;dir_out$&amp;#39;\&amp;#39;name$&amp;#39;_manipulated.TextGrid

	## Cleaning up...
	for j from 1 to nSelected
		curr_objectId = objectArray [&amp;#39;j&amp;#39;]
		if j = 1
			select &amp;#39;curr_objectId&amp;#39;
		else
			plus &amp;#39;curr_objectId&amp;#39;
		endif
	endfor
	plus Sound chain
	plus TextGrid chain
	plus Sound &amp;#39;name$&amp;#39;
	plus TextGrid &amp;#39;name$&amp;#39;
	Remove

endfor

select Strings list_of_files
Remove

################################################################################
# End of script
################################################################################
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Third how-to</title>
      <link>https://hrbosker.github.io/resources/how-to/howto3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/resources/how-to/howto3/</guid>
      <description>&lt;p&gt;This is the subtitle.&lt;/p&gt;
&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; 1-2 hours per week, for 8 weeks&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;firstobject = 3
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Some disclaimer&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Lombard speech</title>
      <link>https://hrbosker.github.io/demos/lombard-speech/</link>
      <pubDate>Thu, 07 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/demos/lombard-speech/</guid>
      <description>&lt;h2 id=&#34;lets-do-a-little-test&#34;&gt;Let&amp;rsquo;s do a little test&amp;hellip;&lt;/h2&gt;
&lt;p&gt;First, let&amp;rsquo;s take care of your audio settings:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;put on your headphones/ear buds/speakers&lt;/li&gt;
&lt;li&gt;turn your volume way down&lt;/li&gt;
&lt;li&gt;play this sound&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;SOME WHITE NOISE&amp;hellip;






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/lombard-speech/whitenoise.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&amp;hellip;and adjust your volume until it&amp;rsquo;s at a &lt;strong&gt;loud but still comfortable&lt;/strong&gt; level.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;OK, now we&amp;rsquo;ll do a short reading test:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;play the video below&lt;/li&gt;
&lt;li&gt;you&amp;rsquo;ll see a counter counting down from 3&amp;hellip;&lt;/li&gt;
&lt;li&gt;&amp;hellip;and then it will present a simple sentence on screen&lt;/li&gt;
&lt;li&gt;your task is simply to &lt;strong&gt;read out the sentence &lt;em&gt;aloud&lt;/em&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ready? Go!&lt;/p&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/lombard-speech/lombard_test.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;p&gt;Why thank you! OK, now let&amp;rsquo;s do this again. Make sure to &lt;strong&gt;keep wearing your headphones&lt;/strong&gt;, play the next video, and read out the sentence aloud.&lt;/p&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/lombard-speech/lombard_test_noise.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;p&gt;&lt;em&gt;Surprise!&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;whats-going-on&#34;&gt;What&amp;rsquo;s going on?&lt;/h2&gt;
&lt;p&gt;Perhaps you noticed your voice sounding somewhat different the second time, when there was loud babble from 16 other talkers playing in your ears, compared to the first time (in quiet). This phenomenon is called &lt;strong&gt;Lombard speech&lt;/strong&gt; (or: &lt;em&gt;Lombard effect&lt;/em&gt;; &lt;em&gt;Lombard reflex&lt;/em&gt;). It&amp;rsquo;s the type of speech people produce when speaking in noise.&lt;/p&gt;
&lt;p&gt;But perhaps you didn&amp;rsquo;t quite hear yourself all too well because it&amp;rsquo;s hard to listen to your own voice when there&amp;rsquo;s other sounds around. So here&amp;rsquo;s two clips from a male speaker of British English (and a rather posh one, if I may say so&amp;hellip;) giving you some really useful dietary advice. The first is from when he was &lt;strong&gt;speaking in quiet&lt;/strong&gt;: this is called &amp;lsquo;plain speech&amp;rsquo;. The second clip is a recording of the same sentence but this time the talker heard loud noise over headphones, &lt;strong&gt;speaking in noise&lt;/strong&gt;: &amp;lsquo;Lombard speech&amp;rsquo;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PLAIN SPEECH&lt;/strong&gt;






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/lombard-speech/MKH800_19_0005.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LOMBARD SPEECH&lt;/strong&gt;






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/lombard-speech/MKH800_25_0005.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;retrieved from the &lt;a href=&#34;https://datashare.ed.ac.uk/handle/10283/343&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Acted clear speech corpus&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;what-is-lombard-speech&#34;&gt;What is Lombard speech?&lt;/h2&gt;
&lt;p&gt;In the clips above, you can clearly hear the difference between &amp;lsquo;plain speech&amp;rsquo; and &amp;lsquo;Lombard speech&amp;rsquo;. Lombard speech sounds louder, higher pitched, is a little slower, with more pronounced higher frequencies, and clearer vowels. In our own research, we demonstrated that Lombard speech is also more rhythmic, having a stronger &amp;lsquo;beat&amp;rsquo; to it compared to plain speech (see &lt;a href=&#34;#relevant-papers&#34;&gt;refs&lt;/a&gt; below).&lt;/p&gt;
&lt;h2 id=&#34;lombard-speech-rulez&#34;&gt;Lombard speech rulez&lt;/h2&gt;
&lt;p&gt;Speech perception studies have demonstrated that these &amp;lsquo;vocal adjustments&amp;rsquo; people make when speaking in noise actually have a purpose: they make you more intelligible! When you take the plain and Lombard clips above, scale their intensities to be exactly the same, and then mix them with loud babble, this is what you get:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PLAIN SPEECH in BABBLE&lt;/strong&gt; (SNR = -6 dB)






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/lombard-speech/MKH800_19_0005.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LOMBARD SPEECH in BABBLE&lt;/strong&gt; (SNR = -6 dB)






  








&lt;audio controls &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/lombard-speech/MKH800_25_0005.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You may experience that it&amp;rsquo;s easier to pick out the male target talker from the babble in the second (Lombard) clip than in the first (plain) clip. Apparently, &amp;lsquo;speaking up&amp;rsquo; actually helps!&lt;/p&gt;
&lt;h2 id=&#34;why-is-this-important&#34;&gt;Why is this important?&lt;/h2&gt;
&lt;p&gt;Lombard speech is more intelligible in noise than plain speech. This means that speech researchers can &amp;lsquo;borrow&amp;rsquo; acoustic aspects of Lombard speech to boost speech intelligibility, for instance in hearing aids. So next time you wanna make sure your message comes across in that busy bar, you&amp;rsquo;d better boost your F0, raise your spectral tilt, and increase your vowel dispersion; got it?!&lt;/p&gt;
&lt;h2 id=&#34;relevant-papers&#34;&gt;Relevant papers&lt;/h2&gt;
&lt;blockquote&gt;










  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/martin-cooke/&#34;&gt;Martin Cooke&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2018).
  &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2018-jasa/&#34;&gt;Talkers produce more pronounced amplitude modulations when speaking in noise&lt;/a&gt;.
  &lt;em&gt;The Journal of the Acoustical Society of America, 143&lt;/em&gt;(2), EL121-EL126, doi:10.1121/1.5024404.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_2537243_6/component/file_2554157/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/bosker-etal-2018-jasa/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;













&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1121/1.5024404&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;
&lt;blockquote&gt;










  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/martin-cooke/&#34;&gt;Martin Cooke&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2020).
  &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2020-jasa/&#34;&gt;Enhanced amplitude modulations contribute to the Lombard intelligibility benefit: Evidence from the Nijmegen Corpus of Lombard Speech&lt;/a&gt;.
  &lt;em&gt;The Journal of the Acoustical Society of America, 147&lt;/em&gt;(2), 721-730, doi:10.1121/10.0000646.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3186181_3/component/file_3186182/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/bosker-etal-2020-jasa/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://hdl.handle.net/1839/21ee5744-b5dc-4eed-9693-c37e871cdaf6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1121/10.0000646&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Look and listen</title>
      <link>https://hrbosker.github.io/demos/visual-world-paradigm/</link>
      <pubDate>Fri, 08 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/demos/visual-world-paradigm/</guid>
      <description>&lt;h2 id=&#34;listening-test&#34;&gt;Listening test&lt;/h2&gt;
&lt;p&gt;In the video below, you&amp;rsquo;ll see a simple display with four objects. First, see if you know each of the four objects. Then play the video. You&amp;rsquo;ll hear a female voice asking you to press a button for one of the objects (i.e., click on it). While watching and listening, try to keep track of where your eyes go in the display&amp;hellip;&lt;/p&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/visual-world-paradigm/vwp_fluent.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;p&gt;Do you have any idea what the next word of the speaker will be? Probably not, right? Did you notice anything particular about where in the display your gaze was at? Since you probably didn&amp;rsquo;t know what object the speaker was going to name, chances are your eyes were all over the place.&lt;/p&gt;
&lt;p&gt;OK, next video. It&amp;rsquo;s the same display, but with a new audio recording. Have a look and see if you can tell which of the four objects the speaker selects&amp;hellip;&lt;/p&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/visual-world-paradigm/vwp_disfluent.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;p&gt;Well, in this case, you may already have a hunch, right? The speaker was hesitating at the end of her utterance, wasn&amp;rsquo;t she? Well, chances are this native speaker of British English won&amp;rsquo;t have much trouble naming common objects, like lion, ear, or bike, would she? So could it be that she&amp;rsquo;ll refer to the Italian moka pot in the top left?&lt;/p&gt;
&lt;h2 id=&#34;disfluencies-help-you-predict-whats-coming-up&#34;&gt;Disfluencies help you predict what&amp;rsquo;s coming up&lt;/h2&gt;
&lt;p&gt;Natural speech is messy. We stumble over words, lose our line of thought, and produce tons of uhm&amp;rsquo;s and uh&amp;rsquo;s. Still, these kinds of &lt;em&gt;disfluencies&lt;/em&gt; don&amp;rsquo;t occur randomly throughout an utterance. We are much more likely to stumble before rarely occurring (low-frequency), novel (not mentioned before), and complex (long) words than we are before common and simple words.&lt;/p&gt;
&lt;p&gt;Interestingly, human listeners seem to be aware of this. In our experiments, we presented listeners with displays like the ones above together with spoken instructions to click on one of the objects. While people were watching/listening, we recorded where they were looking on the screen using eye-tracking (see lab photo below). This allowed us to track their gaze on a millisecond time scale as the utterance unfolds. Results showed that &lt;strong&gt;when people heard the speaker hesitate, they were much more likely to look at a low-frequency object, like moka pot, compared to high-frequency objects&lt;/strong&gt; (&lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2014-jml&#34;&gt;Bosker et al., 2014&lt;/a&gt;).&lt;/p&gt;
&lt;img src=&#34;https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fwww.mpi.nl%2Fsites%2Fdefault%2Ffiles%2F2019-03%2F18085-Eyetracking_8474_small1.jpg&#34; alt=&#34;mpi labs eyetracking&#34; width=&#34;400&#34;/&gt;
&lt;h2 id=&#34;ok-lets-try-again&#34;&gt;OK, let&amp;rsquo;s try again&amp;hellip;&lt;/h2&gt;
&lt;p&gt;Here&amp;rsquo;s another video, again with the same display, but another audio recording. Once again, have a listen and see if you can tell which object the speaker will name:&lt;/p&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/visual-world-paradigm/vwp_li.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;p&gt;In the last few milliseconds of the clip, you may have discovered a glimpse of the object. Did she say &amp;ldquo;Now press the button for the li-&amp;hellip;&amp;rdquo;? Does that mean we&amp;rsquo;ve finally figured out that it&amp;rsquo;ll be the lion after all?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s find out:&lt;/p&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://hrbosker.github.io/demos/visual-world-paradigm/vwp_like.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;p&gt;&lt;em&gt;Aargh!&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;filler-words-can-be-misleading&#34;&gt;Filler words can be misleading&lt;/h2&gt;
&lt;p&gt;As mentioned before, speech is messy. We don&amp;rsquo;t only produce hesitations and disfluencies, but also litter our speech with seemingly meaningless filler words, such as &amp;lsquo;you know&amp;rsquo;, &amp;lsquo;well&amp;rsquo;, and (worst of all) &amp;rsquo;like&amp;rsquo;. Our audience, in turn, is tasked with distilling from this chaos what we actually want to communicate.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;And that can be hard.&lt;/em&gt; Filler words share their sounds (&lt;em&gt;phonology&lt;/em&gt;) with many other words. The filler &amp;rsquo;like&amp;rsquo; shares its initial sounds with words such as &amp;rsquo;lion&amp;rsquo;, &amp;rsquo;lime&amp;rsquo;, &amp;rsquo;lice&amp;rsquo;, lightbulb&amp;rsquo;, etc. Our experiments have shown that listeners are actually considering these similar-sounding words (&lt;em&gt;cohort competitor&lt;/em&gt;) when encountering &amp;rsquo;like&amp;rsquo;. When presented with displays with one &amp;lsquo;cohort competitor&amp;rsquo; (e.g., lion) and three distractors, participants were biased towards looking at the lion upon hearing &amp;ldquo;&amp;hellip;for the like&amp;hellip;&amp;rdquo;. This suggests that filler words, like &amp;ldquo;like&amp;rdquo; (see what I did there?), have an impact on the efficiency of word recognition (&lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2021-discproc&#34;&gt;Bosker et al., 2021&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&#34;relevant-papers&#34;&gt;Relevant papers&lt;/h2&gt;
&lt;blockquote&gt;










  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/esperanza-badaya/&#34;&gt;Esperanza Badaya&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/martin-corley/&#34;&gt;Martin Corley&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2021).
  &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2021-discproc/&#34;&gt;Discourse markers activate their, like, cohort competitors&lt;/a&gt;.
  &lt;em&gt;Discourse Processes, 58&lt;/em&gt;(9), 837-851, doi:10.1080/0163853X.2021.1924000.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3316633_4/component/file_3356284/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/bosker-etal-2021-discproc/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/rmj4e/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1080/0163853X.2021.1924000&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;
&lt;blockquote&gt;










  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/marjolein-van-os/&#34;&gt;Marjolein van Os,&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/rik-does/&#34;&gt;Rik Does&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/geertje-van-bergen/&#34;&gt;Geertje van Bergen&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2019).
  &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2019-jml/&#34;&gt;Counting ‘uhm’s: how tracking the distribution of native and non-native disfluencies influences online language comprehension&lt;/a&gt;.
  &lt;em&gt;Journal of Memory and Language, 106&lt;/em&gt;, 189-202, doi:10.1016/j.jml.2019.02.006.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3029110_7/component/file_3038833/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/bosker-etal-2019-jml/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/5y2e6/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1016/j.jml.2019.02.006&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;
&lt;blockquote&gt;










  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/geertje-van-bergen/&#34;&gt;Geertje van Bergen&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2018).
  &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2018-jml/&#34;&gt;Linguistic expectation management in online discourse processing: An investigation of Dutch inderdaad ‘indeed’ and eigenlijk ‘actually’&lt;/a&gt;.
  &lt;em&gt;Journal of Memory and Language, 103&lt;/em&gt;, 191-209, doi:10.1016/j.jml.2018.08.004.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_2640593_2/component/file_2640592/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/bosker-etal-2018-jml/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;













&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1016/j.jml.2018.08.004&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;
&lt;blockquote&gt;










  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hugo-quene/&#34;&gt;Hugo Quené&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/ted-sanders/&#34;&gt;Ted Sanders&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/nivja-h.-de-jong/&#34;&gt;Nivja H. de Jong&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2014).
  &lt;a href=&#34;https://hrbosker.github.io/publication/bosker-etal-2014-jml/&#34;&gt;Native ‘um’s elicit prediction of low-frequency referents, but non-native ‘um’s do not&lt;/a&gt;.
  &lt;em&gt;Journal of Memory and Language, 75&lt;/em&gt;, 104-116, doi:10.1016/j.jml.2014.05.004.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_1900237_6/component/file_2034223/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/bosker-etal-2014-jml/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;













&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1016/j.jml.2014.05.004&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Acoustic correlates of Dutch lexical stress re-examined: Spectral tilt is not always more reliable than intensity</title>
      <link>https://hrbosker.github.io/publication/severijnen-etal-2022-speechprosody/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/severijnen-etal-2022-speechprosody/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Evidence for selective adaptation and recalibration in the perception of lexical stress</title>
      <link>https://hrbosker.github.io/publication/bosker-2022-ls/index/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-2022-ls/index/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>Visible lexical stress cues on the face do not influence audiovisual speech perception</title>
      <link>https://hrbosker.github.io/publication/bujok-etal-2022-sp/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bujok-etal-2022-sp/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>A tool for efficient and accurate segmentation of speech data: Announcing POnSS</title>
      <link>https://hrbosker.github.io/publication/rodd-etal-2021-brm/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/rodd-etal-2021-brm/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>Beat gestures influence which speech sounds you hear</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2021-procroysocb/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2021-procroysocb/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Discourse markers activate their, like, cohort competitors</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2021-discproc/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2021-discproc/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Listeners track talker-specific prosody to deal with talker-variability</title>
      <link>https://hrbosker.github.io/publication/severijnen-etal-2021-brainres/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/severijnen-etal-2021-brainres/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>The contribution of amplitude modulations in speech to perceived charisma</title>
      <link>https://hrbosker.github.io/publication/bosker-2021-bookchapter/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-2021-bookchapter/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Using fuzzy string matching for automated assessment of listener transcripts in speech intelligibility studies</title>
      <link>https://hrbosker.github.io/publication/bosker-2021-brm/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-2021-brm/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Biasing the perception of spoken words with transcranial alternating current stimulation</title>
      <link>https://hrbosker.github.io/publication/kosem-etal-2020-jcn/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/kosem-etal-2020-jcn/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Contextual speech rate influences morphosyntactic prediction and integration</title>
      <link>https://hrbosker.github.io/publication/kaufeld-etal-2020-lcn/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/kaufeld-etal-2020-lcn/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>Control of speaking rate is achieved by switching between qualitatively distinct cognitive ‘gaits’: Evidence from simulation</title>
      <link>https://hrbosker.github.io/publication/rodd-etal-2020-psychreview/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/rodd-etal-2020-psychreview/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>Enhanced amplitude modulations contribute to the Lombard intelligibility benefit: Evidence from the Nijmegen Corpus of Lombard Speech</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2020-jasa/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2020-jasa/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Eye-tracking the time course of distal and global speech rate effects</title>
      <link>https://hrbosker.github.io/publication/maslowski-etal-2020-jephpp/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/maslowski-etal-2020-jephpp/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Fluency in dialogue: Turn‐taking behavior shapes perceived fluency in native and nonnative speech</title>
      <link>https://hrbosker.github.io/publication/vanos-etal-2020-ll/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/vanos-etal-2020-ll/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>How visual cues to speech rate influence speech perception</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2020-qjep/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2020-qjep/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Knowledge-based and signal-based cues are weighted flexibly during spoken language comprehension</title>
      <link>https://hrbosker.github.io/publication/kaufeld-etal-2020-jeplmc/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/kaufeld-etal-2020-jeplmc/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Linguistic structure and meaning organize neural oscillations into a content-specific hierarchy</title>
      <link>https://hrbosker.github.io/publication/kaufeld-etal-2020-jneuro/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/kaufeld-etal-2020-jneuro/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Spectral contrast effects are modulated by selective attention in ‘cocktail party’ settings</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2020-app/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2020-app/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Temporal contrast effects in human speech perception are immune to selective attention</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2020-scirep/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2020-scirep/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Counting ‘uhm’s: how tracking the distribution of native and non-native disfluencies influences online language comprehension</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2019-jml/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2019-jml/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Deriving the onset and offset times of planning units from acoustic and articulatory measurements</title>
      <link>https://hrbosker.github.io/publication/rodd-etal-2019-jasa/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/rodd-etal-2019-jasa/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>How the tracking of habitual rate influences speech perception</title>
      <link>https://hrbosker.github.io/publication/maslowski-etal-2019-jeplmc/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/maslowski-etal-2019-jeplmc/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Listeners normalize speech for contextual speech rate even without an explicit recognition task</title>
      <link>https://hrbosker.github.io/publication/maslowski-etal-2019-jasa/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/maslowski-etal-2019-jasa/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Entrained theta oscillations guide perception of subsequent speech: Behavioral evidence from rate normalization</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2018-lcn/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2018-lcn/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Linguistic expectation management in online discourse processing: An investigation of Dutch inderdaad ‘indeed’ and eigenlijk ‘actually’</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2018-jml/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2018-jml/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>Listening to yourself is special: Evidence from global speech rate tracking</title>
      <link>https://hrbosker.github.io/publication/maslowski-etal-2018-plosone/index/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/maslowski-etal-2018-plosone/index/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Neural entrainment determines the words we hear</title>
      <link>https://hrbosker.github.io/publication/kosem-etal-2018-currbiol/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/kosem-etal-2018-currbiol/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>Putting Laurel and Yanny in context</title>
      <link>https://hrbosker.github.io/publication/bosker-2018-jasa/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-2018-jasa/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;</description>
    </item>
    
    <item>
      <title>Talkers produce more pronounced amplitude modulations when speaking in noise</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2018-jasa/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2018-jasa/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>Accounting for rate-dependent category boundary shifts in speech perception</title>
      <link>https://hrbosker.github.io/publication/bosker-2017-app/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-2017-app/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>An entrained rhythm’s frequency, not phase, influences temporal sampling of speech</title>
      <link>https://hrbosker.github.io/publication/bosker-kosem-2017-interspeech/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-kosem-2017-interspeech/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Cognitive load makes speech sound fast, but does not modulate acoustic context effects</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2017-jml/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2017-jml/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT --&gt;
</description>
    </item>
    
    <item>
      <title>Foreign languages sound fast: evidence from implicit rate normalization</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2017-frontiers/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2017-frontiers/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>How our own speech rate influences our perception of others</title>
      <link>https://hrbosker.github.io/publication/bosker-2017-jeplmc/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-2017-jeplmc/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>The role of temporal amplitude modulations in the political arena: Hillary Clinton vs. Donald Trump</title>
      <link>https://hrbosker.github.io/publication/bosker-2017-interspeech/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-2017-interspeech/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Whether long-term tracking of speech rate affects perception depends on who is talking</title>
      <link>https://hrbosker.github.io/publication/maslowski-etal-2017-interspeech/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/maslowski-etal-2017-interspeech/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Listening under cognitive load makes speech sound fast</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2016-spire/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2016-spire/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Our own speech rate influences speech perception</title>
      <link>https://hrbosker.github.io/publication/bosker-2016-sp/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-2016-sp/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Both native and non-native disfluencies trigger listeners’ attention</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2015/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2015/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Normalization for speechrate in native and nonnative speech</title>
      <link>https://hrbosker.github.io/publication/bosker-reinisch-2015/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-reinisch-2015/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Native ‘um’s elicit prediction of low-frequency referents, but non-native ‘um’s do not</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2014-jml/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2014-jml/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>Native speakers’ perceptions of fluency and accent in L2 speech</title>
      <link>https://hrbosker.github.io/publication/pinget-etal-2014-lt/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/pinget-etal-2014-lt/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>Perceptual adaptation to segmental and syllabic reductions in continuous spoken Dutch</title>
      <link>https://hrbosker.github.io/publication/poellmann-etal-2014-jphon/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/poellmann-etal-2014-jphon/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>The perception of fluency in native and nonnative speech</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2014-ll/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2014-ll/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT









&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>The processing and evaluation of fluency in native and non-native speech</title>
      <link>https://hrbosker.github.io/publication/bosker-2014-thesis/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-2014-thesis/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>Choosing a threshold for silent pauses to measure second language fluency</title>
      <link>https://hrbosker.github.io/publication/dejong-bosker-2013/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/dejong-bosker-2013/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Juncture (prosodic)</title>
      <link>https://hrbosker.github.io/publication/bosker-2013-ehll-juncture/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-2013-ehll-juncture/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Sibilant consonants</title>
      <link>https://hrbosker.github.io/publication/bosker-2013-ehll-sibilants/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-2013-ehll-sibilants/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>What makes speech sound fluent? The contributions of pauses, speed and repairs</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2013-lt/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2013-lt/</guid>
      <description>&lt;!-- THIS MARKDOWN BIT IS CURRENTLY COMMENTED OUT
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
--&gt;
</description>
    </item>
    
    <item>
      <title>Whispered speech as input for cochlear implants</title>
      <link>https://hrbosker.github.io/publication/bosker-etal-2010/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/publication/bosker-etal-2010/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://hrbosker.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://hrbosker.github.io/contact/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/contact/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://hrbosker.github.io/empty/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/empty/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://hrbosker.github.io/people/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/people/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Research</title>
      <link>https://hrbosker.github.io/research/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/research/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
