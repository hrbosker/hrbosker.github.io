<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Latest News | SPEAC | Hans Rutger Bosker</title>
    <link>https://hrbosker.github.io/news/</link>
      <atom:link href="https://hrbosker.github.io/news/index.xml" rel="self" type="application/rss+xml" />
    <description>Latest News</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Tue, 06 May 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://hrbosker.github.io/media/logo_hu4ee931c5040e3d0fbcac8149a377b250_283935_300x300_fit_lanczos_3.png</url>
      <title>Latest News</title>
      <link>https://hrbosker.github.io/news/</link>
    </image>
    
    <item>
      <title>Ye et al. (2025) published in Attention, Perception, &amp; Psychophysics!</title>
      <link>https://hrbosker.github.io/news/25-05-06-yeetal-paper-app/</link>
      <pubDate>Tue, 06 May 2025 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/25-05-06-yeetal-paper-app/</guid>
      <description>&lt;p&gt;&lt;strong&gt;The same gesture can be perceived differently, depending on the speech you hear!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;published&#34;&gt;Published!&lt;/h2&gt;
&lt;p&gt;Our new paper “Effect of auditory cues to lexical stress on the visual perception of gestural timing”, authored by &lt;a href=&#34;https://hrbosker.github.io/author/chengjia-ye/&#34;&gt;Chengjia Ye&lt;/a&gt;, &lt;a href=&#34;https://www.ru.nl/en/people/mcqueen-j&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;James McQueen&lt;/a&gt; and &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;, is now publicly available online! It&amp;rsquo;s published Open Access in the journal &lt;em&gt;Attention, Perception, &amp;amp; Psychophysics&lt;/em&gt;. Feel free to check it out: links are given at the bottom of the page.&lt;/p&gt;
&lt;h2 id=&#34;whats-it-about&#34;&gt;What&amp;rsquo;s it about?&lt;/h2&gt;
&lt;p&gt;We ran two experiments to see how native listeners of Dutch use auditory cues to lexical stress when perceiving the timing of hand beats. Previous studies of our lab have demonstrated that the timing of hand beats can influence the perception of lexical stress, and thus spoken-word recognition. In this study, we examined &amp;lsquo;a reverse effect&amp;rsquo;: does lexical stress change how you perceive the visual timing of hand gestures? Our results showed that the perceived timing of a hand beat is indeed &amp;lsquo;pulled towards&amp;rsquo; the syllable with stronger stress cues. For example, our participants perceived an &amp;rsquo;early gesture&amp;rsquo;, objectively preceding a stressed syllable by 200 ms, to only precede the stress by 80 ms! In fact, the same gesture, falling midway between two syllables, can be perceived as falling on the first or second syllable, depending on which of two syllables is acoustically stressed.&lt;/p&gt;
&lt;h2 id=&#34;why-is-this-important&#34;&gt;Why is this important?&lt;/h2&gt;
&lt;p&gt;In everyday conversations, people often gesture while they speak. Small rhythmic gestures, in particular, are timed in synchrony with our speech, mostly falling on stressed syllables. However, this is no strict rule of course: people can gesture in different ways and at different times. Our study demonstrates that our brain can handle some variability in the timing of hand gestures and word stress. That is, our brain perceptually shrinks this variability: gestures that are objectively timed &amp;lsquo;out-of-sync&amp;rsquo; with speech are perceived as less &amp;lsquo;out-of-sync&amp;rsquo; because the acoustic stress &amp;lsquo;pulls on&amp;rsquo; the gesture. This means that even if you produce slightly out-of-sync gestures, your audience can still correctly interpret which words and syllables your gestures are emphasizing! Clever ay?&lt;/p&gt;
&lt;h2 id=&#34;full-reference&#34;&gt;Full reference&lt;/h2&gt;
&lt;p&gt;The full citation, open access PDF, and all data are publicly available from the links below:&lt;/p&gt;
&lt;blockquote&gt;












  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/chengjia-ye/&#34;&gt;Chengjia Ye&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/james-m.-mcqueen/&#34;&gt;James M. McQueen&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2025).
  &lt;a href=&#34;https://hrbosker.github.io/publication/ye-etal-2025-app/&#34;&gt;Effect of auditory cues to lexical stress on the visual perception of gestural timing&lt;/a&gt;.
  &lt;em&gt;Attention, Perception, &amp;amp; Psychophysics&lt;/em&gt;, doi:&lt;a href=&#34;https://doi.org/10.3758/s13414-025-03072-z&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.3758/s13414-025-03072-z&lt;/a&gt;.
  
  &lt;p&gt;








  





&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/ye-etal-2025-app/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.34973/zp61-3y60&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.3758/s13414-025-03072-z&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Paper accepted in Psychonomic Bulletin &amp; Review!</title>
      <link>https://hrbosker.github.io/news/25-04-10-rohreretal-paper-pbr/</link>
      <pubDate>Thu, 10 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/25-04-10-rohreretal-paper-pbr/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Curious about how beat gestures can affect the words people hear in Spanish? Read it in: “From ‘I dance’ to ‘she danced’ with a flick of the hands: Audiovisual stress perception in Spanish”&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;published&#34;&gt;Published!&lt;/h2&gt;
&lt;p&gt;This week, our paper “From ‘I dance’ to ‘she danced’ with a flick of the hands: Audiovisual stress perception in Spanish” was published in &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt;, authored by &lt;a href=&#34;https://hrbosker.github.io/author/patrick-louis-rohrer/&#34;&gt;Patrick Louis Rohrer&lt;/a&gt;, &lt;a href=&#34;https://hrbosker.github.io/author/ronny-bujok/&#34;&gt;Ronny Bujok&lt;/a&gt;, Lieke van Maastricht, and &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;. The full-text and all data are publicly available at the links provided at the bottom of this post.&lt;/p&gt;
&lt;h2 id=&#34;whats-it-about&#34;&gt;What&amp;rsquo;s it about?&lt;/h2&gt;
&lt;p&gt;This study explored how hand gestures can change the way people hear words in Spanish. Spanish has words that change meaning based on which syllable is stressed. For example, &lt;em&gt;bailo&lt;/em&gt; means “I dance” while &lt;em&gt;bailó&lt;/em&gt; means “she danced”, making stress patterns very important for Spanish listeners. In addition to stress, we also gesture when we speak. For instance, non-referential “beat” gestures represent simple hand movements that go up and down, marking stressed syllables in speech.&lt;/p&gt;
&lt;p&gt;What we did was to show videos of a speaker saying these types of words while using beat gestures either on the first or second syllable. Crucially, many of these words were acoustically manipulated to have varying degrees of ambiguity — that is, some words sounded more &lt;em&gt;bailo&lt;/em&gt;-like, or &lt;em&gt;bailó&lt;/em&gt;-like, or were completely ambiguous. Participants watched these videos and were asked to say which version of the word they thought they heard.&lt;/p&gt;
&lt;p&gt;The results showed that when the beat gesture matched a certain syllable, listeners were more likely to think that syllable was stressed. So if the hand moved with the first syllable, people often heard &lt;em&gt;bailo&lt;/em&gt;, but if it moved with the second, they heard &lt;em&gt;bailó&lt;/em&gt;. This effect was even stronger when the audio was unclear or ambiguous. Interestingly, a person&amp;rsquo;s working memory (an individual characteristic of how well they could remember sounds or visuals) didn&amp;rsquo;t seem to change how much they were influenced by the gestures.&lt;/p&gt;
&lt;h2 id=&#34;why-is-this-important&#34;&gt;Why is this important?&lt;/h2&gt;
&lt;p&gt;This research is important because it shows that we don&amp;rsquo;t just use our ears to understand spoken language — we also use our eyes. Simple gestures that don&amp;rsquo;t carry meaning on their own can still guide how we hear and interpret words.&lt;/p&gt;
&lt;p&gt;The findings help us understand how spoken language works in real-life settings, where gestures often come with speech. It also has implications for language learning, teaching, and even speech technology, showing that visual cues are a powerful part of communication — even when they&amp;rsquo;re just small, rhythmic hand movements.&lt;/p&gt;
&lt;h2 id=&#34;full-reference&#34;&gt;Full reference&lt;/h2&gt;
&lt;p&gt;The full citation, open access PDF, and all data are publicly available from the links below:&lt;/p&gt;
&lt;blockquote&gt;












  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/patrick-louis-rohrer/&#34;&gt;Patrick Louis Rohrer&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/ronny-bujok/&#34;&gt;Ronny Bujok&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/lieke-van-maastricht/&#34;&gt;Lieke van Maastricht&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2025).
  &lt;a href=&#34;https://hrbosker.github.io/publication/rohrer-etal-2025-pbr/&#34;&gt;From “I dance” to “she danced” with a flick of the hands: Audiovisual stress perception in Spanish&lt;/a&gt;.
  &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt;, doi:&lt;a href=&#34;https://doi.org/10.3758/s13423-025-02683-9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.3758/s13423-025-02683-9&lt;/a&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3644884_1/component/file_3644885/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/rohrer-etal-2025-pbr/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.34973/p0hn-ce44&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.3758/s13423-025-02683-9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Giulio transforms into Dr. Severijnen</title>
      <link>https://hrbosker.github.io/news/25-02-14-giulio-defends-phd/</link>
      <pubDate>Fri, 14 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/25-02-14-giulio-defends-phd/</guid>
      <description>&lt;p&gt;&lt;strong&gt;After a successful defense on February 14, 2025, Giulio was awarded the title of doctor with distinction, &lt;em&gt;cum laude&lt;/em&gt;. Congratulations to Dr. Severijnen!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;a-blessing-in-disguise&#34;&gt;A blessing in disguise&lt;/h2&gt;
&lt;p&gt;On February 14, Giulio defended his PhD thesis entitled &amp;ldquo;A blessing in disguise: How prosodic variability challenges but also aids successful speech perception&amp;rdquo;. Having calmly answered the challenging questions that were fired at him, the committee decided to award him the title of doctor with distinction, &lt;em&gt;cum laude&lt;/em&gt;, a rarely awarded mark of exceptional excellence.&lt;/p&gt;
&lt;p&gt;Many SPEAC members celebrated this milestone together with Giulio, as committee member, paranymph, or supporting audience members. I am proud of having served as Giulio&amp;rsquo;s supervisor, together with Prof. James McQueen (promotor) and Dr. Ashley Lewis (co-promotor). We can&amp;rsquo;t wait to see what the future holds for him! But it may take a while for us to get used to addressing him with &amp;ldquo;Esteemed Dr. Severijnen&amp;rdquo;, though.&lt;/p&gt;
&lt;h2 id=&#34;read-his-book&#34;&gt;Read his book!&lt;/h2&gt;
&lt;p&gt;Interested? Read his thesis here: &lt;a href=&#34;https://hdl.handle.net/2066/315703&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://hdl.handle.net/2066/315703&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Take a look inside the DCC labs!</title>
      <link>https://hrbosker.github.io/news/24-12-12-labvideo/</link>
      <pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/24-12-12-labvideo/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Watch this new video showing some of the lab facilities at the Donders Centre for Cognition (DCC).&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;lab-facilities-at-the-donders-centre-for-cognition&#34;&gt;Lab facilities at the Donders Centre for Cognition&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://www.ru.nl/en/departments/faculty-of-social-sciences/donders-centre-for-cognition&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Donders Centre for Cognition&lt;/a&gt;, part of the larger &lt;a href=&#34;https://www.ru.nl/en/donders-institute&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Donders Institute&lt;/a&gt;, have a &lt;a href=&#34;https://www.youtube.com/playlist?list=PLqgVvHZ5DK6updb0CIiyPQRCgeSjv0pyP&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Youtube playlist&lt;/a&gt; where they present their lab facilities. These are also the facilities we use in our research, focusing on audiovisual speech perception. In this latest video, many SPEAC members appear, demonstrating some of our methods and techniques. Enjoy!&lt;/p&gt;
&lt;h2 id=&#34;watch-it-here&#34;&gt;Watch it here:&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/FmtY4k6xWfk&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;br&gt;
&lt;br&gt;
&lt;br&gt;</description>
    </item>
    
    <item>
      <title>Interspeech 2025 happening in Rotterdam</title>
      <link>https://hrbosker.github.io/news/24-12-05-interspeech2025/</link>
      <pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/24-12-05-interspeech2025/</guid>
      <description>&lt;p&gt;&lt;strong&gt;The 26th edition of the Interspeech conference is to be held in Rotterdam, August 17-21, 2025.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;fair-and-inclusive-speech-science-and-technology&#34;&gt;Fair and Inclusive Speech Science and Technology&lt;/h2&gt;
&lt;p&gt;&amp;hellip;is the theme of the 2025 edition. Interspeech is a big (!) conference with lots of contributions from speech tech as well as speech science. There&amp;rsquo;ll be Challenges, Tutorials, Workshops, Special Sessions, and even a Hackathon, a &amp;lsquo;HollandPlein&amp;rsquo; for Dutch start-ups, and a Speech Science Festival. It is organized by a large crowd of people from industry and science, with many Dutch speech scientists on board trying to make this Holland edition a success. The paper submission deadline is &lt;strong&gt;February 12th, 2025&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;See &lt;a href=&#34;https://www.interspeech2025.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.interspeech2025.org&lt;/a&gt; for all the details. We hope to see you there!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ISGS 2025 in Nijmegen</title>
      <link>https://hrbosker.github.io/news/24-11-01-isgs2025/</link>
      <pubDate>Fri, 15 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/24-11-01-isgs2025/</guid>
      <description>&lt;p&gt;&lt;strong&gt;The 10th conference of the International Society for Gesture Studies (ISGS 10) is coming to Nijmegen on July 9-11, 2025.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;motion-to-meaning-innovations-in-multimodal-language-research&#34;&gt;Motion to meaning: Innovations in multimodal language research&lt;/h2&gt;
&lt;p&gt;&amp;hellip;that is the theme of the 2025 edition. ISGS 2025 aims to unite researchers from various fields and perspectives, investigating sign language, gesture use, (neuro)processing, acquisition and learning, and social interaction, among others. Keynotes are: Erica Cartmill, Tilbe Göksun, Johanna Mesch, Lorna Quandt, and Mark Turner. It is co-organized by CLS, the Donders Institute, and MPI. The paper submission deadline is &lt;strong&gt;January 15th, 2025&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;See &lt;a href=&#34;https://www.isgs10.nl/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.isgs10.nl/&lt;/a&gt; for all the details. We hope to see you there!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Several new preprints out!</title>
      <link>https://hrbosker.github.io/news/24-10-16-several-new-preprints/</link>
      <pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/24-10-16-several-new-preprints/</guid>
      <description>&lt;p&gt;&lt;strong&gt;We have a bunch of new preprints out! Read them here&amp;hellip;&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;what-are-they-about&#34;&gt;What are they about?&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://hrbosker.github.io/author/ronny-bujok/&#34;&gt;Ronny Bujok&lt;/a&gt;, Antje S. Meyer, and &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt; (2024). Beat gestures can influence on-line spoken word recognition. &lt;em&gt;PsyArXiv Preprints&lt;/em&gt;, doi:&lt;a href=&#34;https://doi.org/10.31234/osf.io/6gn3d&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.31234/osf.io/6gn3d&lt;/a&gt;, data:&lt;a href=&#34;https://osf.io/57dvh/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://osf.io/57dvh/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This preprint was posted early October and is currently under review. Using eye-tracking, it demonstrates how beat gestures can already guide spoken word recognition as the word (that the gesture is aligned to) is still unfolding over time!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://hrbosker.github.io/author/ronny-bujok/&#34;&gt;Ronny Bujok&lt;/a&gt;, &lt;a href=&#34;https://hrbosker.github.io/author/matteo-maran/&#34;&gt;Matteo Maran&lt;/a&gt;, Antje S. Meyer, and &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt; (2024). Beat gestures facilitate lexical access in constraining sentence contexts. &lt;em&gt;PsyArXiv Preprints&lt;/em&gt;, doi:&lt;a href=&#34;https://doi.org/10.31234/osf.io/8ntjm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.31234/osf.io/8ntjm&lt;/a&gt;, data:&lt;a href=&#34;https://osf.io/8vsfq/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://osf.io/8vsfq/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This preprint was also posted in October. Here, participants were asked to perform an audiovisual lexical decision task (&amp;lsquo;is this a word or not?&amp;rsquo;) on target words that were placed at the end of sentences with supporting context (e.g., &lt;em&gt;He became a colonel in the ARMY&lt;/em&gt;). Even in such already greatly facilitating contexts, seeing a beat gesture on the critical target word sped up reaction times, revealing the first behavioral influence of simple beat gestures on lexical access.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://hrbosker.github.io/author/ronny-bujok/&#34;&gt;Ronny Bujok&lt;/a&gt;, David Peeters, Antje S. Meyer, and &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt; (2024). Beating stress: evidence for recalibration of word stress perception. &lt;em&gt;PsyArXiv Preprints&lt;/em&gt;, doi:&lt;a href=&#34;https://doi.org/10.31234/osf.io/9sua6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.31234/osf.io/9sua6&lt;/a&gt;, data:&lt;a href=&#34;https://osf.io/s3p6a/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://osf.io/s3p6a/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This preprint was updated in August and is in its second round of reviews. It uses a recalibration paradigm to show how beat gestures can have a lasting impact, changing audio-only spoken word recognition minutes later!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://hrbosker.github.io/author/chengjia-ye/&#34;&gt;Chengjia Ye&lt;/a&gt;, James McQueen, and &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt; (2024). Effect of auditory cues to lexical stress on the visual perception of gestural timing. &lt;em&gt;PsyArXiv Preprints&lt;/em&gt;, doi:&lt;a href=&#34;https://doi.org/10.31234/osf.io/ykbgj&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.31234/osf.io/ykbgj&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This preprint was posted in October upon submission to a peer-reviewed journal. It provides evidence that beat gestures that are misaligned to stress (i.e., precede or follow a stressed syllable) are perceived as closer to the stressed syllable than they actually were. Even the exact same gesture timed midway between two syllables is more likely perceived as having occurred on the first syllable if the word has initial stress, but as occurring on the second syllable if the word has final stress.&lt;/p&gt;
&lt;h2 id=&#34;what-are-preprints-anyway&#34;&gt;What are preprints, anyway?&lt;/h2&gt;
&lt;p&gt;Our preprints are listed on our &lt;a href=&#34;../../publication&#34;&gt;publications page&lt;/a&gt;. They are early versions of scientific papers that have not yet been formally peer-reviewed. Hence, they are subject to change, should not be reported as conclusive, and should be considered separately from peer-reviewed publications. This also means we’re open to feedback and suggestions from interested readers. &lt;a href=&#34;../../contact&#34;&gt;Send us your comments!&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Podcast Alledaagse Vragen</title>
      <link>https://hrbosker.github.io/news/24-09-11-podcast-alledaagse-vragen/</link>
      <pubDate>Wed, 11 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/24-09-11-podcast-alledaagse-vragen/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Ik mocht een bijdrage leveren aan de podcast Alledaagse Vragen #183. Wil jij weten waarom we eigenlijk &amp;lsquo;uh&amp;rsquo; zeggen? Beluister het antwoord dan hier!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;waar-gaat-het-over&#34;&gt;Waar gaat het over?&lt;/h2&gt;
&lt;p&gt;De podcast &lt;a href=&#34;https://open.spotify.com/show/1lr6j4z70G5xke70V6nHhI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Alledaagse Vragen&lt;/a&gt; (BNNVARA) bespreekt elke aflevering een alledaagse vraag die is ingestuurd door een luisteraar. Vraag jij je af waarom pindakaas &amp;lsquo;pindakaas&amp;rsquo; heet? Of of eenden ook zeeziek kunnen worden? Dan is dit de podcast voor jou!&lt;/p&gt;
&lt;p&gt;Aflevering #183 gaat over: &lt;strong&gt;&amp;ldquo;waarom zeggen we zo vaak uhhhhhh?&amp;rdquo;&lt;/strong&gt;, en de podcastmakers vroegen of ik hier wellicht een antwoord op had. Benieuwd geworden? Luister hier naar de aflevering op Spotify (7min): &lt;a href=&#34;https://open.spotify.com/episode/2VvFF8SqnpEYhe7EbBm4Jx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://open.spotify.com/episode/2VvFF8SqnpEYhe7EbBm4Jx&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Teaching at LOT Summer School 2024</title>
      <link>https://hrbosker.github.io/news/24-06-17-lot-summer-school-2024/</link>
      <pubDate>Mon, 17 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/24-06-17-lot-summer-school-2024/</guid>
      <description>&lt;p&gt;&lt;strong&gt;I&amp;rsquo;m teaching a 5-day &lt;em&gt;Prosody in Speech Perception&lt;/em&gt; course at the LOT Summer School 2024 in Leiden. You can find all materials here!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;whats-it-about&#34;&gt;What&amp;rsquo;s it about?&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;Prosody in Speech Perception&lt;/strong&gt; course is part of the &lt;a href=&#34;https://lotschool.nl/events/lot-summer-school-2024/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LOT Summer School 2024&lt;/a&gt;. The aim of this 5-day course is to reveal the central role that prosody plays in low-level speech perception and spoken word recognition.&lt;/p&gt;
&lt;p&gt;Prosody in spoken communication generally refers to those aspects of speech that fall outside the segmental information about consonants and vowels (e.g., intonation, stress, rhythm). Still, this course will describe how suprasegmental prosody and segmental cues in speech are tightly interconnected. As such, it aims to reveal the central role that prosody plays in low-level speech perception and spoken word recognition. Each lecture targets a different processing mechanism by which prosody impacts speech perception, including general-auditory normalization, neural speech tracking, prosody-guided prediction, talker-specific learning, as well as audiovisual integration of multisensory cues to prosody. Thus, prosody - in all its different forms and appearances - is a potent factor in speech perception, determining which words and speech sounds we hear.&lt;/p&gt;
&lt;h2 id=&#34;course-materials&#34;&gt;Course materials&lt;/h2&gt;
&lt;p&gt;You can find &lt;a href=&#34;../../resources/course-materials/prosody-in-speech-perception/&#34;&gt;all course materials&lt;/a&gt; here.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>NWO PhD grant for Floris Cos!</title>
      <link>https://hrbosker.github.io/news/24-06-13-nwo-grant-floris-cos/</link>
      <pubDate>Thu, 13 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/24-06-13-nwo-grant-floris-cos/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Today we heard that Floris Cos successfully received a grant from the Dutch Research Council (NWO) to start a PhD project about how simple beat gestures influence spoken word recognition in L1 and L2.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;what-went-before&#34;&gt;What went before&amp;hellip;&lt;/h2&gt;
&lt;p&gt;Floris Cos did a research internship at the SPEAC group in 2023. For this internship, he re-tested participants who had done a manual McGurk experiment about one year earlier. When these people performed the exact same task once again, we successfully replicated the manual McGurk effect at the group level but also found a weak correlation of the by-participant effect sizes in the two identical experiments. These outcomes were presented in Leiden at Speech Prosody 2024 and published as &lt;a href=&#34;https://hrbosker.github.io/publication/cos-etal-2024-speechprosody&#34;&gt;Cos, Bujok, and Bosker (2024)&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;hooray&#34;&gt;Hooray!&lt;/h2&gt;
&lt;p&gt;After this internship, Floris applied for a &amp;lsquo;PhD in the Humanities&amp;rsquo; grant from the Dutch Research Council (NWO) to continue research into audiovisual speech perception. After a long wait, we now know he was successful and will start a PhD at the Center for Language Studies (CLS) at Radboud University in September 2024. &lt;em&gt;Congratulations, Floris!!&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;official-press-release&#34;&gt;Official press release&lt;/h2&gt;
&lt;p&gt;Check out NWO&amp;rsquo;s &lt;a href=&#34;https://www.nwo.nl/en/news/funding-for-20-new-phd-students-in-the-humanities&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;press release&lt;/a&gt; here.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Paper accepted in Language and Speech!</title>
      <link>https://hrbosker.github.io/news/24-05-13-paper-ls/</link>
      <pubDate>Mon, 13 May 2024 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/24-05-13-paper-ls/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Congratulations to Ronny on his first journal publication from his PhD project! Want to know how people use lip movements and simple hand gestures to perceive lexical stress? Read it here!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;whats-it-about&#34;&gt;What&amp;rsquo;s it about?&lt;/h2&gt;
&lt;p&gt;For Ronny&amp;rsquo;s first-year project, he video-recorded his PhD supervisor saying simple Dutch words that critically differed in which syllable carries stress: for instance &lt;em&gt;CONtent&lt;/em&gt; [noun] vs. &lt;em&gt;conTENT&lt;/em&gt; [adjective]. The talker sometimes did and sometimes didn&amp;rsquo;t produce a simple beat gesture on the word he was saying. Using audio- and video-editing techniques, Ronny created artificial videos that allowed him to play around with the auditory and visual cues to stress in all possible combinations. For instance, he created videos in which the talker&amp;rsquo;s voice was saying &lt;em&gt;CONtent&lt;/em&gt;, but the talker&amp;rsquo;s face was from a recording of &lt;em&gt;conTENT&lt;/em&gt;, while the talker&amp;rsquo;s body was from a recording where the talker produced a beat gesture on the first syllable (biasing towards &lt;em&gt;CONtent&lt;/em&gt; again). He then gave these manipulated videos to several groups of native Dutch participants and asked them what they thought the speaker said.&lt;/p&gt;
&lt;p&gt;Results showed that people primarily relied on auditory cues to stress as well as visual cues about the timing of simple hand gestures. However, despite successfully demonstrating that people could tell apart the lip movements in a &lt;em&gt;CONtent&lt;/em&gt; video from the lip movements in a &lt;em&gt;conTENT&lt;/em&gt; video (i.e., when there&amp;rsquo;s no audio), we did not find that people used those visual articulatory cues to stress in audiovisual videos (i.e., with audio).&lt;/p&gt;
&lt;h2 id=&#34;why-is-this-important&#34;&gt;Why is this important?&lt;/h2&gt;
&lt;p&gt;When it comes to perceiving vowels and consonants in speech, visual articulatory cues play an important role. Seeing a person&amp;rsquo;s face can greatly help with speech perception in noisy listening conditions and can even mislead you to hear things that weren&amp;rsquo;t said at all. Our paper, however, suggests that visual articulatory cues to stress are not heavily relied on in audiovisual speech perception. This is striking because we showed that those cues are informative and could have helped participants make their decisions. As such, these outcomes demonstrate that people weigh the multisensory cues in the audiovisual input signal differently, presumably depending on their reliability.&lt;/p&gt;
&lt;h2 id=&#34;is-that-it&#34;&gt;Is that it?&lt;/h2&gt;
&lt;p&gt;Interestingly, the timing of simple hand gestures did contribute consistently to participants&amp;rsquo; perceptual decisions: a beat gesture timed on the first syllable biased participants to hear &lt;em&gt;CONtent&lt;/em&gt;, while the same gesture but timed on the second syllable biased participants to hear &lt;em&gt;conTENT&lt;/em&gt;. This replicates earlier work from our group but importantly also extends it by using more natural gestures, more realistic presentation size, and richer stimuli (i.e., with the talker&amp;rsquo;s face visible).&lt;/p&gt;
&lt;h2 id=&#34;full-reference&#34;&gt;Full reference&lt;/h2&gt;
&lt;p&gt;The full citation, open access PDF, and all data are publicly available from the links below:&lt;/p&gt;
&lt;blockquote&gt;













&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Proceedings papers accepted for Speech Prosody 2024</title>
      <link>https://hrbosker.github.io/news/24-03-15-proceedings-papers-sp2024/</link>
      <pubDate>Fri, 15 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/24-03-15-proceedings-papers-sp2024/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Several proceedings papers from our group have been accepted for presentation at &lt;a href=&#34;https://www.universiteitleiden.nl/sp2024&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Speech Prosody 2024&lt;/a&gt;. Read them here!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;what-are-they-about&#34;&gt;What are they about?&lt;/h2&gt;
&lt;p&gt;Most of our submissions concerned the temporal synchrony between hand gestures and speech in different languages. &lt;a href=&#34;https://hrbosker.github.io/publication/rohrer-hong-etal-2024-speechprosody&#34;&gt;Rohrer, Hong, and Bosker (2024)&lt;/a&gt; is one of the first studies to test how gestures time with speech in a lexical tone language, namely Mandarin. &lt;a href=&#34;https://hrbosker.github.io/publication/rohrer-bujok-etal-2024-speechprosody&#34;&gt;Rohrer, Bujok, Van Maastricht, and Bosker (2024)&lt;/a&gt; demonstrates that seeing a gesture can change where you hear the stress in Spanish, extending the &amp;lsquo;manual McGurk effect&amp;rsquo; originally observed in Dutch to Spanish. &lt;a href=&#34;https://hrbosker.github.io/publication/cos-etal-2024-speechprosody&#34;&gt;Cos, Bujok, and Bosker (2024)&lt;/a&gt; reran an earlier manual McGurk experiment with the same participants over 1.5 years later, successfully replicating earlier work at the group level but reporting a weak correlation of the by-participant effect sizes in the two identical experiments. &lt;a href=&#34;https://hrbosker.github.io/publication/maran-bosker-2024-speechprosody&#34;&gt;Maran and Bosker (2024)&lt;/a&gt; presents a mini-test of the manual McGurk effect, allowing reliable assessment of gesture-speech integration in under 10 minutes! Finally, &lt;a href=&#34;https://hrbosker.github.io/publication/ulusahin-etal-2024-speechprosody&#34;&gt;Uluşahin, Bosker, McQueen, and Meyer (2024)&lt;/a&gt; reports two experiments investigating how knowledge about a given talker&amp;rsquo;s typical pitch influences subsequent voiceless (!) fricative perception.&lt;/p&gt;
&lt;h2 id=&#34;is-that-it&#34;&gt;Is that it?&lt;/h2&gt;
&lt;p&gt;If that wasn&amp;rsquo;t enough, there&amp;rsquo;s also two submissions accepted for Speech Prosody 2024 that didn&amp;rsquo;t stem from work performed in our group, but that were co-authored by current group members. These are &lt;strong&gt;Severijnen, Gärtner, Walther, and McQueen (2024)&lt;/strong&gt; and &lt;strong&gt;Ye and Boersma (2024)&lt;/strong&gt;. Congratulations!&lt;/p&gt;
&lt;h2 id=&#34;wanna-know-more&#34;&gt;Wanna know more?&lt;/h2&gt;
&lt;p&gt;You can find all fulltexts below, but why not come and find us at SP2024 in Leiden in July? Looking forward to seeing you there!&lt;/p&gt;
&lt;h2 id=&#34;full-references&#34;&gt;Full references&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;











  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/floris-cos/&#34;&gt;Floris Cos&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/ronny-bujok/&#34;&gt;Ronny Bujok&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2024).
  &lt;a href=&#34;https://hrbosker.github.io/publication/cos-etal-2024-speechprosody/&#34;&gt;Test-retest reliability of audiovisual lexical stress perception after &amp;gt;1.5 years&lt;/a&gt;.
  In &lt;em&gt;Proceedings of Speech Prosody 2024&lt;/em&gt;, 871-875, doi:&lt;a href=&#34;https://doi.org/10.21437/SpeechProsody.2024-176&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.21437/SpeechProsody.2024-176&lt;/a&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3580020_1/component/file_3580021/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/cos-etal-2024-speechprosody/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/nqswd/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.21437/SpeechProsody.2024-176&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;















  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/matteo-maran/&#34;&gt;Matteo Maran&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2024).
  &lt;a href=&#34;https://hrbosker.github.io/publication/maran-bosker-2024-speechprosody/&#34;&gt;How to test gesture-speech integration in ten minutes&lt;/a&gt;.
  In &lt;em&gt;Proceedings of Speech Prosody 2024&lt;/em&gt;, 737-741, doi:&lt;a href=&#34;https://doi.org/10.21437/SpeechProsody.2024-149&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.21437/SpeechProsody.2024-149&lt;/a&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3582995_1/component/file_3582996/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/maran-bosker-2024-speechprosody/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/qbyfm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.21437/SpeechProsody.2024-149&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;















  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/patrick-louis-rohrer/&#34;&gt;Patrick Louis Rohrer&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/ronny-bujok/&#34;&gt;Ronny Bujok&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/lieke-van-maastricht/&#34;&gt;Lieke van Maastricht&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2024).
  &lt;a href=&#34;https://hrbosker.github.io/publication/rohrer-bujok-etal-2024-speechprosody/&#34;&gt;The timing of beat gestures affects lexical stress perception in Spanish&lt;/a&gt;.
  In &lt;em&gt;Proceedings of Speech Prosody 2024&lt;/em&gt;, 702-706, doi:&lt;a href=&#34;https://doi.org/10.21437/SpeechProsody.2024-142&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.21437/SpeechProsody.2024-142&lt;/a&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3582989_2/component/file_3582990/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/rohrer-bujok-etal-2024-speechprosody/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/bmk2s/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.21437/SpeechProsody.2024-142&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;















  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/patrick-louis-rohrer/&#34;&gt;Patrick Louis Rohrer&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/yitian-hong/&#34;&gt;Yitian Hong&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2024).
  &lt;a href=&#34;https://hrbosker.github.io/publication/rohrer-hong-etal-2024-speechprosody/&#34;&gt;Gestures time to vowel onset and change the acoustics of the word in Mandarin&lt;/a&gt;.
  In &lt;em&gt;Proceedings of Speech Prosody 2024&lt;/em&gt;, 866-870, doi:&lt;a href=&#34;https://doi.org/10.21437/SpeechProsody.2024-175&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.21437/SpeechProsody.2024-175&lt;/a&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3582991_2/component/file_3582992/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/rohrer-hong-etal-2024-speechprosody/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/w4czh/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.21437/SpeechProsody.2024-175&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;















  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/orhun-ulusahin/&#34;&gt;Orhun Uluşahin&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/james-m.-mcqueen/&#34;&gt;James M. McQueen&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/antje-s.-meyer/&#34;&gt;Antje S. Meyer&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2024).
  &lt;a href=&#34;https://hrbosker.github.io/publication/ulusahin-etal-2024-speechprosody/&#34;&gt;Knowledge of a talker’s f0 affects subsequent perception of voiceless fricatives&lt;/a&gt;.
  In &lt;em&gt;Proceedings of Speech Prosody 2024&lt;/em&gt;, 432-436, doi:&lt;a href=&#34;https://doi.org/10.21437/SpeechProsody.2024-88&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.21437/SpeechProsody.2024-88&lt;/a&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3581041_1/component/file_3581042/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/ulusahin-etal-2024-speechprosody/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/wfp9y/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.21437/SpeechProsody.2024-88&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;


&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Paper accepted in Journal of Phonetics!</title>
      <link>https://hrbosker.github.io/news/24-03-05-paper-jphon/</link>
      <pubDate>Tue, 05 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/24-03-05-paper-jphon/</guid>
      <description>&lt;p&gt;&lt;strong&gt;How do speakers of Dutch produce lexical stress? Read it in: &amp;ldquo;Your “VOORnaam” is not my “VOORnaam”: An acoustic analysis of individual talker differences in word stress in Dutch.&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;published&#34;&gt;Published!&lt;/h2&gt;
&lt;p&gt;Our paper &lt;strong&gt;&amp;ldquo;Your “VOORnaam” is not my “VOORnaam”: An acoustic analysis of individual talker differences in word stress in Dutch&amp;rdquo;&lt;/strong&gt; has recently been published in &lt;em&gt;Journal of Phonetics&lt;/em&gt;, authored by &lt;a href=&#34;https://hrbosker.github.io/author/giulio-severijnen/&#34;&gt;Giulio Severijnen&lt;/a&gt;, &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;, and James McQueen. The fulltext and all data are publicly available at the links provided at the bottom of this post.&lt;/p&gt;
&lt;h2 id=&#34;whats-it-about&#34;&gt;What&amp;rsquo;s it about?&lt;/h2&gt;
&lt;p&gt;As part of his PhD, Giulio recorded 40 speakers of Dutch producing word pairs that critically differed in stress position, like &amp;ldquo;VOORnaam&amp;rdquo; (&lt;em&gt;first name&lt;/em&gt;) - &amp;ldquo;voorNAAM&amp;rdquo; (&lt;em&gt;respectable&lt;/em&gt;). He then measured how a syllable with stress (&amp;ldquo;VOOR-&amp;rdquo;) differs acoustically from the same syllable without stress (&amp;ldquo;voor-&amp;rdquo;). The figure above (reproducing Figure 3 from the original paper) shows that people generally tend to use pitch, intensity, and duration as primary cues to stress, but this depends in part on the context in which the word is spoken.&lt;/p&gt;
&lt;p&gt;More importantly, Giulio saw that each speaker had their own pronunciation preferences. That is, each employed a unique combination of acoustic cues to stress, illustrating large prosodic variability between talkers. In fact, classes of cue-weighting tendencies emerged, differing in which cue was used as the main cue (e.g., &amp;lsquo;pitch-speakers&amp;rsquo; vs. &amp;lsquo;intensity-speakers&amp;rsquo;). This suggests that, while there is a large amount of variability between different speakers (i.e., each talker had a unique set of cue weights), talkers do seem to cluster together regarding which cue is their main cue.&lt;/p&gt;
&lt;h2 id=&#34;why-is-this-important&#34;&gt;Why is this important?&lt;/h2&gt;
&lt;p&gt;This study is the most comprehensive acoustic description, to date, of word stress in Dutch. This is valuable information for speech scientists but also informative for speech synthesizers and automatic speech recognition (ASR) systems. Moreover, we describe large prosodic variability between individual talkers, but at the same time this variability isn&amp;rsquo;t boundless. Listeners, as well as ASR systems, may use this talker-specific information (e.g., &amp;ldquo;Johnny is a pitch-speaker&amp;rdquo;) when trying to comprehend new speech from the same talker, especially in challenging listening conditions such as in noise.&lt;/p&gt;
&lt;h2 id=&#34;full-reference&#34;&gt;Full reference&lt;/h2&gt;
&lt;blockquote&gt;












  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/giulio-severijnen/&#34;&gt;Giulio Severijnen&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/james-m.-mcqueen/&#34;&gt;James M. McQueen&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2024).
  &lt;a href=&#34;https://hrbosker.github.io/publication/severijnen-etal-2024-jphon/&#34;&gt;Your “VOORnaam” is not my “VOORnaam”: An acoustic analysis of individual talker differences in word stress in Dutch&lt;/a&gt;.
  &lt;em&gt;Journal of Phonetics, 103&lt;/em&gt;, 101296, doi:10.1016/j.wocn.2024.101296.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/pubman/item/item_3562664_1/component/file_3562665/2024-01-15_Full%20Manuscript_Production_accepted.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/severijnen-etal-2024-jphon/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://data.donders.ru.nl/login/reviewer-241214775/bbEj3LjTV3m6x6u0JQKatK2RGlMNST-jdqVIzFfHMLc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1016/j.wocn.2024.101296&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>A new PhD student joins the group: Chengjia Ye</title>
      <link>https://hrbosker.github.io/news/23-09-04-new-phd-chengjia-ye/</link>
      <pubDate>Mon, 04 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/23-09-04-new-phd-chengjia-ye/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Chengjia joined the group in Sept 2023 to test how the timing of hand gestures influences speech perception in more naturalistic listening conditions.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;when-listening-is-tough&#34;&gt;When listening is tough&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://hrbosker.github.io/author/chengjia-ye/&#34;&gt;Chengjia Ye&lt;/a&gt; will run Work Package 3 of the HearingHands ERC Starting Grant, together with &lt;a href=&#34;https://www.ru.nl/english/people/mcqueen-j/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prof. James McQueen&lt;/a&gt; (promotor) and &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Dr. Hans Rutger Bosker&lt;/a&gt;. This WP3 tests how beat gestures influence speech perception in more naturalistic listening conditions. We will assess whether the communicative relevance of a gesture influences the manual McGurk effect by making use of talking avatars, how the integration of gestural timing with spoken prosody varies depending on the saliency of the visual/auditory cues, and whether listeners adapt to variability in gesture-speech alignment.&lt;/p&gt;
&lt;p&gt;Great to have you on the team, Chengjia!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Another postdoc joins the group: Matteo Maran</title>
      <link>https://hrbosker.github.io/news/23-06-12-new-postdoc-matteo-maran/</link>
      <pubDate>Mon, 12 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/23-06-12-new-postdoc-matteo-maran/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Matteo joined the group in June 2023 to test the neural correlates of audiovisual integration of gestural timing and spoken prosody, as well as their alteration in Autism Spectrum Disorder (ASD)&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;audiovisual-integration&#34;&gt;Audiovisual integration&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://hrbosker.github.io/author/matteo-maran/&#34;&gt;Matteo Maran&lt;/a&gt; will be in charge of Work Package 2 of the HearingHands ERC Starting Grant. This WP2 tests the audiovisual integration of gestural timing with spoken prosody using electrophysiological methods. A key aim of WP2 is to assess how this integration may be altered in individuals with Autism Spectrum Disorder (ASD).&lt;/p&gt;
&lt;p&gt;Happy to have you on board, Matteo!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>New postdoc joins the group: Patrick Louis Rohrer</title>
      <link>https://hrbosker.github.io/news/23-05-16-new-postdoc-patrick-louis-rohrer/</link>
      <pubDate>Tue, 16 May 2023 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/23-05-16-new-postdoc-patrick-louis-rohrer/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Patrick joined the group in May 2023 to work on a cross-linguistic comparison of gesture-speech temporal alignment in production and perception.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;timing-gestures-and-prosody&#34;&gt;Timing gestures and prosody&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://hrbosker.github.io/author/patrick-louis-rohrer/&#34;&gt;Patrick Louis Rohrer&lt;/a&gt; will be in charge of Work Package 1 of the HearingHands ERC Starting Grant. This WP1 compares the temporal alignment of gestures and speech in languages with different prosodic regimes, including free-stress, fixed-stress, and lexical-tone languages. Next to this production strand, a perception strand investigates how the timing of seemingly meaningless gestures contributes to low-level speech perception in these same languages.&lt;/p&gt;
&lt;p&gt;Glad to have you on the team, Patrick!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Two proceedings papers accepted for ICPhS 2023</title>
      <link>https://hrbosker.github.io/news/23-04-03-proceedings-papers-icphs/</link>
      <pubDate>Mon, 03 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/23-04-03-proceedings-papers-icphs/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Two proceedings papers from our group have been accepted for presentation at ICPhS 2023. Read them here!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;rate-normalization&#34;&gt;Rate normalization&lt;/h2&gt;
&lt;p&gt;In Severijnen et al. (2023), &lt;a href=&#34;https://hrbosker.github.io/author/giulio-severijnen/&#34;&gt;Giulio Severijnen&lt;/a&gt; tested which acoustic and linguistic cues underlie rate normalization in speech perception. He observed that the tempo of Dutch context phrases can change people&amp;rsquo;s perception of vowel length in a following word: a preceding context with twice as many syllables per unit time biases people to report hearing long /a:/, a slower context towards short /ɑ/. However, this relationship between contextual speech rate and vowel length perception is not linear. That is, contexts with three times as many syllables do not lead to even more long /a:/ responses. Therefore, syllable rate is not the only determining factor in rate normalization.&lt;/p&gt;
&lt;h2 id=&#34;converging-to-f2&#34;&gt;Converging to F2&lt;/h2&gt;
&lt;p&gt;When having a conversation, interlocutors tend to sound more like each other over the course of the conversation. In Ulusahin et al. (2023), &lt;a href=&#34;https://hrbosker.github.io/author/orhun-ulusahin/&#34;&gt;Orhun Uluşahin&lt;/a&gt; set out to test the automaticity and grain-size of this phonetic convergence. He presented participants with single words that - unaware to the participants - had the second formant frequency (F2) shifted way down. When participants repeated these words back, we unfortunately did not find any downward shift in participants&amp;rsquo; own F2. As such, these results question theories which view convergence as a product of automatic integration between perception and production.&lt;/p&gt;
&lt;p&gt;Come and find these two posters this summer at ICPhS 2023!&lt;/p&gt;
&lt;h2 id=&#34;full-reference&#34;&gt;Full reference&lt;/h2&gt;
&lt;blockquote&gt;












  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/giulio-severijnen/&#34;&gt;Giulio Severijnen&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/james-m.-mcqueen/&#34;&gt;James M. McQueen&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2023).
  &lt;a href=&#34;https://hrbosker.github.io/publication/severijnen-etal-2023-icphs/&#34;&gt;Syllable rate drives rate normalization, but is not the only factor&lt;/a&gt;.
  In &lt;em&gt;Proceedings of ICPhS 2023&lt;/em&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3505424_4/component/file_3505425/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/severijnen-etal-2023-icphs/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/36anr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;












&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;
&lt;blockquote&gt;












  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/orhun-ulusahin/&#34;&gt;Orhun Uluşahin&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/james-m.-mcqueen/&#34;&gt;James M. McQueen&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/antje-s.-meyer/&#34;&gt;Antje S. Meyer&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2023).
  &lt;a href=&#34;https://hrbosker.github.io/publication/ulusahin-etal-2023-icphs/&#34;&gt;No evidence for convergence to sub-phonemic F2 shifts in shadowing&lt;/a&gt;.
  In &lt;em&gt;Proceedings of ICPhS 2023&lt;/em&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3507211_3/component/file_3507212/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/ulusahin-etal-2023-icphs/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/wb2mn&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;












&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Ivy graduates!</title>
      <link>https://hrbosker.github.io/news/16-03-22-ivy-graduation/</link>
      <pubDate>Thu, 16 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/16-03-22-ivy-graduation/</guid>
      <description>&lt;p&gt;&lt;strong&gt;On March 16, 2023, Ivy Mok successfully graduated from the Linguistics and Communication Sciences ResMA program at Radboud University.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;congratulations&#34;&gt;Congratulations!&lt;/h2&gt;
&lt;p&gt;Ivy wrote &lt;a href=&#34;https://theses.ubn.ru.nl/handle/123456789/14258&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;her MA thesis&lt;/a&gt; on lexical stress perception in noise. She tested whether listeners are flexible in how they weigh auditory and visual articulatory cues to lexical stress when listening is hard. Her thesis results showed convincingly that people weigh the visual cues to lexical stress more heavily when the speech is increasingly masked by loud background babble.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;re happy to see Ivy graduate but we&amp;rsquo;re also sad to see her leave the group. We&amp;rsquo;ll certainly miss the maple syrup biscuits and soesjes at lab meetings. Still, she won&amp;rsquo;t stray far: she&amp;rsquo;s taken up a position as project coordinator at the Multimodal Language Department of the Max Planck Institute for Psycholinguistics.&lt;/p&gt;
&lt;p&gt;Congratulations, Ivy!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>New lab members: Yitian Hong and Floris Cos</title>
      <link>https://hrbosker.github.io/news/23-02-22-new-members/</link>
      <pubDate>Thu, 23 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/23-02-22-new-members/</guid>
      <description>&lt;p&gt;&lt;strong&gt;In March 2023, we will have two new members join the group: Yitian Hong, a visiting researcher from PolyU Hong Kong; and Floris Cos, an intern from the Linguistics and Communication Sciences ResMA program at Radboud University.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;lexical-tone&#34;&gt;Lexical tone&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://hrbosker.github.io/author/yitian-hong/&#34;&gt;Yitian Hong&lt;/a&gt; will be working on a project investigating the audiovisual perception of lexical tone in Mandarin Chinese. This project will run from March - August 2023 and is in collaboration with Patrick Rohrer and &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;test-retest-reliability&#34;&gt;Test-retest reliability&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://hrbosker.github.io/author/floris-cos/&#34;&gt;Floris Cos&lt;/a&gt; will run an internship for his Linguistics and Communication Sciences ResMA program. He will assess the test-retest reliability of the Manual McGurk effect using online experimentation.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;re glad to have them join the group. Welcome!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Come join us at the &#39;Dag van de Fonetiek&#39;</title>
      <link>https://hrbosker.github.io/news/22-12-15-dagvdfonetiek2022/</link>
      <pubDate>Thu, 15 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/22-12-15-dagvdfonetiek2022/</guid>
      <description>&lt;p&gt;&lt;strong&gt;This &amp;lsquo;Phonetics Day&amp;rsquo; is the annual meeting of the &amp;lsquo;Dutch Society for Phonetic Sciences&amp;rsquo; [NVFW], taking place in Utrecht on December 16, 2022.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;open-to-all&#34;&gt;Open to all!&lt;/h2&gt;
&lt;p&gt;The &amp;lsquo;Dag van de Fonetiek&amp;rsquo; 2022 will take place on &lt;strong&gt;Friday Dec 16, 2022&lt;/strong&gt;, in the Sweelinckzaal at Drift 21, Utrecht, The Netherlands. It is an event celebrating everything &amp;lsquo;speechy&amp;rsquo; and is free and open to all: members, non-members, scientists, students, anyone! This year, you can hear &lt;a href=&#34;https://hrbosker.github.io/author/ronny-bujok/&#34;&gt;Ronny Bujok&lt;/a&gt; talk about whether beat gestures recalibrate lexical stress perception, and &lt;a href=&#34;https://hrbosker.github.io/author/orhun-ulusahin/&#34;&gt;Orhun Uluşahin&lt;/a&gt; has some intriguing results about how listeners track a talker&amp;rsquo;s pitch!&lt;/p&gt;
&lt;p&gt;See &lt;a href=&#34;https://www.nvfw.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.nvfw.org/&lt;/a&gt; for the full program. We hope to see you there!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Paper accepted in JEP:HPP!</title>
      <link>https://hrbosker.github.io/news/22-12-12-paper-jephpp/</link>
      <pubDate>Mon, 12 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/22-12-12-paper-jephpp/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Congratulazioni a Giulio e Giuseppe for successfully publishing their collaborative project &amp;ldquo;Tracking talker-specific cues to lexical stress: Evidence from perceptual learning&amp;rdquo;!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;accepted&#34;&gt;Accepted!&lt;/h2&gt;
&lt;p&gt;Today we heard that the paper &amp;ldquo;Tracking talker-specific cues to lexical stress: Evidence from perceptual learning&amp;rdquo; has been accepted for publication in the &lt;em&gt;Journal of Experimental Psychology: Human Perception and Performance (JEP:HPP)&lt;/em&gt;, authored by &lt;a href=&#34;https://hrbosker.github.io/author/giulio-severijnen/&#34;&gt;Giulio Severijnen&lt;/a&gt;, Giuseppe Di Dona, &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;, and James McQueen. The fulltext and all data are publicly available at the links provided at the bottom of this post.&lt;/p&gt;
&lt;h2 id=&#34;whats-it-about&#34;&gt;What&amp;rsquo;s it about?&lt;/h2&gt;
&lt;p&gt;The joint first-authors Giulio and Giuseppe set out to test whether listeners track how different talkers cue lexical stress. They first exposed two groups of listeners to two talkers: Talker A and B. These two talkers consistently used different cues to signal lexical stress in Dutch (e.g., differentiating &lt;em&gt;PLAto&lt;/em&gt; from &lt;em&gt;plaTEAU&lt;/em&gt;). Group 1 always heard Talker A use F0 to cue stressed syllables in Dutch, while Talker B always used intensity. Conversely, Group 2 heard the reverse talker-cue mappings: Talker A always used intensity, and Talker B always F0.&lt;/p&gt;
&lt;p&gt;After this (admittedly strange) exposure phase, participants were given an (admittedly even stranger) test phase. They were presented with audio recordings from the two talkers but this time the F0 and intensity cues had been artificially manipulated to &amp;lsquo;point in different directions&amp;rsquo;. For instance, while F0 would clearly cue stress on the first syllable of the word, intensity cues would signal stress on the second syllable. Critically, these &amp;lsquo;mixed items&amp;rsquo; were perceived by listeners according to the talker-cue mappings they had learnt during exposure. That is, Group 1 had learnt that Talker A always used F0 in the exposure phase and therefore, at test, when they heard Talker A produce a mixed item, they were more likely to perceive stress on the syllable marked by F0. However, Group 2 was more likely to perceive the exact same mixed item as having stress on the syllable marked by intensity.&lt;/p&gt;
&lt;h2 id=&#34;why-is-this-important&#34;&gt;Why is this important?&lt;/h2&gt;
&lt;p&gt;These findings support Bayesian models of spoken word recognition. These predict that listeners can adjust their prior beliefs about the perceptual weight of different phonetic cues on the basis of short-term regularities in a talker-specific fashion. This had already been observed for segmental contrasts (e.g., the perception of different consonants and vowels). Now we demonstrate that people also track suprasegmental variability in prosody, such as lexical stress.&lt;/p&gt;
&lt;h2 id=&#34;full-reference&#34;&gt;Full reference&lt;/h2&gt;
&lt;blockquote&gt;












  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/giulio-severijnen/&#34;&gt;Giulio Severijnen&lt;/a&gt;&lt;/span&gt;&lt;i class=&#34;author-notes fas fa-info-circle&#34; data-toggle=&#34;tooltip&#34; title=&#34;Equal contribution&#34;&gt;&lt;/i&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/giuseppe-di-dona/&#34;&gt;Giuseppe Di Dona&lt;/a&gt;&lt;/span&gt;&lt;i class=&#34;author-notes fas fa-info-circle&#34; data-toggle=&#34;tooltip&#34; title=&#34;Equal contribution&#34;&gt;&lt;/i&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/james-m.-mcqueen/&#34;&gt;James M. McQueen&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2023).
  &lt;a href=&#34;https://hrbosker.github.io/publication/severijnen-etal-2023-jephpp/&#34;&gt; Tracking talker-specific cues to lexical stress: Evidence from perceptual learning&lt;/a&gt;.
  &lt;em&gt;Journal of Experimental Psychology: Human Perception and Performance, 49&lt;/em&gt;(4), 549-565. doi:10.1037/xhp0001105.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3479620_2/component/file_3484213/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/severijnen-etal-2023-jephpp/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/dczx9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1037/xhp0001105&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>GESPIN 2023 in Nijmegen</title>
      <link>https://hrbosker.github.io/news/22-11-01-gespin2023/</link>
      <pubDate>Tue, 01 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/22-11-01-gespin2023/</guid>
      <description>&lt;p&gt;&lt;strong&gt;The Gesture and Speech in Interaction [GESPIN] conference is coming to Nijmegen on September 13-15, 2023.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;broadening-perspectives-integrating-views&#34;&gt;Broadening perspectives, integrating views&lt;/h2&gt;
&lt;p&gt;&amp;hellip;that is the theme of the 2023 edition. This promises a highly interdisciplinary event, approaching the interaction of gesture and speech from the perspectives of language development, neurobiology, biomechanics, animal models, and many other fields. Keynotes are: Nuria Esteve Gibert, Yifei He, Susanne Fuchs, and Franz Goller. It is co-organized by CLS, the Donders Institute, and MPI. Paper submission opens January 10th, 2023 and the deadline is &lt;strong&gt;March 15th, 2023&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;See &lt;a href=&#34;https://gespin2023.nl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gespin2023.nl&lt;/a&gt; for all the details. We hope to see you there!&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
