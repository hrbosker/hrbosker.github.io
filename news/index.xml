<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Latest News | SPEAC | Hans Rutger Bosker</title>
    <link>https://hrbosker.github.io/news/</link>
      <atom:link href="https://hrbosker.github.io/news/index.xml" rel="self" type="application/rss+xml" />
    <description>Latest News</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 12 Jun 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://hrbosker.github.io/media/logo_hu4ee931c5040e3d0fbcac8149a377b250_283935_300x300_fit_lanczos_3.png</url>
      <title>Latest News</title>
      <link>https://hrbosker.github.io/news/</link>
    </image>
    
    <item>
      <title>Another postdoc joins the group: Matteo Maran</title>
      <link>https://hrbosker.github.io/news/23-06-12-new-postdoc-matteo-maran/</link>
      <pubDate>Mon, 12 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/23-06-12-new-postdoc-matteo-maran/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Matteo joined the group in June 2023 to test the neural correlates of audiovisual integration of gestural timing and spoken prosody, as well as their alteration in Autism Spectrum Disorder (ASD)&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;audiovisual-integration&#34;&gt;Audiovisual integration&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://hrbosker.github.io/author/matteo-maran/&#34;&gt;Matteo Maran&lt;/a&gt; will be in charge of Work Package 2 of the HearingHands ERC Starting Grant. This WP2 tests the audiovisual integration of gestural timing with spoken prosody using electrophysiological methods. A key aim of WP2 is to assess how this integration may be altered in individuals with Autism Spectrum Disorder (ASD).&lt;/p&gt;
&lt;p&gt;Happy to have you on board, Matteo!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>New postdoc joins the group: Patrick Louis Rohrer</title>
      <link>https://hrbosker.github.io/news/23-05-16-new-postdoc-patrick-louis-rohrer/</link>
      <pubDate>Tue, 16 May 2023 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/23-05-16-new-postdoc-patrick-louis-rohrer/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Patrick joined the group in May 2023 to work on a cross-linguistic comparison of gesture-speech temporal alignment in production and perception.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;timing-gestures-and-prosody&#34;&gt;Timing gestures and prosody&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://hrbosker.github.io/author/patrick-louis-rohrer/&#34;&gt;Patrick Louis Rohrer&lt;/a&gt; will be in charge of Work Package 1 of the HearingHands ERC Starting Grant. This WP1 compares the temporal alignment of gestures and speech in languages with different prosodic regimes, including free-stress, fixed-stress, and lexical-tone languages. Next to this production strand, a perception strand investigates how the timing of seemingly meaningless gestures contributes to low-level speech perception in these same languages.&lt;/p&gt;
&lt;p&gt;Glad to have you on the team, Patrick!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Two proceedings papers accepted for ICPhS 2023</title>
      <link>https://hrbosker.github.io/news/23-04-03-proceedings-papers-icphs/</link>
      <pubDate>Mon, 03 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/23-04-03-proceedings-papers-icphs/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Two proceedings papers from our group have been accepted for presentation at ICPhS 2023. Read them here!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;rate-normalization&#34;&gt;Rate normalization&lt;/h2&gt;
&lt;p&gt;In Severijnen et al. (2023), &lt;a href=&#34;https://hrbosker.github.io/author/giulio-severijnen/&#34;&gt;Giulio Severijnen&lt;/a&gt; tested which acoustic and linguistic cues underlie rate normalization in speech perception. He observed that the tempo of Dutch context phrases can change people&amp;rsquo;s perception of vowel length in a following word: a preceding context with twice as many syllables per unit time biases people to report hearing long /a:/, a slower context towards short /ɑ/. However, this relationship between contextual speech rate and vowel length perception is not linear. That is, contexts with three times as many syllables do not lead to even more long /a:/ responses. Therefore, syllable rate is not the only determining factor in rate normalization.&lt;/p&gt;
&lt;h2 id=&#34;converging-to-f2&#34;&gt;Converging to F2&lt;/h2&gt;
&lt;p&gt;When having a conversation, interlocutors tend to sound more like each other over the course of the conversation. In Ulusahin et al. (2023), &lt;a href=&#34;https://hrbosker.github.io/author/orhun-ulusahin/&#34;&gt;Orhun Uluşahin&lt;/a&gt; set out to test the automaticity and grain-size of this phonetic convergence. He presented participants with single words that - unaware to the participants - had the second formant frequency (F2) shifted way down. When participants repeated these words back, we unfortunately did not find any downward shift in participants&amp;rsquo; own F2. As such, these results question theories which view convergence as a product of automatic integration between perception and production.&lt;/p&gt;
&lt;p&gt;Come and find these two posters this summer at ICPhS 2023!&lt;/p&gt;
&lt;h2 id=&#34;full-reference&#34;&gt;Full reference&lt;/h2&gt;
&lt;blockquote&gt;










  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/giulio-severijnen/&#34;&gt;Giulio Severijnen&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/james-m.-mcqueen/&#34;&gt;James M. McQueen&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2023).
  &lt;a href=&#34;https://hrbosker.github.io/publication/severijnen-etal-2023-icphs/&#34;&gt;Syllable rate drives rate normalization, but is not the only factor&lt;/a&gt;.
  In &lt;em&gt;Proceedings of ICPhS 2023&lt;/em&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3505424_4/component/file_3505425/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/severijnen-etal-2023-icphs/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/36anr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;












&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;
&lt;blockquote&gt;










  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/orhun-ulusahin/&#34;&gt;Orhun Uluşahin&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/james-m.-mcqueen/&#34;&gt;James M. McQueen&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/antje-s.-meyer/&#34;&gt;Antje S. Meyer&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2023).
  &lt;a href=&#34;https://hrbosker.github.io/publication/ulusahin-etal-2023-icphs/&#34;&gt;No evidence for convergence to sub-phonemic F2 shifts in shadowing&lt;/a&gt;.
  In &lt;em&gt;Proceedings of ICPhS 2023&lt;/em&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3507211_3/component/file_3507212/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/ulusahin-etal-2023-icphs/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/wb2mn&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;












&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Ivy graduates!</title>
      <link>https://hrbosker.github.io/news/16-03-22-ivy-graduation/</link>
      <pubDate>Thu, 16 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/16-03-22-ivy-graduation/</guid>
      <description>&lt;p&gt;&lt;strong&gt;On March 16, 2023, Ivy Mok successfully graduated from the Linguistics and Communication Sciences ResMA program at Radboud University.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;congratulations&#34;&gt;Congratulations!&lt;/h2&gt;
&lt;p&gt;Ivy wrote &lt;a href=&#34;https://theses.ubn.ru.nl/handle/123456789/14258&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;her MA thesis&lt;/a&gt; on lexical stress perception in noise. She tested whether listeners are flexible in how they weigh auditory and visual articulatory cues to lexical stress when listening is hard. Her thesis results showed convincingly that people weigh the visual cues to lexical stress more heavily when the speech is increasingly masked by loud background babble.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;re happy to see Ivy graduate but we&amp;rsquo;re also sad to see her leave the group. We&amp;rsquo;ll certainly miss the maple syrup biscuits and soesjes at lab meetings. Still, she won&amp;rsquo;t stray far: she&amp;rsquo;s taken up a position as project coordinator at the Multimodal Language Department of the Max Planck Institute for Psycholinguistics.&lt;/p&gt;
&lt;p&gt;Congratulations, Ivy!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>New lab members: Yitian Hong and Floris Cos</title>
      <link>https://hrbosker.github.io/news/23-02-22-new-members/</link>
      <pubDate>Thu, 23 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/23-02-22-new-members/</guid>
      <description>&lt;p&gt;&lt;strong&gt;In March 2023, we will have two new members join the group: Yitian Hong, a visiting researcher from PolyU Hong Kong; and Floris Cos, an intern from the Linguistics and Communication Sciences ResMA program at Radboud University.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;lexical-tone&#34;&gt;Lexical tone&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://hrbosker.github.io/author/yitian-hong/&#34;&gt;Yitian Hong&lt;/a&gt; will be working on a project investigating the audiovisual perception of lexical tone in Mandarin Chinese. This project will run from March - August 2023 and is in collaboration with Patrick Rohrer and &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;test-retest-reliability&#34;&gt;Test-retest reliability&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://hrbosker.github.io/author/floris-cos/&#34;&gt;Floris Cos&lt;/a&gt; will run an internship for his Linguistics and Communication Sciences ResMA program. He will assess the test-retest reliability of the Manual McGurk effect using online experimentation.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;re glad to have them join the group. Welcome!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Come join us at the &#39;Dag van de Fonetiek&#39;</title>
      <link>https://hrbosker.github.io/news/22-12-15-dagvdfonetiek2022/</link>
      <pubDate>Thu, 15 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/22-12-15-dagvdfonetiek2022/</guid>
      <description>&lt;p&gt;&lt;strong&gt;This &amp;lsquo;Phonetics Day&amp;rsquo; is the annual meeting of the &amp;lsquo;Dutch Society for Phonetic Sciences&amp;rsquo; [NVFW], taking place in Utrecht on December 16, 2022.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;open-to-all&#34;&gt;Open to all!&lt;/h2&gt;
&lt;p&gt;The &amp;lsquo;Dag van de Fonetiek&amp;rsquo; 2022 will take place on &lt;strong&gt;Friday Dec 16, 2022&lt;/strong&gt;, in the Sweelinckzaal at Drift 21, Utrecht, The Netherlands. It is an event celebrating everything &amp;lsquo;speechy&amp;rsquo; and is free and open to all: members, non-members, scientists, students, anyone! This year, you can hear &lt;a href=&#34;https://hrbosker.github.io/author/ronny-bujok/&#34;&gt;Ronny Bujok&lt;/a&gt; talk about whether beat gestures recalibrate lexical stress perception, and &lt;a href=&#34;https://hrbosker.github.io/author/orhun-ulusahin/&#34;&gt;Orhun Uluşahin&lt;/a&gt; has some intriguing results about how listeners track a talker&amp;rsquo;s pitch!&lt;/p&gt;
&lt;p&gt;See &lt;a href=&#34;https://www.nvfw.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.nvfw.org/&lt;/a&gt; for the full program. We hope to see you there!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Paper accepted in JEP:HPP!</title>
      <link>https://hrbosker.github.io/news/22-12-12-paper-jephpp/</link>
      <pubDate>Mon, 12 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/22-12-12-paper-jephpp/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Congratulazioni a Giulio e Giuseppe for successfully publishing their collaborative project &amp;ldquo;Tracking talker-specific cues to lexical stress: Evidence from perceptual learning&amp;rdquo;!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;accepted&#34;&gt;Accepted!&lt;/h2&gt;
&lt;p&gt;Today we heard that the paper &amp;ldquo;Tracking talker-specific cues to lexical stress: Evidence from perceptual learning&amp;rdquo; has been accepted for publication in the &lt;em&gt;Journal of Experimental Psychology: Human Perception and Performance (JEP:HPP)&lt;/em&gt;, authored by &lt;a href=&#34;https://hrbosker.github.io/author/giulio-severijnen/&#34;&gt;Giulio Severijnen&lt;/a&gt;, Giuseppe Di Dona, &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;, and James McQueen. The fulltext and all data are publicly available at the links provided at the bottom of this post.&lt;/p&gt;
&lt;h2 id=&#34;whats-it-about&#34;&gt;What&amp;rsquo;s it about?&lt;/h2&gt;
&lt;p&gt;The joint first-authors Giulio and Giuseppe set out to test whether listeners track how different talkers cue lexical stress. They first exposed two groups of listeners to two talkers: Talker A and B. These two talkers consistently used different cues to signal lexical stress in Dutch (e.g., differentiating &lt;em&gt;PLAto&lt;/em&gt; from &lt;em&gt;plaTEAU&lt;/em&gt;). Group 1 always heard Talker A use F0 to cue stressed syllables in Dutch, while Talker B always used intensity. Conversely, Group 2 heard the reverse talker-cue mappings: Talker A always used intensity, and Talker B always F0.&lt;/p&gt;
&lt;p&gt;After this (admittedly strange) exposure phase, participants were given an (admittedly even stranger) test phase. They were presented with audio recordings from the two talkers but this time the F0 and intensity cues had been artificially manipulated to &amp;lsquo;point in different directions&amp;rsquo;. For instance, while F0 would clearly cue stress on the first syllable of the word, intensity cues would signal stress on the second syllable. Critically, these &amp;lsquo;mixed items&amp;rsquo; were perceived by listeners according to the talker-cue mappings they had learnt during exposure. That is, Group 1 had learnt that Talker A always used F0 in the exposure phase and therefore, at test, when they heard Talker A produce a mixed item, they were more likely to perceive stress on the syllable marked by F0. However, Group 2 was more likely to perceive the exact same mixed item as having stress on the syllable marked by intensity.&lt;/p&gt;
&lt;h2 id=&#34;why-is-this-important&#34;&gt;Why is this important?&lt;/h2&gt;
&lt;p&gt;These findings support Bayesian models of spoken word recognition. These predict that listeners can adjust their prior beliefs about the perceptual weight of different phonetic cues on the basis of short-term regularities in a talker-specific fashion. This had already been observed for segmental contrasts (e.g., the perception of different consonants and vowels). Now we demonstrate that people also track suprasegmental variability in prosody, such as lexical stress.&lt;/p&gt;
&lt;h2 id=&#34;full-reference&#34;&gt;Full reference&lt;/h2&gt;
&lt;blockquote&gt;










  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/giulio-severijnen/&#34;&gt;Giulio Severijnen&lt;/a&gt;&lt;/span&gt;&lt;i class=&#34;author-notes fas fa-info-circle&#34; data-toggle=&#34;tooltip&#34; title=&#34;Equal contribution&#34;&gt;&lt;/i&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/giuseppe-di-dona/&#34;&gt;Giuseppe Di Dona&lt;/a&gt;&lt;/span&gt;&lt;i class=&#34;author-notes fas fa-info-circle&#34; data-toggle=&#34;tooltip&#34; title=&#34;Equal contribution&#34;&gt;&lt;/i&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/james-m.-mcqueen/&#34;&gt;James M. McQueen&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2023).
  &lt;a href=&#34;https://hrbosker.github.io/publication/severijnen-etal-2023-jephpp/&#34;&gt; Tracking talker-specific cues to lexical stress: Evidence from perceptual learning&lt;/a&gt;.
  &lt;em&gt;Journal of Experimental Psychology: Human Perception and Performance, 49&lt;/em&gt;(4), 549-565. doi:10.1037/xhp0001105.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3479620_2/component/file_3484213/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/severijnen-etal-2023-jephpp/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/dczx9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1037/xhp0001105&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>GESPIN 2023 in Nijmegen</title>
      <link>https://hrbosker.github.io/news/22-11-01-gespin2023/</link>
      <pubDate>Tue, 01 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/22-11-01-gespin2023/</guid>
      <description>&lt;p&gt;&lt;strong&gt;The Gesture and Speech in Interaction [GESPIN] conference is coming to Nijmegen on September 13-15, 2023.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;broadening-perspectives-integrating-views&#34;&gt;Broadening perspectives, integrating views&lt;/h2&gt;
&lt;p&gt;&amp;hellip;that is the theme of the 2023 edition. This promises a highly interdisciplinary event, approaching the interaction of gesture and speech from the perspectives of language development, neurobiology, biomechanics, animal models, and many other fields. Keynotes are: Nuria Esteve Gibert, Yifei He, Susanne Fuchs, and Franz Goller. It is co-organized by CLS, the Donders Institute, and MPI. Paper submission opens January 10th, 2023 and the deadline is &lt;strong&gt;March 15th, 2023&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;See &lt;a href=&#34;https://gespin2023.nl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gespin2023.nl&lt;/a&gt; for all the details. We hope to see you there!&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
