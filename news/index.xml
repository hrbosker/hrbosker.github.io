<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Latest News | SPEAC | Hans Rutger Bosker</title>
    <link>https://hrbosker.github.io/news/</link>
      <atom:link href="https://hrbosker.github.io/news/index.xml" rel="self" type="application/rss+xml" />
    <description>Latest News</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Tue, 05 Aug 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://hrbosker.github.io/media/logo_hu4ee931c5040e3d0fbcac8149a377b250_283935_300x300_fit_lanczos_3.png</url>
      <title>Latest News</title>
      <link>https://hrbosker.github.io/news/</link>
    </image>
    
    <item>
      <title>Join us at the Speech Science Festival!</title>
      <link>https://hrbosker.github.io/news/25-08-05-speech-science-festival/</link>
      <pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/25-08-05-speech-science-festival/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Come and enjoy the Speech Science Festival on August 17, 2025, in Ahoy, Rotterdam. Discover the world of voice and sound, and learn how to listen with your eyes!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;curious-about-language-technology-and-science&#34;&gt;Curious about language, technology, and science?&lt;/h2&gt;
&lt;p&gt;Join us on August 17 at the Speech Science Festival in Ahoy Rotterdam! Enjoy a fun and inspiring day filled with interactive demos presented by scientists and companies from around the world, all working in the field of speech and speech technology. Discover how we communicate, how technology understands our voices ‚Äì and much more!&lt;/p&gt;
&lt;p&gt;üìç Where? Ahoy, Rotterdam (Ahoyweg 10)&lt;/p&gt;
&lt;p&gt;üïô When? August 17, from 10:00 AM to 5:00 PM&lt;/p&gt;
&lt;p&gt;üë• Who? Everyone of any age with an interest in language, technology, and communication&lt;/p&gt;
&lt;p&gt;üí∂ How much? &lt;strong&gt;It&amp;rsquo;s free!&lt;/strong&gt; Just show up, although registration through the &lt;a href=&#34;https://festival.interspeech2025.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt; is appreciated.&lt;/p&gt;
&lt;p&gt;ü§ñ Hackathon: Are you between 10 and 18 years old and interested in robots and programming? Join the hackathon! In teams of three, you‚Äôll help robots complete an exciting space mission.&lt;/p&gt;
&lt;p&gt;üîó More info: Visit the website: &lt;a href=&#34;https://festival.interspeech2025.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://festival.interspeech2025.org&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;visit-our-stand&#34;&gt;Visit our stand!&lt;/h2&gt;
&lt;p&gt;We&amp;rsquo;re at the festival too, giving a short demo of how to listen with your eyes! At our demo &lt;strong&gt;&amp;lsquo;SAsa of saSA: wie heeft het gedaan?&amp;rsquo;&lt;/strong&gt;, you can watch some short and fun fairytales about &lt;em&gt;SAsa&lt;/em&gt; and &lt;em&gt;saSA&lt;/em&gt;, two cute mushroom heads üçÑ üçÑ. Can you tell who did what? Looking forward to seeing you there&amp;hellip; and&amp;hellip; &lt;strong&gt;there&amp;rsquo;ll be stickers!&lt;/strong&gt;&lt;/p&gt;
&lt;img src=&#34;Sticker.png&#34;&gt;</description>
    </item>
    
    <item>
      <title>Two talks at Interspeech 2025, Rotterdam</title>
      <link>https://hrbosker.github.io/news/25-07-02-talks-at-interspeech/</link>
      <pubDate>Wed, 02 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/25-07-02-talks-at-interspeech/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Two submissions from our group have been accepted as oral presentations at Interspeech 2025 (Rotterdam, 17-21 August). Both talks will present how the timing of simple gestures influences speech perception.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;comparing-gestures-made-by-avatars-and-humans&#34;&gt;Comparing gestures made by avatars and humans&lt;/h2&gt;
&lt;p&gt;One of the talks is: ‚ÄúBeat gestures made by human-like avatars affect speech perception‚Äù, authored by &lt;a href=&#34;https://hrbosker.github.io/author/matteo-maran/&#34;&gt;Matteo Maran&lt;/a&gt;, Renske R√∂tjes, Anna R. E. Schreurs and &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;. This study found that beat gestures produced by an avatar also affect word stress perception, just like human-made gestures do (though the effect was slightly reduced). The oral presentation is preliminarily planned on the morning of Thursday 21 August, and will be given by &lt;a href=&#34;https://hrbosker.github.io/author/matteo-maran/&#34;&gt;Matteo Maran&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;comparing-the-effect-of-various-ambiguous-gesture-timings&#34;&gt;Comparing the effect of various ambiguous gesture timings&lt;/h2&gt;
&lt;p&gt;The other talk is: ‚ÄúA gradient effect of hand beat timing on spoken word recognition‚Äù, authored by &lt;a href=&#34;https://hrbosker.github.io/author/chengjia-ye/&#34;&gt;Chengjia Ye&lt;/a&gt;, James M. McQueen and &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;. This study showed that the hand beat timing effect on the perception of word stress is gradient, instead of categorical. Moreover, this visual effect even surfaced when participants had received brief feedback to ignore the visual cues. This talk is preliminarily planned on the afternoon of Wednesday 20 August, and will be given by &lt;a href=&#34;https://hrbosker.github.io/author/chengjia-ye/&#34;&gt;Chengjia Ye&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;come-and-join-us&#34;&gt;Come and join us!&lt;/h2&gt;
&lt;p&gt;The two full papers will be made available online in August. They will be in the proceedings of Interspeech 2025 and will also appear on the Publications page of this website. Interested? Come and meet us in Rotterdam! We look forward to seeing you there.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>How strong is the rhythm of perception?</title>
      <link>https://hrbosker.github.io/news/25-06-20-rhythm-of-perception/</link>
      <pubDate>Fri, 20 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/25-06-20-rhythm-of-perception/</guid>
      <description>&lt;p&gt;&lt;strong&gt;With 25 teams from around the globe, we tested if a simple acoustic rhythm guides your auditory perception. Check out what we found in ‚ÄúHow strong is the rhythm of perception? A registered replication of Hickok et al. (2015)‚Äù&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;first-some-time-travel&#34;&gt;First: some time travel&lt;/h2&gt;
&lt;p&gt;In 2015, Greg Hickok, Haleh Farahbod, and Kourosh Saberi (HFS, for short) published &lt;a href=&#34;https:/doi.org/10.1177/0956797615576533&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a paper in Psychological Science&lt;/a&gt;, entitled ‚ÄúThe rhythm of perception: entrainment to acoustic rhythms induces subsequent perceptual oscillation‚Äù. They had come up with a clever new paradigm to test if listening to a short rhythmic noise stimulus would guide subsequent auditory perception. As illustrated in the figure above, they played noise that rhythmically fluctuated in amplitude (loud, quiet, loud, quiet, etc.) for 3 seconds. After those 3 seconds, the noise stopped fluctuating, staying at a fixed level. Importantly, HFS sometimes hid a very short and hard-to-hear beep inside this steady-noise part. This beep, if present, could be played at any of 9 different time points (the lightblue dots in the figure above). Their participants were instructed to say if there was a beep inside the noise, yes or no.&lt;/p&gt;
&lt;p&gt;Interestingly, &lt;strong&gt;the detection accuracy of these participants fluctuated with the rhythm of the preceding noise signal&lt;/strong&gt;. Keep in mind that the noise has exactly the same level at any of the 9 blue time points. Therefore, there&amp;rsquo;s no particular reason why you&amp;rsquo;d be better at detecting the beep at time point 3 compared to time point 5, for instance. Nevertheless, that&amp;rsquo;s what HFS found: people had low accuracy at time points 1, 5, and 9 (when the noise would have been high, if the preceding rhythm had continued) and better accuracy at time points 3 and 7 (when the noise would have been low). This suggests that the human brain aligns its perception to acoustic rhythms, with this &lt;strong&gt;perceptual rhythm&lt;/strong&gt; continuing even when the acoustic rhythm itself has already ceased.&lt;/p&gt;
&lt;h2 id=&#34;whats-the-problem&#34;&gt;What&amp;rsquo;s the problem?&lt;/h2&gt;
&lt;p&gt;The beauty of HFS&amp;rsquo;s paradigm and their impressive results made a big impact on the field. Therefore, several researchers adopted the paradigm, hoping to find the same effect. However, even though some succeeded, many &lt;em&gt;did not&lt;/em&gt; find the same outcomes, despite using a similar or even the same methods as HFS (inside info: my own attempt failed too, actually). Moreover, HFS had tested only 5 participants, and their conclusions rested on only 10% of their data. Together, this raised questions around the reliability of the reported effect and if the results actually hold up. Therefore, Molly Henry and Jonathan Peelle initiated a &lt;strong&gt;multi-lab replication attempt&lt;/strong&gt;, inviting research groups from around the globe to join the effort.&lt;/p&gt;
&lt;h2 id=&#34;what-did-you-do&#34;&gt;What did you do?&lt;/h2&gt;
&lt;p&gt;Molly and Jonathan received the audio files from HFS, made them freely available to all teams, together with a ready-made experiment protocol. All we had to do is find some people who had the guts to listen to 2,250 (!) of these audio files and each time press a button indicating if they heard a beep, yes or no. All in all, 25 teams joined forces, together testing 151 participants, collecting over 335,000 button presses in total!&lt;/p&gt;
&lt;h2 id=&#34;and-what-came-out&#34;&gt;And? What came out?&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;It replicated!&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-figure-3-from-henry-et-al-2025&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://royalsocietypublishing.org/cms/asset/2801e2e9-801b-4202-886c-b9135deac7a9/rsos.220497.f003.jpg&#34; alt=&#34;Figure 3 from Henry et al. (2025)&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Figure 3 from Henry et al. (2025)
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;In the topright panel (a) of Figure 3 above, you can see that the detection accuracy across all participants (the thick black line) indeed fluctuated at the rhythm of the preceding noise signal! However, in panel (c) below it, you can also see that individual participants varied a lot in their &amp;lsquo;perceptual rhythm&amp;rsquo;: some indeed showed evidence for the effect (large positive effect sizes), but many only showed a small effect, or none at all (or even in the opposite direction). It&amp;rsquo;s only when analyzing across participants that we see reliable evidence for a rhythm in auditory perception.&lt;/p&gt;
&lt;h2 id=&#34;why-is-this-important&#34;&gt;Why is this important?&lt;/h2&gt;
&lt;p&gt;This multi-lab collaboration is, in my view, an &lt;strong&gt;excellent example of open team science&lt;/strong&gt;. It ticks all the boxes: all methods were pre-registered; the study was externally reviewed before data collection started; data acquisition, analysis, and archiving is fully transparent and open; state-of-the-art analyses were applied; and working together with teams from across the globe was so much fun! Even independent from the outcomes, this team science project on its own already demonstrates how science should be done.&lt;/p&gt;
&lt;p&gt;The fact that the effect replicated was actually a bit of a surprise to me. The individual variability shows the effect is pretty fragile: only when testing lots of people, who each take part in multiple sessions, making up 1000s of trials, do we indeed find evidence for a &amp;lsquo;perceptual rhythm&amp;rsquo; in auditory perception. This also raises questions about the &lt;strong&gt;theoretical implications&lt;/strong&gt; of this finding as well as the &lt;strong&gt;suitability of the paradigm&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;That is: if we play highly artificial noise signals to 1000s of people, we find evidence for a &amp;lsquo;perceptual rhythm&amp;rsquo;. But few sounds in real life are this rhythmic: music and speech are both pseudo-rhythmic auditory signals with certain temporal regularities, but they are no way near as regularly rhythmic as these noise sounds. Does the &amp;lsquo;perceptual rhythm&amp;rsquo;, as demonstrated here, then actually have any &lt;strong&gt;implications for our understanding of human music and speech perception&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;Moreover, imagine you want to run an experiment on human auditory perception. &lt;strong&gt;Would you use this paradigm?&lt;/strong&gt; If your resources are limited (and they often are), you&amp;rsquo;d not probably not, right? You&amp;rsquo;d need to test lots of people, in multiple sessions, each involving hundreds of trials&amp;hellip; and even then you&amp;rsquo;ll probably only find a small effect. Therefore, even though this team effort was indeed successful, it at the same time also reveals the limitations of the paradigm, as originally presented by HFS.&lt;/p&gt;
&lt;h2 id=&#34;full-reference&#34;&gt;Full reference&lt;/h2&gt;
&lt;p&gt;The full citation, open access PDF, and all data are publicly available from the links below:&lt;/p&gt;
&lt;blockquote&gt;












  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/molly-j.-henry/&#34;&gt;Molly J. Henry&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/jonas-obleser/&#34;&gt;Jonas Obleser&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/maria-r.-crusey/&#34;&gt;Maria R. Crusey&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/...et-al./&#34;&gt;(...et al.)&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/giulio-severijnen/&#34;&gt;Giulio Severijnen&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/...et-alii/&#34;&gt;(...et alii)&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/jonathan-r.-peelle/&#34;&gt;Jonathan R. Peelle&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2025).
  &lt;a href=&#34;https://hrbosker.github.io/publication/henry-etal-2025-roysocopenscience/&#34;&gt;How strong is the rhythm of perception? A registered replication of Hickok et al. (2015)&lt;/a&gt;.
  &lt;em&gt;Royal Society Open Science, 12&lt;/em&gt;(6), 220497, doi:&lt;a href=&#34;https://doi.org/10.1098/rsos.220497&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.1098/rsos.220497&lt;/a&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/pubman/item/item_3653884_1/component/file_3653885/Henry_etal_2025_how%20strong%20is%20the....pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/henry-etal-2025-roysocopenscience/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/aytez/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1098/rsos.220497&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Bujok et al. (2025) accepted in Attention, Perception, &amp; Psychophysics!</title>
      <link>https://hrbosker.github.io/news/25-05-21-bujoketal-paper-app/</link>
      <pubDate>Wed, 21 May 2025 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/25-05-21-bujoketal-paper-app/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Did you know watching a few beat gestures can change which words you hear minutes later? Read all about it in: ‚ÄúBeating stress: Evidence for recalibration of word stress perception‚Äù&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;published&#34;&gt;Published!&lt;/h2&gt;
&lt;p&gt;Our new paper ‚ÄúBeating stress: Evidence for recalibration of word stress perception‚Äù authored by &lt;a href=&#34;https://hrbosker.github.io/author/ronny-bujok/&#34;&gt;Ronny Bujok&lt;/a&gt;, David Peeters, Antje Meyer, and &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt; was published in &lt;em&gt;Attention, Perception &amp;amp; Psychophysics&lt;/em&gt;. The full text and all supporting data are publicly available from links at the bottom of this post.&lt;/p&gt;
&lt;h2 id=&#34;whats-it-about&#34;&gt;What&amp;rsquo;s it about?&lt;/h2&gt;
&lt;p&gt;In this study, we tested if beat gestures could guide &lt;em&gt;speech adaptation&lt;/em&gt;, to help you understand a speaker who happens to produce &amp;lsquo;atypical&amp;rsquo; acoustic cues to lexical stress. In previous studies, we had found that beat gestures are used as a visual indicator of the position of lexical stress within a word and can thus change our perception of that word in-the-moment (&lt;a href=&#34;../24-05-13-paper-ls/&#34;&gt;Bujok et al., 2025&lt;/a&gt;). For example, hearing the Dutch word /vo:r.na:m/, while seeing a beat gesture on the first syllable biases the listener to perceive &lt;em&gt;VOORnaam&lt;/em&gt; with stress on the first syllable (meaning &amp;ldquo;first name&amp;rdquo;; rather than &lt;em&gt;voorNAAM&lt;/em&gt; with stress on the second syllable, meaning &amp;ldquo;respectable&amp;rdquo;). In the now published paper, we decided to investigate if this effect of beat gestures also leads to lasting changes in our perception.&lt;/p&gt;
&lt;p&gt;We ran a new experiment in which participants first passively watched a set of short videos. In this &amp;rsquo;exposure phase&amp;rsquo;, participants saw a Dutch speaker who produced &lt;em&gt;ambiguously stressed words&lt;/em&gt; (e.g., perceptually in between &lt;em&gt;VOORnaam&lt;/em&gt; and &lt;em&gt;voorNAAM&lt;/em&gt;). Importantly, one group of participants heard these words paired with a video of the talker consistently producing a beat gesture on the first syllable &lt;em&gt;voor-&lt;/em&gt;. This way, we hoped that this group would learn that this speaker happens to produce kinda &amp;lsquo;odd speech&amp;rsquo; when intending to produce word-initial stress. The other group of participants heard the exact same speech, but they saw the talker consistently produce a beat gesture on the second syllable &lt;em&gt;-naam&lt;/em&gt;. They were expected to learn that this speaker happens to produce &amp;lsquo;odd speech&amp;rsquo; when intending to produce word-final stress.&lt;/p&gt;
&lt;p&gt;After simply watching 48 of such videos, people were given a new task. In this &amp;rsquo;test phase&amp;rsquo;, people got audio-only speech (no videos, just audio) of the same talker producing words that fell perceptually roughly in between &lt;em&gt;VOORnaam&lt;/em&gt; and &lt;em&gt;voorNAAM&lt;/em&gt;. Their task was to categorize this ambiguous audio as either having stress on the first or second syllable. Importantly, both groups performed the same audio-only test phase, listening to the exact same speech. &lt;em&gt;Yet our results showed different performance between the two groups.&lt;/em&gt; The first group, who had consistently seen a beat gesture on the first syllable in the exposure phase, were more likely to perceive lexical stress on the first syllable in the subsequent &lt;em&gt;audio-only&lt;/em&gt; test phase. In contrast, the other group, who had seen beat gestures on the second syllable in the exposure phase, were more likely to perceive the exact same audio as having stress on the second syllable!&lt;/p&gt;
&lt;h2 id=&#34;why-is-this-important&#34;&gt;Why is this important?&lt;/h2&gt;
&lt;p&gt;This research shows that beat gestures, the kind of gestures we use the most, can guide speech adaptation and lead to lasting changes in later perception. This is important because speech is infinitely variable: we all speak differently. Dealing with this variability is crucial for successful communication. Our findings help us understand how we are able to use visual cues, such as beat gestures, to solve this variability problem. That is, we can use baet gestures to help us learn about how a given talker tends to speak. Moreover, it highlights speech, and speech learning, is a multimodal phenomenon involving sound, lips, and hands. üëã&lt;/p&gt;
&lt;h2 id=&#34;full-reference&#34;&gt;Full reference&lt;/h2&gt;
&lt;p&gt;The full citation, open access PDF, and all data are publicly available from the links below:&lt;/p&gt;
&lt;blockquote&gt;












  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/ronny-bujok/&#34;&gt;Ronny Bujok&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/david-peeters/&#34;&gt;David Peeters&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/antje-s.-meyer/&#34;&gt;Antje S. Meyer&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2025).
  &lt;a href=&#34;https://hrbosker.github.io/publication/bujok-etal-2025-app/&#34;&gt;Beating stress: evidence for recalibration of word stress perception&lt;/a&gt;.
  &lt;em&gt;Attention, Perception, &amp;amp; Psychophysics 87&lt;/em&gt;, 1729-1749. doi:&lt;a href=&#34;https://doi.org/10.3758/s13414-025-03088-5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.3758/s13414-025-03088-5&lt;/a&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://link.springer.com/content/pdf/10.3758/s13414-025-03088-5.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/bujok-etal-2025-app/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/s3p6a/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.3758/s13414-025-03088-5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Ye et al. (2025) published in Attention, Perception, &amp; Psychophysics!</title>
      <link>https://hrbosker.github.io/news/25-05-06-yeetal-paper-app/</link>
      <pubDate>Tue, 06 May 2025 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/25-05-06-yeetal-paper-app/</guid>
      <description>&lt;p&gt;&lt;strong&gt;The same gesture can be perceived differently, depending on the speech you hear!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;published&#34;&gt;Published!&lt;/h2&gt;
&lt;p&gt;Our new paper ‚ÄúEffect of auditory cues to lexical stress on the visual perception of gestural timing‚Äù, authored by &lt;a href=&#34;https://hrbosker.github.io/author/chengjia-ye/&#34;&gt;Chengjia Ye&lt;/a&gt;, &lt;a href=&#34;https://www.ru.nl/en/people/mcqueen-j&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;James McQueen&lt;/a&gt; and &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;, is now publicly available online! It&amp;rsquo;s published Open Access in the journal &lt;em&gt;Attention, Perception, &amp;amp; Psychophysics&lt;/em&gt;. Feel free to check it out: links are given at the bottom of the page.&lt;/p&gt;
&lt;h2 id=&#34;whats-it-about&#34;&gt;What&amp;rsquo;s it about?&lt;/h2&gt;
&lt;p&gt;We ran two experiments to see how native listeners of Dutch use auditory cues to lexical stress when perceiving the timing of hand beats. Previous studies of our lab have demonstrated that the timing of hand beats can influence the perception of lexical stress, and thus spoken-word recognition. In this study, we examined &amp;lsquo;a reverse effect&amp;rsquo;: does lexical stress change how you perceive the visual timing of hand gestures? Our results showed that the perceived timing of a hand beat is indeed &amp;lsquo;pulled towards&amp;rsquo; the syllable with stronger stress cues. For example, our participants perceived an &amp;rsquo;early gesture&amp;rsquo;, objectively preceding a stressed syllable by 200 ms, to only precede the stress by 80 ms! In fact, the same gesture, falling midway between two syllables, can be perceived as falling on the first or second syllable, depending on which of two syllables is acoustically stressed.&lt;/p&gt;
&lt;h2 id=&#34;why-is-this-important&#34;&gt;Why is this important?&lt;/h2&gt;
&lt;p&gt;In everyday conversations, people often gesture while they speak. Small rhythmic gestures, in particular, are timed in synchrony with our speech, mostly falling on stressed syllables. However, this is no strict rule of course: people can gesture in different ways and at different times. Our study demonstrates that our brain can handle some variability in the timing of hand gestures and word stress. That is, our brain perceptually shrinks this variability: gestures that are objectively timed &amp;lsquo;out-of-sync&amp;rsquo; with speech are perceived as less &amp;lsquo;out-of-sync&amp;rsquo; because the acoustic stress &amp;lsquo;pulls on&amp;rsquo; the gesture. This means that even if you produce slightly out-of-sync gestures, your audience can still correctly interpret which words and syllables your gestures are emphasizing! Clever ay?&lt;/p&gt;
&lt;h2 id=&#34;full-reference&#34;&gt;Full reference&lt;/h2&gt;
&lt;p&gt;The full citation, open access PDF, and all data are publicly available from the links below:&lt;/p&gt;
&lt;blockquote&gt;












  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/chengjia-ye/&#34;&gt;Chengjia Ye&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/james-m.-mcqueen/&#34;&gt;James M. McQueen&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2025).
  &lt;a href=&#34;https://hrbosker.github.io/publication/ye-etal-2025-app/&#34;&gt;Effect of auditory cues to lexical stress on the visual perception of gestural timing&lt;/a&gt;.
  &lt;em&gt;Attention, Perception, &amp;amp; Psychophysics 87&lt;/em&gt;, 2207-2222, doi:&lt;a href=&#34;https://doi.org/10.3758/s13414-025-03072-z&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.3758/s13414-025-03072-z&lt;/a&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/pubman/item/item_3649569_1/component/file_3649570/Ye_etal_2025_effect%20of%20auditory%20cues%20to....pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/ye-etal-2025-app/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.34973/zp61-3y60&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.3758/s13414-025-03072-z&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Paper accepted in Psychonomic Bulletin &amp; Review!</title>
      <link>https://hrbosker.github.io/news/25-04-10-rohreretal-paper-pbr/</link>
      <pubDate>Thu, 10 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/25-04-10-rohreretal-paper-pbr/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Curious about how beat gestures can affect the words people hear in Spanish? Read it in: ‚ÄúFrom ‚ÄòI dance‚Äô to ‚Äòshe danced‚Äô with a flick of the hands: Audiovisual stress perception in Spanish‚Äù&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;published&#34;&gt;Published!&lt;/h2&gt;
&lt;p&gt;This week, our paper ‚ÄúFrom ‚ÄòI dance‚Äô to ‚Äòshe danced‚Äô with a flick of the hands: Audiovisual stress perception in Spanish‚Äù was published in &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt;, authored by &lt;a href=&#34;https://hrbosker.github.io/author/patrick-louis-rohrer/&#34;&gt;Patrick Louis Rohrer&lt;/a&gt;, &lt;a href=&#34;https://hrbosker.github.io/author/ronny-bujok/&#34;&gt;Ronny Bujok&lt;/a&gt;, Lieke van Maastricht, and &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;. The full-text and all data are publicly available at the links provided at the bottom of this post.&lt;/p&gt;
&lt;h2 id=&#34;whats-it-about&#34;&gt;What&amp;rsquo;s it about?&lt;/h2&gt;
&lt;p&gt;This study explored how hand gestures can change the way people hear words in Spanish. Spanish has words that change meaning based on which syllable is stressed. For example, &lt;em&gt;bailo&lt;/em&gt; means ‚ÄúI dance‚Äù while &lt;em&gt;bail√≥&lt;/em&gt; means ‚Äúshe danced‚Äù, making stress patterns very important for Spanish listeners. In addition to stress, we also gesture when we speak. For instance, non-referential ‚Äúbeat‚Äù gestures represent simple hand movements that go up and down, marking stressed syllables in speech.&lt;/p&gt;
&lt;p&gt;What we did was to show videos of a speaker saying these types of words while using beat gestures either on the first or second syllable. Crucially, many of these words were acoustically manipulated to have varying degrees of ambiguity ‚Äî that is, some words sounded more &lt;em&gt;bailo&lt;/em&gt;-like, or &lt;em&gt;bail√≥&lt;/em&gt;-like, or were completely ambiguous. Participants watched these videos and were asked to say which version of the word they thought they heard.&lt;/p&gt;
&lt;p&gt;The results showed that when the beat gesture matched a certain syllable, listeners were more likely to think that syllable was stressed. So if the hand moved with the first syllable, people often heard &lt;em&gt;bailo&lt;/em&gt;, but if it moved with the second, they heard &lt;em&gt;bail√≥&lt;/em&gt;. This effect was even stronger when the audio was unclear or ambiguous. Interestingly, a person&amp;rsquo;s working memory (an individual characteristic of how well they could remember sounds or visuals) didn&amp;rsquo;t seem to change how much they were influenced by the gestures.&lt;/p&gt;
&lt;h2 id=&#34;why-is-this-important&#34;&gt;Why is this important?&lt;/h2&gt;
&lt;p&gt;This research is important because it shows that we don&amp;rsquo;t just use our ears to understand spoken language ‚Äî we also use our eyes. Simple gestures that don&amp;rsquo;t carry meaning on their own can still guide how we hear and interpret words.&lt;/p&gt;
&lt;p&gt;The findings help us understand how spoken language works in real-life settings, where gestures often come with speech. It also has implications for language learning, teaching, and even speech technology, showing that visual cues are a powerful part of communication ‚Äî even when they&amp;rsquo;re just small, rhythmic hand movements.&lt;/p&gt;
&lt;h2 id=&#34;full-reference&#34;&gt;Full reference&lt;/h2&gt;
&lt;p&gt;The full citation, open access PDF, and all data are publicly available from the links below:&lt;/p&gt;
&lt;blockquote&gt;












  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/patrick-louis-rohrer/&#34;&gt;Patrick Louis Rohrer&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/ronny-bujok/&#34;&gt;Ronny Bujok&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/lieke-van-maastricht/&#34;&gt;Lieke van Maastricht&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2025).
  &lt;a href=&#34;https://hrbosker.github.io/publication/rohrer-etal-2025-pbr/&#34;&gt;From ‚ÄúI dance‚Äù to ‚Äúshe danced‚Äù with a flick of the hands: Audiovisual stress perception in Spanish&lt;/a&gt;.
  &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt;, doi:&lt;a href=&#34;https://doi.org/10.3758/s13423-025-02683-9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.3758/s13423-025-02683-9&lt;/a&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3644884_1/component/file_3644885/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/rohrer-etal-2025-pbr/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.34973/p0hn-ce44&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.3758/s13423-025-02683-9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Giulio transforms into Dr. Severijnen</title>
      <link>https://hrbosker.github.io/news/25-02-14-giulio-defends-phd/</link>
      <pubDate>Fri, 14 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/25-02-14-giulio-defends-phd/</guid>
      <description>&lt;p&gt;&lt;strong&gt;After a successful defense on February 14, 2025, Giulio was awarded the title of doctor with distinction, &lt;em&gt;cum laude&lt;/em&gt;. Congratulations to Dr. Severijnen!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;a-blessing-in-disguise&#34;&gt;A blessing in disguise&lt;/h2&gt;
&lt;p&gt;On February 14, Giulio defended his PhD thesis entitled &amp;ldquo;A blessing in disguise: How prosodic variability challenges but also aids successful speech perception&amp;rdquo;. Having calmly answered the challenging questions that were fired at him, the committee decided to award him the title of doctor with distinction, &lt;em&gt;cum laude&lt;/em&gt;, a rarely awarded mark of exceptional excellence.&lt;/p&gt;
&lt;p&gt;Many SPEAC members celebrated this milestone together with Giulio, as committee member, paranymph, or supporting audience members. I am proud of having served as Giulio&amp;rsquo;s supervisor, together with Prof. James McQueen (promotor) and Dr. Ashley Lewis (co-promotor). We can&amp;rsquo;t wait to see what the future holds for him! But it may take a while for us to get used to addressing him with &amp;ldquo;Esteemed Dr. Severijnen&amp;rdquo;, though.&lt;/p&gt;
&lt;h2 id=&#34;read-his-book&#34;&gt;Read his book!&lt;/h2&gt;
&lt;p&gt;Interested? Read his thesis here: &lt;a href=&#34;https://hdl.handle.net/2066/315703&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://hdl.handle.net/2066/315703&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Take a look inside the DCC labs!</title>
      <link>https://hrbosker.github.io/news/24-12-12-labvideo/</link>
      <pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/24-12-12-labvideo/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Watch this new video showing some of the lab facilities at the Donders Centre for Cognition (DCC).&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;lab-facilities-at-the-donders-centre-for-cognition&#34;&gt;Lab facilities at the Donders Centre for Cognition&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://www.ru.nl/en/departments/faculty-of-social-sciences/donders-centre-for-cognition&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Donders Centre for Cognition&lt;/a&gt;, part of the larger &lt;a href=&#34;https://www.ru.nl/en/donders-institute&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Donders Institute&lt;/a&gt;, have a &lt;a href=&#34;https://www.youtube.com/playlist?list=PLqgVvHZ5DK6updb0CIiyPQRCgeSjv0pyP&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Youtube playlist&lt;/a&gt; where they present their lab facilities. These are also the facilities we use in our research, focusing on audiovisual speech perception. In this latest video, many SPEAC members appear, demonstrating some of our methods and techniques. Enjoy!&lt;/p&gt;
&lt;h2 id=&#34;watch-it-here&#34;&gt;Watch it here:&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/FmtY4k6xWfk&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;br&gt;
&lt;br&gt;
&lt;br&gt;</description>
    </item>
    
    <item>
      <title>Interspeech 2025 happening in Rotterdam</title>
      <link>https://hrbosker.github.io/news/24-12-05-interspeech2025/</link>
      <pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/24-12-05-interspeech2025/</guid>
      <description>&lt;p&gt;&lt;strong&gt;The 26th edition of the Interspeech conference is to be held in Rotterdam, August 17-21, 2025.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;fair-and-inclusive-speech-science-and-technology&#34;&gt;Fair and Inclusive Speech Science and Technology&lt;/h2&gt;
&lt;p&gt;&amp;hellip;is the theme of the 2025 edition. Interspeech is a big (!) conference with lots of contributions from speech tech as well as speech science. There&amp;rsquo;ll be Challenges, Tutorials, Workshops, Special Sessions, and even a Hackathon, a &amp;lsquo;HollandPlein&amp;rsquo; for Dutch start-ups, and a Speech Science Festival. It is organized by a large crowd of people from industry and science, with many Dutch speech scientists on board trying to make this Holland edition a success. The paper submission deadline is &lt;strong&gt;February 12th, 2025&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;See &lt;a href=&#34;https://www.interspeech2025.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.interspeech2025.org&lt;/a&gt; for all the details. We hope to see you there!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ISGS 2025 in Nijmegen</title>
      <link>https://hrbosker.github.io/news/24-11-01-isgs2025/</link>
      <pubDate>Fri, 15 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/24-11-01-isgs2025/</guid>
      <description>&lt;p&gt;&lt;strong&gt;The 10th conference of the International Society for Gesture Studies (ISGS 10) is coming to Nijmegen on July 9-11, 2025.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;motion-to-meaning-innovations-in-multimodal-language-research&#34;&gt;Motion to meaning: Innovations in multimodal language research&lt;/h2&gt;
&lt;p&gt;&amp;hellip;that is the theme of the 2025 edition. ISGS 2025 aims to unite researchers from various fields and perspectives, investigating sign language, gesture use, (neuro)processing, acquisition and learning, and social interaction, among others. Keynotes are: Erica Cartmill, Tilbe G√∂ksun, Johanna Mesch, Lorna Quandt, and Mark Turner. It is co-organized by CLS, the Donders Institute, and MPI. The paper submission deadline is &lt;strong&gt;January 15th, 2025&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;See &lt;a href=&#34;https://www.isgs10.nl/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.isgs10.nl/&lt;/a&gt; for all the details. We hope to see you there!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Several new preprints out!</title>
      <link>https://hrbosker.github.io/news/24-10-16-several-new-preprints/</link>
      <pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/24-10-16-several-new-preprints/</guid>
      <description>&lt;p&gt;&lt;strong&gt;We have a bunch of new preprints out! Read them here&amp;hellip;&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;what-are-they-about&#34;&gt;What are they about?&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://hrbosker.github.io/author/ronny-bujok/&#34;&gt;Ronny Bujok&lt;/a&gt;, Antje S. Meyer, and &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt; (2024). Beat gestures can influence on-line spoken word recognition. &lt;em&gt;PsyArXiv Preprints&lt;/em&gt;, doi:&lt;a href=&#34;https://doi.org/10.31234/osf.io/6gn3d&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.31234/osf.io/6gn3d&lt;/a&gt;, data:&lt;a href=&#34;https://osf.io/57dvh/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://osf.io/57dvh/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This preprint was posted early October and is currently under review. Using eye-tracking, it demonstrates how beat gestures can already guide spoken word recognition as the word (that the gesture is aligned to) is still unfolding over time!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://hrbosker.github.io/author/ronny-bujok/&#34;&gt;Ronny Bujok&lt;/a&gt;, &lt;a href=&#34;https://hrbosker.github.io/author/matteo-maran/&#34;&gt;Matteo Maran&lt;/a&gt;, Antje S. Meyer, and &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt; (2024). Beat gestures facilitate lexical access in constraining sentence contexts. &lt;em&gt;PsyArXiv Preprints&lt;/em&gt;, doi:&lt;a href=&#34;https://doi.org/10.31234/osf.io/8ntjm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.31234/osf.io/8ntjm&lt;/a&gt;, data:&lt;a href=&#34;https://osf.io/8vsfq/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://osf.io/8vsfq/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This preprint was also posted in October. Here, participants were asked to perform an audiovisual lexical decision task (&amp;lsquo;is this a word or not?&amp;rsquo;) on target words that were placed at the end of sentences with supporting context (e.g., &lt;em&gt;He became a colonel in the ARMY&lt;/em&gt;). Even in such already greatly facilitating contexts, seeing a beat gesture on the critical target word sped up reaction times, revealing the first behavioral influence of simple beat gestures on lexical access.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://hrbosker.github.io/author/ronny-bujok/&#34;&gt;Ronny Bujok&lt;/a&gt;, David Peeters, Antje S. Meyer, and &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt; (2024). Beating stress: evidence for recalibration of word stress perception. &lt;em&gt;PsyArXiv Preprints&lt;/em&gt;, doi:&lt;a href=&#34;https://doi.org/10.31234/osf.io/9sua6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.31234/osf.io/9sua6&lt;/a&gt;, data:&lt;a href=&#34;https://osf.io/s3p6a/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://osf.io/s3p6a/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This preprint was updated in August and is in its second round of reviews. It uses a recalibration paradigm to show how beat gestures can have a lasting impact, changing audio-only spoken word recognition minutes later!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://hrbosker.github.io/author/chengjia-ye/&#34;&gt;Chengjia Ye&lt;/a&gt;, James McQueen, and &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt; (2024). Effect of auditory cues to lexical stress on the visual perception of gestural timing. &lt;em&gt;PsyArXiv Preprints&lt;/em&gt;, doi:&lt;a href=&#34;https://doi.org/10.31234/osf.io/ykbgj&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.31234/osf.io/ykbgj&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This preprint was posted in October upon submission to a peer-reviewed journal. It provides evidence that beat gestures that are misaligned to stress (i.e., precede or follow a stressed syllable) are perceived as closer to the stressed syllable than they actually were. Even the exact same gesture timed midway between two syllables is more likely perceived as having occurred on the first syllable if the word has initial stress, but as occurring on the second syllable if the word has final stress.&lt;/p&gt;
&lt;h2 id=&#34;what-are-preprints-anyway&#34;&gt;What are preprints, anyway?&lt;/h2&gt;
&lt;p&gt;Our preprints are listed on our &lt;a href=&#34;../../publication&#34;&gt;publications page&lt;/a&gt;. They are early versions of scientific papers that have not yet been formally peer-reviewed. Hence, they are subject to change, should not be reported as conclusive, and should be considered separately from peer-reviewed publications. This also means we‚Äôre open to feedback and suggestions from interested readers. &lt;a href=&#34;../../contact&#34;&gt;Send us your comments!&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Podcast Alledaagse Vragen</title>
      <link>https://hrbosker.github.io/news/24-09-11-podcast-alledaagse-vragen/</link>
      <pubDate>Wed, 11 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/24-09-11-podcast-alledaagse-vragen/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Ik mocht een bijdrage leveren aan de podcast Alledaagse Vragen #183. Wil jij weten waarom we eigenlijk &amp;lsquo;uh&amp;rsquo; zeggen? Beluister het antwoord dan hier!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;waar-gaat-het-over&#34;&gt;Waar gaat het over?&lt;/h2&gt;
&lt;p&gt;De podcast &lt;a href=&#34;https://open.spotify.com/show/1lr6j4z70G5xke70V6nHhI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Alledaagse Vragen&lt;/a&gt; (BNNVARA) bespreekt elke aflevering een alledaagse vraag die is ingestuurd door een luisteraar. Vraag jij je af waarom pindakaas &amp;lsquo;pindakaas&amp;rsquo; heet? Of of eenden ook zeeziek kunnen worden? Dan is dit de podcast voor jou!&lt;/p&gt;
&lt;p&gt;Aflevering #183 gaat over: &lt;strong&gt;&amp;ldquo;waarom zeggen we zo vaak uhhhhhh?&amp;rdquo;&lt;/strong&gt;, en de podcastmakers vroegen of ik hier wellicht een antwoord op had. Benieuwd geworden? Luister hier naar de aflevering op Spotify (7min): &lt;a href=&#34;https://open.spotify.com/episode/2VvFF8SqnpEYhe7EbBm4Jx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://open.spotify.com/episode/2VvFF8SqnpEYhe7EbBm4Jx&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Teaching at LOT Summer School 2024</title>
      <link>https://hrbosker.github.io/news/24-06-17-lot-summer-school-2024/</link>
      <pubDate>Mon, 17 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/24-06-17-lot-summer-school-2024/</guid>
      <description>&lt;p&gt;&lt;strong&gt;I&amp;rsquo;m teaching a 5-day &lt;em&gt;Prosody in Speech Perception&lt;/em&gt; course at the LOT Summer School 2024 in Leiden. You can find all materials here!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;whats-it-about&#34;&gt;What&amp;rsquo;s it about?&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;Prosody in Speech Perception&lt;/strong&gt; course is part of the &lt;a href=&#34;https://lotschool.nl/events/lot-summer-school-2024/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LOT Summer School 2024&lt;/a&gt;. The aim of this 5-day course is to reveal the central role that prosody plays in low-level speech perception and spoken word recognition.&lt;/p&gt;
&lt;p&gt;Prosody in spoken communication generally refers to those aspects of speech that fall outside the segmental information about consonants and vowels (e.g., intonation, stress, rhythm). Still, this course will describe how suprasegmental prosody and segmental cues in speech are tightly interconnected. As such, it aims to reveal the central role that prosody plays in low-level speech perception and spoken word recognition. Each lecture targets a different processing mechanism by which prosody impacts speech perception, including general-auditory normalization, neural speech tracking, prosody-guided prediction, talker-specific learning, as well as audiovisual integration of multisensory cues to prosody. Thus, prosody - in all its different forms and appearances - is a potent factor in speech perception, determining which words and speech sounds we hear.&lt;/p&gt;
&lt;h2 id=&#34;course-materials&#34;&gt;Course materials&lt;/h2&gt;
&lt;p&gt;You can find &lt;a href=&#34;../../resources/course-materials/prosody-in-speech-perception/&#34;&gt;all course materials&lt;/a&gt; here.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>NWO PhD grant for Floris Cos!</title>
      <link>https://hrbosker.github.io/news/24-06-13-nwo-grant-floris-cos/</link>
      <pubDate>Thu, 13 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/24-06-13-nwo-grant-floris-cos/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Today we heard that Floris Cos successfully received a grant from the Dutch Research Council (NWO) to start a PhD project about how simple beat gestures influence spoken word recognition in L1 and L2.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;what-went-before&#34;&gt;What went before&amp;hellip;&lt;/h2&gt;
&lt;p&gt;Floris Cos did a research internship at the SPEAC group in 2023. For this internship, he re-tested participants who had done a manual McGurk experiment about one year earlier. When these people performed the exact same task once again, we successfully replicated the manual McGurk effect at the group level but also found a weak correlation of the by-participant effect sizes in the two identical experiments. These outcomes were presented in Leiden at Speech Prosody 2024 and published as &lt;a href=&#34;https://hrbosker.github.io/publication/cos-etal-2024-speechprosody&#34;&gt;Cos, Bujok, and Bosker (2024)&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;hooray&#34;&gt;Hooray!&lt;/h2&gt;
&lt;p&gt;After this internship, Floris applied for a &amp;lsquo;PhD in the Humanities&amp;rsquo; grant from the Dutch Research Council (NWO) to continue research into audiovisual speech perception. After a long wait, we now know he was successful and will start a PhD at the Center for Language Studies (CLS) at Radboud University in September 2024. &lt;em&gt;Congratulations, Floris!!&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;official-press-release&#34;&gt;Official press release&lt;/h2&gt;
&lt;p&gt;Check out NWO&amp;rsquo;s &lt;a href=&#34;https://www.nwo.nl/en/news/funding-for-20-new-phd-students-in-the-humanities&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;press release&lt;/a&gt; here.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Paper accepted in Language and Speech!</title>
      <link>https://hrbosker.github.io/news/24-05-13-paper-ls/</link>
      <pubDate>Mon, 13 May 2024 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/24-05-13-paper-ls/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Congratulations to Ronny on his first journal publication from his PhD project! Want to know how people use lip movements and simple hand gestures to perceive lexical stress? Read it here!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;whats-it-about&#34;&gt;What&amp;rsquo;s it about?&lt;/h2&gt;
&lt;p&gt;For Ronny&amp;rsquo;s first-year project, he video-recorded his PhD supervisor saying simple Dutch words that critically differed in which syllable carries stress: for instance &lt;em&gt;CONtent&lt;/em&gt; [noun] vs. &lt;em&gt;conTENT&lt;/em&gt; [adjective]. The talker sometimes did and sometimes didn&amp;rsquo;t produce a simple beat gesture on the word he was saying. Using audio- and video-editing techniques, Ronny created artificial videos that allowed him to play around with the auditory and visual cues to stress in all possible combinations. For instance, he created videos in which the talker&amp;rsquo;s voice was saying &lt;em&gt;CONtent&lt;/em&gt;, but the talker&amp;rsquo;s face was from a recording of &lt;em&gt;conTENT&lt;/em&gt;, while the talker&amp;rsquo;s body was from a recording where the talker produced a beat gesture on the first syllable (biasing towards &lt;em&gt;CONtent&lt;/em&gt; again). He then gave these manipulated videos to several groups of native Dutch participants and asked them what they thought the speaker said.&lt;/p&gt;
&lt;p&gt;Results showed that people primarily relied on auditory cues to stress as well as visual cues about the timing of simple hand gestures. However, despite successfully demonstrating that people could tell apart the lip movements in a &lt;em&gt;CONtent&lt;/em&gt; video from the lip movements in a &lt;em&gt;conTENT&lt;/em&gt; video (i.e., when there&amp;rsquo;s no audio), we did not find that people used those visual articulatory cues to stress in audiovisual videos (i.e., with audio).&lt;/p&gt;
&lt;h2 id=&#34;why-is-this-important&#34;&gt;Why is this important?&lt;/h2&gt;
&lt;p&gt;When it comes to perceiving vowels and consonants in speech, visual articulatory cues play an important role. Seeing a person&amp;rsquo;s face can greatly help with speech perception in noisy listening conditions and can even mislead you to hear things that weren&amp;rsquo;t said at all. Our paper, however, suggests that visual articulatory cues to stress are not heavily relied on in audiovisual speech perception. This is striking because we showed that those cues are informative and could have helped participants make their decisions. As such, these outcomes demonstrate that people weigh the multisensory cues in the audiovisual input signal differently, presumably depending on their reliability.&lt;/p&gt;
&lt;h2 id=&#34;is-that-it&#34;&gt;Is that it?&lt;/h2&gt;
&lt;p&gt;Interestingly, the timing of simple hand gestures did contribute consistently to participants&amp;rsquo; perceptual decisions: a beat gesture timed on the first syllable biased participants to hear &lt;em&gt;CONtent&lt;/em&gt;, while the same gesture but timed on the second syllable biased participants to hear &lt;em&gt;conTENT&lt;/em&gt;. This replicates earlier work from our group but importantly also extends it by using more natural gestures, more realistic presentation size, and richer stimuli (i.e., with the talker&amp;rsquo;s face visible).&lt;/p&gt;
&lt;h2 id=&#34;full-reference&#34;&gt;Full reference&lt;/h2&gt;
&lt;p&gt;The full citation, open access PDF, and all data are publicly available from the links below:&lt;/p&gt;
&lt;blockquote&gt;













&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Proceedings papers accepted for Speech Prosody 2024</title>
      <link>https://hrbosker.github.io/news/24-03-15-proceedings-papers-sp2024/</link>
      <pubDate>Fri, 15 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/24-03-15-proceedings-papers-sp2024/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Several proceedings papers from our group have been accepted for presentation at &lt;a href=&#34;https://www.universiteitleiden.nl/sp2024&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Speech Prosody 2024&lt;/a&gt;. Read them here!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;what-are-they-about&#34;&gt;What are they about?&lt;/h2&gt;
&lt;p&gt;Most of our submissions concerned the temporal synchrony between hand gestures and speech in different languages. &lt;a href=&#34;https://hrbosker.github.io/publication/rohrer-hong-etal-2024-speechprosody&#34;&gt;Rohrer, Hong, and Bosker (2024)&lt;/a&gt; is one of the first studies to test how gestures time with speech in a lexical tone language, namely Mandarin. &lt;a href=&#34;https://hrbosker.github.io/publication/rohrer-bujok-etal-2024-speechprosody&#34;&gt;Rohrer, Bujok, Van Maastricht, and Bosker (2024)&lt;/a&gt; demonstrates that seeing a gesture can change where you hear the stress in Spanish, extending the &amp;lsquo;manual McGurk effect&amp;rsquo; originally observed in Dutch to Spanish. &lt;a href=&#34;https://hrbosker.github.io/publication/cos-etal-2024-speechprosody&#34;&gt;Cos, Bujok, and Bosker (2024)&lt;/a&gt; reran an earlier manual McGurk experiment with the same participants over 1.5 years later, successfully replicating earlier work at the group level but reporting a weak correlation of the by-participant effect sizes in the two identical experiments. &lt;a href=&#34;https://hrbosker.github.io/publication/maran-bosker-2024-speechprosody&#34;&gt;Maran and Bosker (2024)&lt;/a&gt; presents a mini-test of the manual McGurk effect, allowing reliable assessment of gesture-speech integration in under 10 minutes! Finally, &lt;a href=&#34;https://hrbosker.github.io/publication/ulusahin-etal-2024-speechprosody&#34;&gt;Ulu≈üahin, Bosker, McQueen, and Meyer (2024)&lt;/a&gt; reports two experiments investigating how knowledge about a given talker&amp;rsquo;s typical pitch influences subsequent voiceless (!) fricative perception.&lt;/p&gt;
&lt;h2 id=&#34;is-that-it&#34;&gt;Is that it?&lt;/h2&gt;
&lt;p&gt;If that wasn&amp;rsquo;t enough, there&amp;rsquo;s also two submissions accepted for Speech Prosody 2024 that didn&amp;rsquo;t stem from work performed in our group, but that were co-authored by current group members. These are &lt;strong&gt;Severijnen, G√§rtner, Walther, and McQueen (2024)&lt;/strong&gt; and &lt;strong&gt;Ye and Boersma (2024)&lt;/strong&gt;. Congratulations!&lt;/p&gt;
&lt;h2 id=&#34;wanna-know-more&#34;&gt;Wanna know more?&lt;/h2&gt;
&lt;p&gt;You can find all fulltexts below, but why not come and find us at SP2024 in Leiden in July? Looking forward to seeing you there!&lt;/p&gt;
&lt;h2 id=&#34;full-references&#34;&gt;Full references&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;











  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/floris-cos/&#34;&gt;Floris Cos&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/ronny-bujok/&#34;&gt;Ronny Bujok&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2024).
  &lt;a href=&#34;https://hrbosker.github.io/publication/cos-etal-2024-speechprosody/&#34;&gt;Test-retest reliability of audiovisual lexical stress perception after &amp;gt;1.5 years&lt;/a&gt;.
  In &lt;em&gt;Proceedings of Speech Prosody 2024&lt;/em&gt;, 871-875, doi:&lt;a href=&#34;https://doi.org/10.21437/SpeechProsody.2024-176&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.21437/SpeechProsody.2024-176&lt;/a&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3580020_1/component/file_3580021/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/cos-etal-2024-speechprosody/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/nqswd/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.21437/SpeechProsody.2024-176&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;















  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/matteo-maran/&#34;&gt;Matteo Maran&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2024).
  &lt;a href=&#34;https://hrbosker.github.io/publication/maran-bosker-2024-speechprosody/&#34;&gt;How to test gesture-speech integration in ten minutes&lt;/a&gt;.
  In &lt;em&gt;Proceedings of Speech Prosody 2024&lt;/em&gt;, 737-741, doi:&lt;a href=&#34;https://doi.org/10.21437/SpeechProsody.2024-149&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.21437/SpeechProsody.2024-149&lt;/a&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3582995_1/component/file_3582996/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/maran-bosker-2024-speechprosody/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/qbyfm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.21437/SpeechProsody.2024-149&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;















  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/patrick-louis-rohrer/&#34;&gt;Patrick Louis Rohrer&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/ronny-bujok/&#34;&gt;Ronny Bujok&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/lieke-van-maastricht/&#34;&gt;Lieke van Maastricht&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2024).
  &lt;a href=&#34;https://hrbosker.github.io/publication/rohrer-bujok-etal-2024-speechprosody/&#34;&gt;The timing of beat gestures affects lexical stress perception in Spanish&lt;/a&gt;.
  In &lt;em&gt;Proceedings of Speech Prosody 2024&lt;/em&gt;, 702-706, doi:&lt;a href=&#34;https://doi.org/10.21437/SpeechProsody.2024-142&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.21437/SpeechProsody.2024-142&lt;/a&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3582989_2/component/file_3582990/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/rohrer-bujok-etal-2024-speechprosody/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/bmk2s/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.21437/SpeechProsody.2024-142&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;















  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/patrick-louis-rohrer/&#34;&gt;Patrick Louis Rohrer&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/yitian-hong/&#34;&gt;Yitian Hong&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2024).
  &lt;a href=&#34;https://hrbosker.github.io/publication/rohrer-hong-etal-2024-speechprosody/&#34;&gt;Gestures time to vowel onset and change the acoustics of the word in Mandarin&lt;/a&gt;.
  In &lt;em&gt;Proceedings of Speech Prosody 2024&lt;/em&gt;, 866-870, doi:&lt;a href=&#34;https://doi.org/10.21437/SpeechProsody.2024-175&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.21437/SpeechProsody.2024-175&lt;/a&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3582991_2/component/file_3582992/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/rohrer-hong-etal-2024-speechprosody/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/w4czh/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.21437/SpeechProsody.2024-175&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;















  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/orhun-ulusahin/&#34;&gt;Orhun Ulu≈üahin&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/james-m.-mcqueen/&#34;&gt;James M. McQueen&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/antje-s.-meyer/&#34;&gt;Antje S. Meyer&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2024).
  &lt;a href=&#34;https://hrbosker.github.io/publication/ulusahin-etal-2024-speechprosody/&#34;&gt;Knowledge of a talker‚Äôs f0 affects subsequent perception of voiceless fricatives&lt;/a&gt;.
  In &lt;em&gt;Proceedings of Speech Prosody 2024&lt;/em&gt;, 432-436, doi:&lt;a href=&#34;https://doi.org/10.21437/SpeechProsody.2024-88&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.21437/SpeechProsody.2024-88&lt;/a&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3581041_1/component/file_3581042/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/ulusahin-etal-2024-speechprosody/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/wfp9y/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.21437/SpeechProsody.2024-88&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;


&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Paper accepted in Journal of Phonetics!</title>
      <link>https://hrbosker.github.io/news/24-03-05-paper-jphon/</link>
      <pubDate>Tue, 05 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/24-03-05-paper-jphon/</guid>
      <description>&lt;p&gt;&lt;strong&gt;How do speakers of Dutch produce lexical stress? Read it in: &amp;ldquo;Your ‚ÄúVOORnaam‚Äù is not my ‚ÄúVOORnaam‚Äù: An acoustic analysis of individual talker differences in word stress in Dutch.&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;published&#34;&gt;Published!&lt;/h2&gt;
&lt;p&gt;Our paper &lt;strong&gt;&amp;ldquo;Your ‚ÄúVOORnaam‚Äù is not my ‚ÄúVOORnaam‚Äù: An acoustic analysis of individual talker differences in word stress in Dutch&amp;rdquo;&lt;/strong&gt; has recently been published in &lt;em&gt;Journal of Phonetics&lt;/em&gt;, authored by &lt;a href=&#34;https://hrbosker.github.io/author/giulio-severijnen/&#34;&gt;Giulio Severijnen&lt;/a&gt;, &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;, and James McQueen. The fulltext and all data are publicly available at the links provided at the bottom of this post.&lt;/p&gt;
&lt;h2 id=&#34;whats-it-about&#34;&gt;What&amp;rsquo;s it about?&lt;/h2&gt;
&lt;p&gt;As part of his PhD, Giulio recorded 40 speakers of Dutch producing word pairs that critically differed in stress position, like &amp;ldquo;VOORnaam&amp;rdquo; (&lt;em&gt;first name&lt;/em&gt;) - &amp;ldquo;voorNAAM&amp;rdquo; (&lt;em&gt;respectable&lt;/em&gt;). He then measured how a syllable with stress (&amp;ldquo;VOOR-&amp;rdquo;) differs acoustically from the same syllable without stress (&amp;ldquo;voor-&amp;rdquo;). The figure above (reproducing Figure 3 from the original paper) shows that people generally tend to use pitch, intensity, and duration as primary cues to stress, but this depends in part on the context in which the word is spoken.&lt;/p&gt;
&lt;p&gt;More importantly, Giulio saw that each speaker had their own pronunciation preferences. That is, each employed a unique combination of acoustic cues to stress, illustrating large prosodic variability between talkers. In fact, classes of cue-weighting tendencies emerged, differing in which cue was used as the main cue (e.g., &amp;lsquo;pitch-speakers&amp;rsquo; vs. &amp;lsquo;intensity-speakers&amp;rsquo;). This suggests that, while there is a large amount of variability between different speakers (i.e., each talker had a unique set of cue weights), talkers do seem to cluster together regarding which cue is their main cue.&lt;/p&gt;
&lt;h2 id=&#34;why-is-this-important&#34;&gt;Why is this important?&lt;/h2&gt;
&lt;p&gt;This study is the most comprehensive acoustic description, to date, of word stress in Dutch. This is valuable information for speech scientists but also informative for speech synthesizers and automatic speech recognition (ASR) systems. Moreover, we describe large prosodic variability between individual talkers, but at the same time this variability isn&amp;rsquo;t boundless. Listeners, as well as ASR systems, may use this talker-specific information (e.g., &amp;ldquo;Johnny is a pitch-speaker&amp;rdquo;) when trying to comprehend new speech from the same talker, especially in challenging listening conditions such as in noise.&lt;/p&gt;
&lt;h2 id=&#34;full-reference&#34;&gt;Full reference&lt;/h2&gt;
&lt;blockquote&gt;












  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/giulio-severijnen/&#34;&gt;Giulio Severijnen&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/james-m.-mcqueen/&#34;&gt;James M. McQueen&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2024).
  &lt;a href=&#34;https://hrbosker.github.io/publication/severijnen-etal-2024-jphon/&#34;&gt;Your ‚ÄúVOORnaam‚Äù is not my ‚ÄúVOORnaam‚Äù: An acoustic analysis of individual talker differences in word stress in Dutch&lt;/a&gt;.
  &lt;em&gt;Journal of Phonetics, 103&lt;/em&gt;, 101296, doi:10.1016/j.wocn.2024.101296.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/pubman/item/item_3562664_1/component/file_3562665/2024-01-15_Full%20Manuscript_Production_accepted.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/severijnen-etal-2024-jphon/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://data.donders.ru.nl/login/reviewer-241214775/bbEj3LjTV3m6x6u0JQKatK2RGlMNST-jdqVIzFfHMLc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1016/j.wocn.2024.101296&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>A new PhD student joins the group: Chengjia Ye</title>
      <link>https://hrbosker.github.io/news/23-09-04-new-phd-chengjia-ye/</link>
      <pubDate>Mon, 04 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/23-09-04-new-phd-chengjia-ye/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Chengjia joined the group in Sept 2023 to test how the timing of hand gestures influences speech perception in more naturalistic listening conditions.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;when-listening-is-tough&#34;&gt;When listening is tough&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://hrbosker.github.io/author/chengjia-ye/&#34;&gt;Chengjia Ye&lt;/a&gt; will run Work Package 3 of the HearingHands ERC Starting Grant, together with &lt;a href=&#34;https://www.ru.nl/english/people/mcqueen-j/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prof. James McQueen&lt;/a&gt; (promotor) and &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Dr. Hans Rutger Bosker&lt;/a&gt;. This WP3 tests how beat gestures influence speech perception in more naturalistic listening conditions. We will assess whether the communicative relevance of a gesture influences the manual McGurk effect by making use of talking avatars, how the integration of gestural timing with spoken prosody varies depending on the saliency of the visual/auditory cues, and whether listeners adapt to variability in gesture-speech alignment.&lt;/p&gt;
&lt;p&gt;Great to have you on the team, Chengjia!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Another postdoc joins the group: Matteo Maran</title>
      <link>https://hrbosker.github.io/news/23-06-12-new-postdoc-matteo-maran/</link>
      <pubDate>Mon, 12 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/23-06-12-new-postdoc-matteo-maran/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Matteo joined the group in June 2023 to test the neural correlates of audiovisual integration of gestural timing and spoken prosody, as well as their alteration in Autism Spectrum Disorder (ASD)&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;audiovisual-integration&#34;&gt;Audiovisual integration&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://hrbosker.github.io/author/matteo-maran/&#34;&gt;Matteo Maran&lt;/a&gt; will be in charge of Work Package 2 of the HearingHands ERC Starting Grant. This WP2 tests the audiovisual integration of gestural timing with spoken prosody using electrophysiological methods. A key aim of WP2 is to assess how this integration may be altered in individuals with Autism Spectrum Disorder (ASD).&lt;/p&gt;
&lt;p&gt;Happy to have you on board, Matteo!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>New postdoc joins the group: Patrick Louis Rohrer</title>
      <link>https://hrbosker.github.io/news/23-05-16-new-postdoc-patrick-louis-rohrer/</link>
      <pubDate>Tue, 16 May 2023 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/23-05-16-new-postdoc-patrick-louis-rohrer/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Patrick joined the group in May 2023 to work on a cross-linguistic comparison of gesture-speech temporal alignment in production and perception.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;timing-gestures-and-prosody&#34;&gt;Timing gestures and prosody&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://hrbosker.github.io/author/patrick-louis-rohrer/&#34;&gt;Patrick Louis Rohrer&lt;/a&gt; will be in charge of Work Package 1 of the HearingHands ERC Starting Grant. This WP1 compares the temporal alignment of gestures and speech in languages with different prosodic regimes, including free-stress, fixed-stress, and lexical-tone languages. Next to this production strand, a perception strand investigates how the timing of seemingly meaningless gestures contributes to low-level speech perception in these same languages.&lt;/p&gt;
&lt;p&gt;Glad to have you on the team, Patrick!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Two proceedings papers accepted for ICPhS 2023</title>
      <link>https://hrbosker.github.io/news/23-04-03-proceedings-papers-icphs/</link>
      <pubDate>Mon, 03 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/23-04-03-proceedings-papers-icphs/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Two proceedings papers from our group have been accepted for presentation at ICPhS 2023. Read them here!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;rate-normalization&#34;&gt;Rate normalization&lt;/h2&gt;
&lt;p&gt;In Severijnen et al. (2023), &lt;a href=&#34;https://hrbosker.github.io/author/giulio-severijnen/&#34;&gt;Giulio Severijnen&lt;/a&gt; tested which acoustic and linguistic cues underlie rate normalization in speech perception. He observed that the tempo of Dutch context phrases can change people&amp;rsquo;s perception of vowel length in a following word: a preceding context with twice as many syllables per unit time biases people to report hearing long /a:/, a slower context towards short /…ë/. However, this relationship between contextual speech rate and vowel length perception is not linear. That is, contexts with three times as many syllables do not lead to even more long /a:/ responses. Therefore, syllable rate is not the only determining factor in rate normalization.&lt;/p&gt;
&lt;h2 id=&#34;converging-to-f2&#34;&gt;Converging to F2&lt;/h2&gt;
&lt;p&gt;When having a conversation, interlocutors tend to sound more like each other over the course of the conversation. In Ulusahin et al. (2023), &lt;a href=&#34;https://hrbosker.github.io/author/orhun-ulusahin/&#34;&gt;Orhun Ulu≈üahin&lt;/a&gt; set out to test the automaticity and grain-size of this phonetic convergence. He presented participants with single words that - unaware to the participants - had the second formant frequency (F2) shifted way down. When participants repeated these words back, we unfortunately did not find any downward shift in participants&amp;rsquo; own F2. As such, these results question theories which view convergence as a product of automatic integration between perception and production.&lt;/p&gt;
&lt;p&gt;Come and find these two posters this summer at ICPhS 2023!&lt;/p&gt;
&lt;h2 id=&#34;full-reference&#34;&gt;Full reference&lt;/h2&gt;
&lt;blockquote&gt;












  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/giulio-severijnen/&#34;&gt;Giulio Severijnen&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/james-m.-mcqueen/&#34;&gt;James M. McQueen&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2023).
  &lt;a href=&#34;https://hrbosker.github.io/publication/severijnen-etal-2023-icphs/&#34;&gt;Syllable rate drives rate normalization, but is not the only factor&lt;/a&gt;.
  In &lt;em&gt;Proceedings of ICPhS 2023&lt;/em&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3505424_4/component/file_3505425/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/severijnen-etal-2023-icphs/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/36anr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;












&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;
&lt;blockquote&gt;












  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/orhun-ulusahin/&#34;&gt;Orhun Ulu≈üahin&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/james-m.-mcqueen/&#34;&gt;James M. McQueen&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/antje-s.-meyer/&#34;&gt;Antje S. Meyer&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2023).
  &lt;a href=&#34;https://hrbosker.github.io/publication/ulusahin-etal-2023-icphs/&#34;&gt;No evidence for convergence to sub-phonemic F2 shifts in shadowing&lt;/a&gt;.
  In &lt;em&gt;Proceedings of ICPhS 2023&lt;/em&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3507211_3/component/file_3507212/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/ulusahin-etal-2023-icphs/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/wb2mn&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;












&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Ivy graduates!</title>
      <link>https://hrbosker.github.io/news/16-03-22-ivy-graduation/</link>
      <pubDate>Thu, 16 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/16-03-22-ivy-graduation/</guid>
      <description>&lt;p&gt;&lt;strong&gt;On March 16, 2023, Ivy Mok successfully graduated from the Linguistics and Communication Sciences ResMA program at Radboud University.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;congratulations&#34;&gt;Congratulations!&lt;/h2&gt;
&lt;p&gt;Ivy wrote &lt;a href=&#34;https://theses.ubn.ru.nl/handle/123456789/14258&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;her MA thesis&lt;/a&gt; on lexical stress perception in noise. She tested whether listeners are flexible in how they weigh auditory and visual articulatory cues to lexical stress when listening is hard. Her thesis results showed convincingly that people weigh the visual cues to lexical stress more heavily when the speech is increasingly masked by loud background babble.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;re happy to see Ivy graduate but we&amp;rsquo;re also sad to see her leave the group. We&amp;rsquo;ll certainly miss the maple syrup biscuits and soesjes at lab meetings. Still, she won&amp;rsquo;t stray far: she&amp;rsquo;s taken up a position as project coordinator at the Multimodal Language Department of the Max Planck Institute for Psycholinguistics.&lt;/p&gt;
&lt;p&gt;Congratulations, Ivy!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>New lab members: Yitian Hong and Floris Cos</title>
      <link>https://hrbosker.github.io/news/23-02-22-new-members/</link>
      <pubDate>Thu, 23 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/23-02-22-new-members/</guid>
      <description>&lt;p&gt;&lt;strong&gt;In March 2023, we will have two new members join the group: Yitian Hong, a visiting researcher from PolyU Hong Kong; and Floris Cos, an intern from the Linguistics and Communication Sciences ResMA program at Radboud University.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;lexical-tone&#34;&gt;Lexical tone&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://hrbosker.github.io/author/yitian-hong/&#34;&gt;Yitian Hong&lt;/a&gt; will be working on a project investigating the audiovisual perception of lexical tone in Mandarin Chinese. This project will run from March - August 2023 and is in collaboration with Patrick Rohrer and &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;test-retest-reliability&#34;&gt;Test-retest reliability&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://hrbosker.github.io/author/floris-cos/&#34;&gt;Floris Cos&lt;/a&gt; will run an internship for his Linguistics and Communication Sciences ResMA program. He will assess the test-retest reliability of the Manual McGurk effect using online experimentation.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;re glad to have them join the group. Welcome!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Come join us at the &#39;Dag van de Fonetiek&#39;</title>
      <link>https://hrbosker.github.io/news/22-12-15-dagvdfonetiek2022/</link>
      <pubDate>Thu, 15 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/22-12-15-dagvdfonetiek2022/</guid>
      <description>&lt;p&gt;&lt;strong&gt;This &amp;lsquo;Phonetics Day&amp;rsquo; is the annual meeting of the &amp;lsquo;Dutch Society for Phonetic Sciences&amp;rsquo; [NVFW], taking place in Utrecht on December 16, 2022.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;open-to-all&#34;&gt;Open to all!&lt;/h2&gt;
&lt;p&gt;The &amp;lsquo;Dag van de Fonetiek&amp;rsquo; 2022 will take place on &lt;strong&gt;Friday Dec 16, 2022&lt;/strong&gt;, in the Sweelinckzaal at Drift 21, Utrecht, The Netherlands. It is an event celebrating everything &amp;lsquo;speechy&amp;rsquo; and is free and open to all: members, non-members, scientists, students, anyone! This year, you can hear &lt;a href=&#34;https://hrbosker.github.io/author/ronny-bujok/&#34;&gt;Ronny Bujok&lt;/a&gt; talk about whether beat gestures recalibrate lexical stress perception, and &lt;a href=&#34;https://hrbosker.github.io/author/orhun-ulusahin/&#34;&gt;Orhun Ulu≈üahin&lt;/a&gt; has some intriguing results about how listeners track a talker&amp;rsquo;s pitch!&lt;/p&gt;
&lt;p&gt;See &lt;a href=&#34;https://www.nvfw.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.nvfw.org/&lt;/a&gt; for the full program. We hope to see you there!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Paper accepted in JEP:HPP!</title>
      <link>https://hrbosker.github.io/news/22-12-12-paper-jephpp/</link>
      <pubDate>Mon, 12 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/22-12-12-paper-jephpp/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Congratulazioni a Giulio e Giuseppe for successfully publishing their collaborative project &amp;ldquo;Tracking talker-specific cues to lexical stress: Evidence from perceptual learning&amp;rdquo;!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;accepted&#34;&gt;Accepted!&lt;/h2&gt;
&lt;p&gt;Today we heard that the paper &amp;ldquo;Tracking talker-specific cues to lexical stress: Evidence from perceptual learning&amp;rdquo; has been accepted for publication in the &lt;em&gt;Journal of Experimental Psychology: Human Perception and Performance (JEP:HPP)&lt;/em&gt;, authored by &lt;a href=&#34;https://hrbosker.github.io/author/giulio-severijnen/&#34;&gt;Giulio Severijnen&lt;/a&gt;, Giuseppe Di Dona, &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;, and James McQueen. The fulltext and all data are publicly available at the links provided at the bottom of this post.&lt;/p&gt;
&lt;h2 id=&#34;whats-it-about&#34;&gt;What&amp;rsquo;s it about?&lt;/h2&gt;
&lt;p&gt;The joint first-authors Giulio and Giuseppe set out to test whether listeners track how different talkers cue lexical stress. They first exposed two groups of listeners to two talkers: Talker A and B. These two talkers consistently used different cues to signal lexical stress in Dutch (e.g., differentiating &lt;em&gt;PLAto&lt;/em&gt; from &lt;em&gt;plaTEAU&lt;/em&gt;). Group 1 always heard Talker A use F0 to cue stressed syllables in Dutch, while Talker B always used intensity. Conversely, Group 2 heard the reverse talker-cue mappings: Talker A always used intensity, and Talker B always F0.&lt;/p&gt;
&lt;p&gt;After this (admittedly strange) exposure phase, participants were given an (admittedly even stranger) test phase. They were presented with audio recordings from the two talkers but this time the F0 and intensity cues had been artificially manipulated to &amp;lsquo;point in different directions&amp;rsquo;. For instance, while F0 would clearly cue stress on the first syllable of the word, intensity cues would signal stress on the second syllable. Critically, these &amp;lsquo;mixed items&amp;rsquo; were perceived by listeners according to the talker-cue mappings they had learnt during exposure. That is, Group 1 had learnt that Talker A always used F0 in the exposure phase and therefore, at test, when they heard Talker A produce a mixed item, they were more likely to perceive stress on the syllable marked by F0. However, Group 2 was more likely to perceive the exact same mixed item as having stress on the syllable marked by intensity.&lt;/p&gt;
&lt;h2 id=&#34;why-is-this-important&#34;&gt;Why is this important?&lt;/h2&gt;
&lt;p&gt;These findings support Bayesian models of spoken word recognition. These predict that listeners can adjust their prior beliefs about the perceptual weight of different phonetic cues on the basis of short-term regularities in a talker-specific fashion. This had already been observed for segmental contrasts (e.g., the perception of different consonants and vowels). Now we demonstrate that people also track suprasegmental variability in prosody, such as lexical stress.&lt;/p&gt;
&lt;h2 id=&#34;full-reference&#34;&gt;Full reference&lt;/h2&gt;
&lt;blockquote&gt;












  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/giulio-severijnen/&#34;&gt;Giulio Severijnen&lt;/a&gt;&lt;/span&gt;&lt;i class=&#34;author-notes fas fa-info-circle&#34; data-toggle=&#34;tooltip&#34; title=&#34;Equal contribution&#34;&gt;&lt;/i&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/giuseppe-di-dona/&#34;&gt;Giuseppe Di Dona&lt;/a&gt;&lt;/span&gt;&lt;i class=&#34;author-notes fas fa-info-circle&#34; data-toggle=&#34;tooltip&#34; title=&#34;Equal contribution&#34;&gt;&lt;/i&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/james-m.-mcqueen/&#34;&gt;James M. McQueen&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2023).
  &lt;a href=&#34;https://hrbosker.github.io/publication/severijnen-etal-2023-jephpp/&#34;&gt; Tracking talker-specific cues to lexical stress: Evidence from perceptual learning&lt;/a&gt;.
  &lt;em&gt;Journal of Experimental Psychology: Human Perception and Performance, 49&lt;/em&gt;(4), 549-565. doi:10.1037/xhp0001105.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3479620_2/component/file_3484213/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/severijnen-etal-2023-jephpp/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/dczx9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1037/xhp0001105&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>GESPIN 2023 in Nijmegen</title>
      <link>https://hrbosker.github.io/news/22-11-01-gespin2023/</link>
      <pubDate>Tue, 01 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/22-11-01-gespin2023/</guid>
      <description>&lt;p&gt;&lt;strong&gt;The Gesture and Speech in Interaction [GESPIN] conference is coming to Nijmegen on September 13-15, 2023.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;broadening-perspectives-integrating-views&#34;&gt;Broadening perspectives, integrating views&lt;/h2&gt;
&lt;p&gt;&amp;hellip;that is the theme of the 2023 edition. This promises a highly interdisciplinary event, approaching the interaction of gesture and speech from the perspectives of language development, neurobiology, biomechanics, animal models, and many other fields. Keynotes are: Nuria Esteve Gibert, Yifei He, Susanne Fuchs, and Franz Goller. It is co-organized by CLS, the Donders Institute, and MPI. Paper submission opens January 10th, 2023 and the deadline is &lt;strong&gt;March 15th, 2023&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;See &lt;a href=&#34;https://gespin2023.nl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gespin2023.nl&lt;/a&gt; for all the details. We hope to see you there!&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
