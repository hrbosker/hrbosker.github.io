<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Latest News | SPEAC | Hans Rutger Bosker</title>
    <link>https://hrbosker.github.io/news/</link>
      <atom:link href="https://hrbosker.github.io/news/index.xml" rel="self" type="application/rss+xml" />
    <description>Latest News</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Fri, 15 Mar 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://hrbosker.github.io/media/logo_hu4ee931c5040e3d0fbcac8149a377b250_283935_300x300_fit_lanczos_3.png</url>
      <title>Latest News</title>
      <link>https://hrbosker.github.io/news/</link>
    </image>
    
    <item>
      <title>Proceedings papers accepted for Speech Prosody 2024</title>
      <link>https://hrbosker.github.io/news/24-03-15-proceedings-papers-sp2024/</link>
      <pubDate>Fri, 15 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/24-03-15-proceedings-papers-sp2024/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Several proceedings papers from our group have been accepted for presentation at &lt;a href=&#34;https://www.universiteitleiden.nl/sp2024&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Speech Prosody 2024&lt;/a&gt;. Read them here!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;what-are-they-about&#34;&gt;What are they about?&lt;/h2&gt;
&lt;p&gt;Most of our submissions concerned the temporal synchrony between hand gestures and speech in different languages. &lt;a href=&#34;https://hrbosker.github.io/publication/rohrer-hong-etal-2024-speechprosody&#34;&gt;Rohrer, Hong, and Bosker (2024)&lt;/a&gt; is one of the first studies to test how gestures time with speech in a lexical tone language, namely Mandarin. &lt;a href=&#34;https://hrbosker.github.io/publication/rohrer-bujok-etal-2024-speechprosody&#34;&gt;Rohrer, Bujok, Van Maastricht, and Bosker (2024)&lt;/a&gt; demonstrates that seeing a gesture can change where you hear the stress in Spanish, extending the &amp;lsquo;manual McGurk effect&amp;rsquo; originally observed in Dutch to Spanish. &lt;a href=&#34;https://hrbosker.github.io/publication/cos-etal-2024-speechprosody&#34;&gt;Cos, Bujok, and Bosker (2024)&lt;/a&gt; reran an earlier manual McGurk experiment with the same participants over 1.5 years later, successfully replicating earlier work at the group level but reporting a weak correlation of the by-participant effect sizes in the two identical experiments. &lt;a href=&#34;https://hrbosker.github.io/publication/maran-bosker-2024-speechprosody&#34;&gt;Maran and Bosker (2024)&lt;/a&gt; presents a mini-test of the manual McGurk effect, allowing reliable assessment of gesture-speech integration in under 10 minutes! Finally, &lt;a href=&#34;https://hrbosker.github.io/publication/ulusahin-etal-2024-speechprosody&#34;&gt;Uluşahin, Bosker, McQueen, and Meyer (2024)&lt;/a&gt; reports two experiments investigating how knowledge about a given talker&amp;rsquo;s typical pitch influences subsequent voiceless (!) fricative perception.&lt;/p&gt;
&lt;h2 id=&#34;is-that-it&#34;&gt;Is that it?&lt;/h2&gt;
&lt;p&gt;If that wasn&amp;rsquo;t enough, there&amp;rsquo;s also two submissions accepted for Speech Prosody 2024 that didn&amp;rsquo;t stem from work performed in our group, but that were co-authored by current group members. These are &lt;strong&gt;Severijnen, Gärtner, Walther, and McQueen (2024)&lt;/strong&gt; and &lt;strong&gt;Ye and Boersma (2024)&lt;/strong&gt;. Congratulations!&lt;/p&gt;
&lt;h2 id=&#34;wanna-know-more&#34;&gt;Wanna know more?&lt;/h2&gt;
&lt;p&gt;You can find all fulltexts below, but why not come and find us at SP2024 in Leiden in July? Looking forward to seeing you there!&lt;/p&gt;
&lt;h2 id=&#34;full-references&#34;&gt;Full references&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;











  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/floris-cos/&#34;&gt;Floris Cos&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/ronny-bujok/&#34;&gt;Ronny Bujok&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2024).
  &lt;a href=&#34;https://hrbosker.github.io/publication/cos-etal-2024-speechprosody/&#34;&gt;Test-retest reliability of audiovisual lexical stress perception after &amp;gt;1.5 years&lt;/a&gt;.
  In &lt;em&gt;Proceedings of Speech Prosody 2024&lt;/em&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3580020_1/component/file_3580021/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/cos-etal-2024-speechprosody/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/nqswd/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;












&lt;/p&gt;

  
  
&lt;/div&gt;















  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/matteo-maran/&#34;&gt;Matteo Maran&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2024).
  &lt;a href=&#34;https://hrbosker.github.io/publication/maran-bosker-2024-speechprosody/&#34;&gt;How to test gesture-speech integration in ten minutes&lt;/a&gt;.
  In &lt;em&gt;Proceedings of Speech Prosody 2024&lt;/em&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3582995_1/component/file_3582996/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/maran-bosker-2024-speechprosody/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/qbyfm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;












&lt;/p&gt;

  
  
&lt;/div&gt;















  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/patrick-louis-rohrer/&#34;&gt;Patrick Louis Rohrer&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/ronny-bujok/&#34;&gt;Ronny Bujok&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/lieke-van-maastricht/&#34;&gt;Lieke Van Maastricht&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2024).
  &lt;a href=&#34;https://hrbosker.github.io/publication/rohrer-bujok-etal-2024-speechprosody/&#34;&gt;The timing of beat gestures affects lexical stress perception in Spanish&lt;/a&gt;.
  In &lt;em&gt;Proceedings of Speech Prosody 2024&lt;/em&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3582989_2/component/file_3582990/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/rohrer-bujok-etal-2024-speechprosody/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/bmk2s/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;












&lt;/p&gt;

  
  
&lt;/div&gt;















  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/patrick-louis-rohrer/&#34;&gt;Patrick Louis Rohrer&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/yitian-hong/&#34;&gt;Yitian Hong&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2024).
  &lt;a href=&#34;https://hrbosker.github.io/publication/rohrer-hong-etal-2024-speechprosody/&#34;&gt;Gestures time to vowel onset and change the acoustics of the word in Mandarin&lt;/a&gt;.
  In &lt;em&gt;Proceedings of Speech Prosody 2024&lt;/em&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3582991_2/component/file_3582992/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/rohrer-hong-etal-2024-speechprosody/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/w4czh/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;












&lt;/p&gt;

  
  
&lt;/div&gt;















  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/orhun-ulusahin/&#34;&gt;Orhun Uluşahin&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/james-m.-mcqueen/&#34;&gt;James M. McQueen&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/antje-s.-meyer/&#34;&gt;Antje S. Meyer&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2024).
  &lt;a href=&#34;https://hrbosker.github.io/publication/ulusahin-etal-2024-speechprosody/&#34;&gt;Knowledge of a talker’s f0 affects subsequent perception of voiceless fricatives&lt;/a&gt;.
  In &lt;em&gt;Proceedings of Speech Prosody 2024&lt;/em&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3581041_1/component/file_3581042/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/ulusahin-etal-2024-speechprosody/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/wfp9y/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;












&lt;/p&gt;

  
  
&lt;/div&gt;


&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Paper accepted in Journal of Phonetics!</title>
      <link>https://hrbosker.github.io/news/24-03-05-paper-jphon/</link>
      <pubDate>Tue, 05 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/24-03-05-paper-jphon/</guid>
      <description>&lt;p&gt;&lt;strong&gt;How do speakers of Dutch produce lexical stress? Read it in: &amp;ldquo;Your “VOORnaam” is not my “VOORnaam”: An acoustic analysis of individual talker differences in word stress in Dutch.&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;published&#34;&gt;Published!&lt;/h2&gt;
&lt;p&gt;Our paper &lt;strong&gt;&amp;ldquo;Your “VOORnaam” is not my “VOORnaam”: An acoustic analysis of individual talker differences in word stress in Dutch&amp;rdquo;&lt;/strong&gt; has recently been published in &lt;em&gt;Journal of Phonetics&lt;/em&gt;, authored by &lt;a href=&#34;https://hrbosker.github.io/author/giulio-severijnen/&#34;&gt;Giulio Severijnen&lt;/a&gt;, &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;, and James McQueen. The fulltext and all data are publicly available at the links provided at the bottom of this post.&lt;/p&gt;
&lt;h2 id=&#34;whats-it-about&#34;&gt;What&amp;rsquo;s it about?&lt;/h2&gt;
&lt;p&gt;As part of his PhD, Giulio recorded 40 speakers of Dutch producing word pairs that critically differed in stress position, like &amp;ldquo;VOORnaam&amp;rdquo; (&lt;em&gt;first name&lt;/em&gt;) - &amp;ldquo;voorNAAM&amp;rdquo; (&lt;em&gt;respectable&lt;/em&gt;). He then measured how a syllable with stress (&amp;ldquo;VOOR-&amp;rdquo;) differs acoustically from the same syllable without stress (&amp;ldquo;voor-&amp;rdquo;). The figure above (reproducing Figure 3 from the original paper) shows that people generally tend to use pitch, intensity, and duration as primary cues to stress, but this depends in part on the context in which the word is spoken.&lt;/p&gt;
&lt;p&gt;More importantly, Giulio saw that each speaker had their own pronunciation preferences. That is, each employed a unique combination of acoustic cues to stress, illustrating large prosodic variability between talkers. In fact, classes of cue-weighting tendencies emerged, differing in which cue was used as the main cue (e.g., &amp;lsquo;pitch-speakers&amp;rsquo; vs. &amp;lsquo;intensity-speakers&amp;rsquo;). This suggests that, while there is a large amount of variability between different speakers (i.e., each talker had a unique set of cue weights), talkers do seem to cluster together regarding which cue is their main cue.&lt;/p&gt;
&lt;h2 id=&#34;why-is-this-important&#34;&gt;Why is this important?&lt;/h2&gt;
&lt;p&gt;This study is the most comprehensive acoustic description, to date, of word stress in Dutch. This is valuable information for speech scientists but also informative for speech synthesizers and automatic speech recognition (ASR) systems. Moreover, we describe large prosodic variability between individual talkers, but at the same time this variability isn&amp;rsquo;t boundless. Listeners, as well as ASR systems, may use this talker-specific information (e.g., &amp;ldquo;Johnny is a pitch-speaker&amp;rdquo;) when trying to comprehend new speech from the same talker, especially in challenging listening conditions such as in noise.&lt;/p&gt;
&lt;h2 id=&#34;full-reference&#34;&gt;Full reference&lt;/h2&gt;
&lt;blockquote&gt;












  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/giulio-severijnen/&#34;&gt;Giulio Severijnen&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/james-m.-mcqueen/&#34;&gt;James M. McQueen&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2024).
  &lt;a href=&#34;https://hrbosker.github.io/publication/severijnen-etal-2024-jphon/&#34;&gt;Your “VOORnaam” is not my “VOORnaam”: An acoustic analysis of individual talker differences in word stress in Dutch&lt;/a&gt;.
  &lt;em&gt;Journal of Phonetics, 103&lt;/em&gt;, 101296, doi:10.1016/j.wocn.2024.101296.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/pubman/item/item_3562664_1/component/file_3562665/2024-01-15_Full%20Manuscript_Production_accepted.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/severijnen-etal-2024-jphon/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://data.donders.ru.nl/login/reviewer-241214775/bbEj3LjTV3m6x6u0JQKatK2RGlMNST-jdqVIzFfHMLc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1016/j.wocn.2024.101296&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>A new PhD student joins the group: Chengjia Ye</title>
      <link>https://hrbosker.github.io/news/23-09-04-new-phd-chengjia-ye/</link>
      <pubDate>Mon, 04 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/23-09-04-new-phd-chengjia-ye/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Chengjia joined the group in Sept 2023 to test how the timing of hand gestures influences speech perception in more naturalistic listening conditions.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;when-listening-is-tough&#34;&gt;When listening is tough&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://hrbosker.github.io/author/chengjia-ye/&#34;&gt;Chengjia Ye&lt;/a&gt; will run Work Package 3 of the HearingHands ERC Starting Grant, together with &lt;a href=&#34;https://www.ru.nl/english/people/mcqueen-j/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prof. James McQueen&lt;/a&gt; (promotor) and &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Dr. Hans Rutger Bosker&lt;/a&gt;. This WP3 tests how beat gestures influence speech perception in more naturalistic listening conditions. We will assess whether the communicative relevance of a gesture influences the manual McGurk effect by making use of talking avatars, how the integration of gestural timing with spoken prosody varies depending on the saliency of the visual/auditory cues, and whether listeners adapt to variability in gesture-speech alignment.&lt;/p&gt;
&lt;p&gt;Great to have you on the team, Chengjia!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Another postdoc joins the group: Matteo Maran</title>
      <link>https://hrbosker.github.io/news/23-06-12-new-postdoc-matteo-maran/</link>
      <pubDate>Mon, 12 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/23-06-12-new-postdoc-matteo-maran/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Matteo joined the group in June 2023 to test the neural correlates of audiovisual integration of gestural timing and spoken prosody, as well as their alteration in Autism Spectrum Disorder (ASD)&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;audiovisual-integration&#34;&gt;Audiovisual integration&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://hrbosker.github.io/author/matteo-maran/&#34;&gt;Matteo Maran&lt;/a&gt; will be in charge of Work Package 2 of the HearingHands ERC Starting Grant. This WP2 tests the audiovisual integration of gestural timing with spoken prosody using electrophysiological methods. A key aim of WP2 is to assess how this integration may be altered in individuals with Autism Spectrum Disorder (ASD).&lt;/p&gt;
&lt;p&gt;Happy to have you on board, Matteo!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>New postdoc joins the group: Patrick Louis Rohrer</title>
      <link>https://hrbosker.github.io/news/23-05-16-new-postdoc-patrick-louis-rohrer/</link>
      <pubDate>Tue, 16 May 2023 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/23-05-16-new-postdoc-patrick-louis-rohrer/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Patrick joined the group in May 2023 to work on a cross-linguistic comparison of gesture-speech temporal alignment in production and perception.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;timing-gestures-and-prosody&#34;&gt;Timing gestures and prosody&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://hrbosker.github.io/author/patrick-louis-rohrer/&#34;&gt;Patrick Louis Rohrer&lt;/a&gt; will be in charge of Work Package 1 of the HearingHands ERC Starting Grant. This WP1 compares the temporal alignment of gestures and speech in languages with different prosodic regimes, including free-stress, fixed-stress, and lexical-tone languages. Next to this production strand, a perception strand investigates how the timing of seemingly meaningless gestures contributes to low-level speech perception in these same languages.&lt;/p&gt;
&lt;p&gt;Glad to have you on the team, Patrick!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Two proceedings papers accepted for ICPhS 2023</title>
      <link>https://hrbosker.github.io/news/23-04-03-proceedings-papers-icphs/</link>
      <pubDate>Mon, 03 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/23-04-03-proceedings-papers-icphs/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Two proceedings papers from our group have been accepted for presentation at ICPhS 2023. Read them here!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;rate-normalization&#34;&gt;Rate normalization&lt;/h2&gt;
&lt;p&gt;In Severijnen et al. (2023), &lt;a href=&#34;https://hrbosker.github.io/author/giulio-severijnen/&#34;&gt;Giulio Severijnen&lt;/a&gt; tested which acoustic and linguistic cues underlie rate normalization in speech perception. He observed that the tempo of Dutch context phrases can change people&amp;rsquo;s perception of vowel length in a following word: a preceding context with twice as many syllables per unit time biases people to report hearing long /a:/, a slower context towards short /ɑ/. However, this relationship between contextual speech rate and vowel length perception is not linear. That is, contexts with three times as many syllables do not lead to even more long /a:/ responses. Therefore, syllable rate is not the only determining factor in rate normalization.&lt;/p&gt;
&lt;h2 id=&#34;converging-to-f2&#34;&gt;Converging to F2&lt;/h2&gt;
&lt;p&gt;When having a conversation, interlocutors tend to sound more like each other over the course of the conversation. In Ulusahin et al. (2023), &lt;a href=&#34;https://hrbosker.github.io/author/orhun-ulusahin/&#34;&gt;Orhun Uluşahin&lt;/a&gt; set out to test the automaticity and grain-size of this phonetic convergence. He presented participants with single words that - unaware to the participants - had the second formant frequency (F2) shifted way down. When participants repeated these words back, we unfortunately did not find any downward shift in participants&amp;rsquo; own F2. As such, these results question theories which view convergence as a product of automatic integration between perception and production.&lt;/p&gt;
&lt;p&gt;Come and find these two posters this summer at ICPhS 2023!&lt;/p&gt;
&lt;h2 id=&#34;full-reference&#34;&gt;Full reference&lt;/h2&gt;
&lt;blockquote&gt;












  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/giulio-severijnen/&#34;&gt;Giulio Severijnen&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/james-m.-mcqueen/&#34;&gt;James M. McQueen&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2023).
  &lt;a href=&#34;https://hrbosker.github.io/publication/severijnen-etal-2023-icphs/&#34;&gt;Syllable rate drives rate normalization, but is not the only factor&lt;/a&gt;.
  In &lt;em&gt;Proceedings of ICPhS 2023&lt;/em&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3505424_4/component/file_3505425/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/severijnen-etal-2023-icphs/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/36anr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;












&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;
&lt;blockquote&gt;












  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/orhun-ulusahin/&#34;&gt;Orhun Uluşahin&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/james-m.-mcqueen/&#34;&gt;James M. McQueen&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/antje-s.-meyer/&#34;&gt;Antje S. Meyer&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2023).
  &lt;a href=&#34;https://hrbosker.github.io/publication/ulusahin-etal-2023-icphs/&#34;&gt;No evidence for convergence to sub-phonemic F2 shifts in shadowing&lt;/a&gt;.
  In &lt;em&gt;Proceedings of ICPhS 2023&lt;/em&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3507211_3/component/file_3507212/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/ulusahin-etal-2023-icphs/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/wb2mn&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;












&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Ivy graduates!</title>
      <link>https://hrbosker.github.io/news/16-03-22-ivy-graduation/</link>
      <pubDate>Thu, 16 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/16-03-22-ivy-graduation/</guid>
      <description>&lt;p&gt;&lt;strong&gt;On March 16, 2023, Ivy Mok successfully graduated from the Linguistics and Communication Sciences ResMA program at Radboud University.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;congratulations&#34;&gt;Congratulations!&lt;/h2&gt;
&lt;p&gt;Ivy wrote &lt;a href=&#34;https://theses.ubn.ru.nl/handle/123456789/14258&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;her MA thesis&lt;/a&gt; on lexical stress perception in noise. She tested whether listeners are flexible in how they weigh auditory and visual articulatory cues to lexical stress when listening is hard. Her thesis results showed convincingly that people weigh the visual cues to lexical stress more heavily when the speech is increasingly masked by loud background babble.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;re happy to see Ivy graduate but we&amp;rsquo;re also sad to see her leave the group. We&amp;rsquo;ll certainly miss the maple syrup biscuits and soesjes at lab meetings. Still, she won&amp;rsquo;t stray far: she&amp;rsquo;s taken up a position as project coordinator at the Multimodal Language Department of the Max Planck Institute for Psycholinguistics.&lt;/p&gt;
&lt;p&gt;Congratulations, Ivy!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>New lab members: Yitian Hong and Floris Cos</title>
      <link>https://hrbosker.github.io/news/23-02-22-new-members/</link>
      <pubDate>Thu, 23 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/23-02-22-new-members/</guid>
      <description>&lt;p&gt;&lt;strong&gt;In March 2023, we will have two new members join the group: Yitian Hong, a visiting researcher from PolyU Hong Kong; and Floris Cos, an intern from the Linguistics and Communication Sciences ResMA program at Radboud University.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;lexical-tone&#34;&gt;Lexical tone&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://hrbosker.github.io/author/yitian-hong/&#34;&gt;Yitian Hong&lt;/a&gt; will be working on a project investigating the audiovisual perception of lexical tone in Mandarin Chinese. This project will run from March - August 2023 and is in collaboration with Patrick Rohrer and &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;test-retest-reliability&#34;&gt;Test-retest reliability&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://hrbosker.github.io/author/floris-cos/&#34;&gt;Floris Cos&lt;/a&gt; will run an internship for his Linguistics and Communication Sciences ResMA program. He will assess the test-retest reliability of the Manual McGurk effect using online experimentation.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;re glad to have them join the group. Welcome!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Come join us at the &#39;Dag van de Fonetiek&#39;</title>
      <link>https://hrbosker.github.io/news/22-12-15-dagvdfonetiek2022/</link>
      <pubDate>Thu, 15 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/22-12-15-dagvdfonetiek2022/</guid>
      <description>&lt;p&gt;&lt;strong&gt;This &amp;lsquo;Phonetics Day&amp;rsquo; is the annual meeting of the &amp;lsquo;Dutch Society for Phonetic Sciences&amp;rsquo; [NVFW], taking place in Utrecht on December 16, 2022.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;open-to-all&#34;&gt;Open to all!&lt;/h2&gt;
&lt;p&gt;The &amp;lsquo;Dag van de Fonetiek&amp;rsquo; 2022 will take place on &lt;strong&gt;Friday Dec 16, 2022&lt;/strong&gt;, in the Sweelinckzaal at Drift 21, Utrecht, The Netherlands. It is an event celebrating everything &amp;lsquo;speechy&amp;rsquo; and is free and open to all: members, non-members, scientists, students, anyone! This year, you can hear &lt;a href=&#34;https://hrbosker.github.io/author/ronny-bujok/&#34;&gt;Ronny Bujok&lt;/a&gt; talk about whether beat gestures recalibrate lexical stress perception, and &lt;a href=&#34;https://hrbosker.github.io/author/orhun-ulusahin/&#34;&gt;Orhun Uluşahin&lt;/a&gt; has some intriguing results about how listeners track a talker&amp;rsquo;s pitch!&lt;/p&gt;
&lt;p&gt;See &lt;a href=&#34;https://www.nvfw.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.nvfw.org/&lt;/a&gt; for the full program. We hope to see you there!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Paper accepted in JEP:HPP!</title>
      <link>https://hrbosker.github.io/news/22-12-12-paper-jephpp/</link>
      <pubDate>Mon, 12 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/22-12-12-paper-jephpp/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Congratulazioni a Giulio e Giuseppe for successfully publishing their collaborative project &amp;ldquo;Tracking talker-specific cues to lexical stress: Evidence from perceptual learning&amp;rdquo;!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;accepted&#34;&gt;Accepted!&lt;/h2&gt;
&lt;p&gt;Today we heard that the paper &amp;ldquo;Tracking talker-specific cues to lexical stress: Evidence from perceptual learning&amp;rdquo; has been accepted for publication in the &lt;em&gt;Journal of Experimental Psychology: Human Perception and Performance (JEP:HPP)&lt;/em&gt;, authored by &lt;a href=&#34;https://hrbosker.github.io/author/giulio-severijnen/&#34;&gt;Giulio Severijnen&lt;/a&gt;, Giuseppe Di Dona, &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;, and James McQueen. The fulltext and all data are publicly available at the links provided at the bottom of this post.&lt;/p&gt;
&lt;h2 id=&#34;whats-it-about&#34;&gt;What&amp;rsquo;s it about?&lt;/h2&gt;
&lt;p&gt;The joint first-authors Giulio and Giuseppe set out to test whether listeners track how different talkers cue lexical stress. They first exposed two groups of listeners to two talkers: Talker A and B. These two talkers consistently used different cues to signal lexical stress in Dutch (e.g., differentiating &lt;em&gt;PLAto&lt;/em&gt; from &lt;em&gt;plaTEAU&lt;/em&gt;). Group 1 always heard Talker A use F0 to cue stressed syllables in Dutch, while Talker B always used intensity. Conversely, Group 2 heard the reverse talker-cue mappings: Talker A always used intensity, and Talker B always F0.&lt;/p&gt;
&lt;p&gt;After this (admittedly strange) exposure phase, participants were given an (admittedly even stranger) test phase. They were presented with audio recordings from the two talkers but this time the F0 and intensity cues had been artificially manipulated to &amp;lsquo;point in different directions&amp;rsquo;. For instance, while F0 would clearly cue stress on the first syllable of the word, intensity cues would signal stress on the second syllable. Critically, these &amp;lsquo;mixed items&amp;rsquo; were perceived by listeners according to the talker-cue mappings they had learnt during exposure. That is, Group 1 had learnt that Talker A always used F0 in the exposure phase and therefore, at test, when they heard Talker A produce a mixed item, they were more likely to perceive stress on the syllable marked by F0. However, Group 2 was more likely to perceive the exact same mixed item as having stress on the syllable marked by intensity.&lt;/p&gt;
&lt;h2 id=&#34;why-is-this-important&#34;&gt;Why is this important?&lt;/h2&gt;
&lt;p&gt;These findings support Bayesian models of spoken word recognition. These predict that listeners can adjust their prior beliefs about the perceptual weight of different phonetic cues on the basis of short-term regularities in a talker-specific fashion. This had already been observed for segmental contrasts (e.g., the perception of different consonants and vowels). Now we demonstrate that people also track suprasegmental variability in prosody, such as lexical stress.&lt;/p&gt;
&lt;h2 id=&#34;full-reference&#34;&gt;Full reference&lt;/h2&gt;
&lt;blockquote&gt;












  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/giulio-severijnen/&#34;&gt;Giulio Severijnen&lt;/a&gt;&lt;/span&gt;&lt;i class=&#34;author-notes fas fa-info-circle&#34; data-toggle=&#34;tooltip&#34; title=&#34;Equal contribution&#34;&gt;&lt;/i&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/giuseppe-di-dona/&#34;&gt;Giuseppe Di Dona&lt;/a&gt;&lt;/span&gt;&lt;i class=&#34;author-notes fas fa-info-circle&#34; data-toggle=&#34;tooltip&#34; title=&#34;Equal contribution&#34;&gt;&lt;/i&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/hans-rutger-bosker/&#34;&gt;Hans Rutger Bosker&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://hrbosker.github.io/author/james-m.-mcqueen/&#34;&gt;James M. McQueen&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2023).
  &lt;a href=&#34;https://hrbosker.github.io/publication/severijnen-etal-2023-jephpp/&#34;&gt; Tracking talker-specific cues to lexical stress: Evidence from perceptual learning&lt;/a&gt;.
  &lt;em&gt;Journal of Experimental Psychology: Human Perception and Performance, 49&lt;/em&gt;(4), 549-565. doi:10.1037/xhp0001105.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://pure.mpg.de/rest/items/item_3479620_2/component/file_3484213/content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/severijnen-etal-2023-jephpp/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://osf.io/dczx9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Dataset
&lt;/a&gt;











&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1037/xhp0001105&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;



&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>GESPIN 2023 in Nijmegen</title>
      <link>https://hrbosker.github.io/news/22-11-01-gespin2023/</link>
      <pubDate>Tue, 01 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://hrbosker.github.io/news/22-11-01-gespin2023/</guid>
      <description>&lt;p&gt;&lt;strong&gt;The Gesture and Speech in Interaction [GESPIN] conference is coming to Nijmegen on September 13-15, 2023.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;broadening-perspectives-integrating-views&#34;&gt;Broadening perspectives, integrating views&lt;/h2&gt;
&lt;p&gt;&amp;hellip;that is the theme of the 2023 edition. This promises a highly interdisciplinary event, approaching the interaction of gesture and speech from the perspectives of language development, neurobiology, biomechanics, animal models, and many other fields. Keynotes are: Nuria Esteve Gibert, Yifei He, Susanne Fuchs, and Franz Goller. It is co-organized by CLS, the Donders Institute, and MPI. Paper submission opens January 10th, 2023 and the deadline is &lt;strong&gt;March 15th, 2023&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;See &lt;a href=&#34;https://gespin2023.nl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gespin2023.nl&lt;/a&gt; for all the details. We hope to see you there!&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
